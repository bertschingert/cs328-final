{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import audio_processing as ap\n",
    "import audio_utils as au\n",
    "import math\n",
    "import numpy.fft as fft\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 328 Final Project\n",
    "## Thomas Bertschinger and Sanders McMillan\n",
    "### Lab Notebook\n",
    "\n",
    "### Tuesday, April 26, 2016 **(joint entry)**\n",
    "\n",
    "Our final project will (likely) be *model* focused rather than experiment focused. We will \"implemenet computational model(s) and compare results to existing human data.\" In this case, the human data will be musical compositions created by humans (*e.g.* Bach fugues) and our model will be a system that \"composes\" music (perhaps learning from human compositions). \n",
    "\n",
    "We set up a bibliography using Latex and BibTex, and a git repository to keep track of our code and materials. \n",
    "\n",
    "https://github.com/bertschingert/cs328-final/tree/master/references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wednesday, April 27, 2016 (joint entry)\n",
    "\n",
    "We are thinking about doing a signal-processing based model that creates a representation of a signal and can classify the signal into categories such as instrument family, voice, etc. We will need to create a representation of audio sound in a (hopefully) small number of dimensions that includes enough relevant information so that we can classify a sound into a category such as brass versus string instrument, for example. \n",
    "\n",
    "The representation will likely include attributes such as attack and decay time of the waveform; the spectrum at various points in time; how much the spectrum changes over time; irregularities in the spectrum. \n",
    "\n",
    "Our model will ideally be able to classify instruments correctly at different amplitudes and pitches. It would also be important to limit the model to audio information that humans can actually percieve. (For example, it wouldn't make sense for our model to take into account frequences substantially above 20,000Hz because humans cannot hear that high.) \n",
    "\n",
    "Being able to pull out the relevant information from an audio signal is important because it will help us understand how humans can do things such as distinguish different people's voices. We know probably hundreds of different voices that we can identify from only a few words of speech. This is also important for being able to recognize different instruments present in a single audio signal. \n",
    "\n",
    "We think it would be plausible to create a neural network to identify different audio signals. We will first take an audio signal and use tools such as the discrete Fourier transform to create a suitable representation of the signal that omits extraneous information or information that humans cannot perciever. Then, we train a neural network to be able to identify, from the features of the representation, what category the signal belongs to. \n",
    "\n",
    "Theoretically, our model will be grounded in the similarity models learned earlier in this course, in addition to Gibsonian and Gestalt principles of perceiving invariants in stimuli and grouping similar and proximal stimuli as belonging to the same perceptual unit (e.g. being able to classify a signal as being of a certain category regardless of it's amplitude and signal, and classifying/perceiving similar successive waveforms as being from the same instrument). It will also involve the place and time theories for how humans transform air pressure hitting the ear into an auditory representation (which is where our Fourier transform and representation stages come in). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Monday, May 16, 2016 (joint entry)\n",
    "\n",
    "We have written some code to do basic audio processing (FFT). We are also starting to create the NN..\n",
    "\n",
    "In order to keep the neural network consistent, we will have the option to save weights and biases to a text file so that they can be loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'audio_files/violin-a440.raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9097abe804ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_raw_stereo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"audio_files/violin-a440.raw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_fft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Accounts/mcmillans/Desktop/Final Project/cs328-final/audio_utils.py\u001b[0m in \u001b[0;36mread_raw_stereo\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_raw_stereo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mraw_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'audio_files/violin-a440.raw'"
     ]
    }
   ],
   "source": [
    "left, right = au.read_raw_stereo(\"audio_files/violin-a440.raw\")\n",
    "chunk = left[:44100]\n",
    "freqs = fft.rfft(chunk)\n",
    "au.graph_fft(freqs[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also started compiling our audio sample library. The audio samples are 1.5 second long clips downloaded from http://www.philharmonia.co.uk/explore/make_music. So far we have 5 different instruments (guitar, saxophone, flute, violin, and trumpet), each at three different pitches (A, C, and E). The octaves of the pitches are different for each instrument as the library did not contain all octaves for each instrument, and each instrument has different pitch restraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tuesday, May 17, 2016 (joint entry)\n",
    "\n",
    "We are starting to write code that uses the FFT to get some information on the signal, such as attack time. This will be helpful for our representation. We are also starting to implement the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturday, May 21, 2016\n",
    "\n",
    "Created the Hann window function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_signal = []\n",
    "for i in range(100):\n",
    "    test_signal.append(1)\n",
    "w = ap.hann_window(test_signal)\n",
    "au.graph_signal(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created code to compute the spectral centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wave = au.create_sine_wave(440, 1, 1)\n",
    "spectrum = fft.rfft(wave)\n",
    "print(\"before window: spectral centroid is \", ap.get_spectral_centroid(spectrum, 44100))\n",
    "au.graph_fft(spectrum[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_signal = ap.hann_window(wave)\n",
    "w_spectrum = fft.rfft(w_signal)\n",
    "print(\"after window: spectral centroid is \", ap.get_spectral_centroid(w_spectrum, 44100))\n",
    "au.graph_fft(w_spectrum[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sunday, May 22, 2016\n",
    "\n",
    "Now using the python wave library to read .wav files. Created the function read_wav_mono in audio_utils.py which returns a list of the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = au.read_wav_mono('audio_files/guitar_A4_very-long_forte_normal.wav')\n",
    "au.graph_signal(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step= int(44100 / 4)\n",
    "start = 44100\n",
    "chunk = f[start:start+step]\n",
    "s = fft.rfft(chunk)\n",
    "au.graph_fft(s, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = ap.hann_window(chunk)\n",
    "s = fft.rfft(w)\n",
    "au.graph_fft(s, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = ap.spectral_flux(f)\n",
    "au.graph_signal(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f2 = au.read_wav_mono('audio_files/saxophone_A4_15_forte_normal.wav')\n",
    "au.graph_signal(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s2 = ap.spectral_flux(f2)\n",
    "au.graph_signal(s2)\n",
    "for i in s2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Thursday, May 26\n",
    "\n",
    "I finished writing code for our neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.27843456,  0.46674594,  0.54660266],\n",
      "       [ 0.48294033,  0.74670695,  0.64887128],\n",
      "       [ 0.3295587 ,  0.44350384,  0.34668042]]), array([[ 0.06294714,  0.21763975,  0.68011961],\n",
      "       [ 0.53662074,  0.26200762,  0.8247706 ],\n",
      "       [ 0.42264477,  0.21525684,  0.831234  ]])]\n",
      "[array([[ 0.27843456,  0.46674594,  0.54660266],\n",
      "       [ 0.48294033,  0.74670695,  0.64887128],\n",
      "       [ 0.3295587 ,  0.44350384,  0.34668042]]), array([[ 0.06294714,  0.21763975,  0.68011961],\n",
      "       [ 0.53662074,  0.26200762,  0.8247706 ],\n",
      "       [ 0.42264477,  0.21525684,  0.831234  ]])]\n"
     ]
    }
   ],
   "source": [
    "import neural_net as nn\n",
    "\n",
    "print(nn.initialize_network(3, 3, 3, 3))\n",
    "\n",
    "print(nn.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
