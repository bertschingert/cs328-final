{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import audio_processing as ap\n",
    "import audio_utils as au\n",
    "import math\n",
    "import numpy.fft as fft\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 328 Final Project\n",
    "## Thomas Bertschinger and Sanders McMillan\n",
    "### Lab Notebook\n",
    "\n",
    "### Tuesday, April 26, 2016 **(joint entry)**\n",
    "\n",
    "Our final project will (likely) be *model* focused rather than experiment focused. We will \"implemenet computational model(s) and compare results to existing human data.\" In this case, the human data will be musical compositions created by humans (*e.g.* Bach fugues) and our model will be a system that \"composes\" music (perhaps learning from human compositions). \n",
    "\n",
    "We set up a bibliography using Latex and BibTex, and a git repository to keep track of our code and materials. \n",
    "\n",
    "https://github.com/bertschingert/cs328-final/tree/master/references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wednesday, April 27, 2016 (joint entry)\n",
    "\n",
    "We are thinking about doing a signal-processing based model that creates a representation of a signal and can classify the signal into categories such as instrument family, voice, etc. We will need to create a representation of audio sound in a (hopefully) small number of dimensions that includes enough relevant information so that we can classify a sound into a category such as brass versus string instrument, for example. \n",
    "\n",
    "The representation will likely include attributes such as attack and decay time of the waveform; the spectrum at various points in time; how much the spectrum changes over time; irregularities in the spectrum. \n",
    "\n",
    "Our model will ideally be able to classify instruments correctly at different amplitudes and pitches. It would also be important to limit the model to audio information that humans can actually percieve. (For example, it wouldn't make sense for our model to take into account frequences substantially above 20,000Hz because humans cannot hear that high.) \n",
    "\n",
    "Being able to pull out the relevant information from an audio signal is important because it will help us understand how humans can do things such as distinguish different people's voices. We know probably hundreds of different voices that we can identify from only a few words of speech. This is also important for being able to recognize different instruments present in a single audio signal. \n",
    "\n",
    "We think it would be plausible to create a neural network to identify different audio signals. We will first take an audio signal and use tools such as the discrete Fourier transform to create a suitable representation of the signal that omits extraneous information or information that humans cannot perciever. Then, we train a neural network to be able to identify, from the features of the representation, what category the signal belongs to. \n",
    "\n",
    "Theoretically, our model will be grounded in the similarity models learned earlier in this course, in addition to Gibsonian and Gestalt principles of perceiving invariants in stimuli and grouping similar and proximal stimuli as belonging to the same perceptual unit (e.g. being able to classify a signal as being of a certain category regardless of it's amplitude and signal, and classifying/perceiving similar successive waveforms as being from the same instrument). It will also involve the place and time theories for how humans transform air pressure hitting the ear into an auditory representation (which is where our Fourier transform and representation stages come in). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Monday, May 16, 2016 (joint entry)\n",
    "\n",
    "We have written some code to do basic audio processing (FFT). We are also starting to create the NN..\n",
    "\n",
    "In order to keep the neural network consistent, we will have the option to save weights and biases to a text file so that they can be loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'audio_files/violin-a440.raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9097abe804ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_raw_stereo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"audio_files/violin-a440.raw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_fft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Accounts/mcmillans/Desktop/Final Project/cs328-final/audio_utils.py\u001b[0m in \u001b[0;36mread_raw_stereo\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_raw_stereo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mraw_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'audio_files/violin-a440.raw'"
     ]
    }
   ],
   "source": [
    "left, right = au.read_raw_stereo(\"audio_files/violin-a440.raw\")\n",
    "chunk = left[:44100]\n",
    "freqs = fft.rfft(chunk)\n",
    "au.graph_fft(freqs[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also started compiling our audio sample library. The audio samples are 1.5 second long clips downloaded from http://www.philharmonia.co.uk/explore/make_music. So far we have 5 different instruments (guitar, saxophone, flute, violin, and trumpet), each at three different pitches (A, C, and E). The octaves of the pitches are different for each instrument as the library did not contain all octaves for each instrument, and each instrument has different pitch restraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tuesday, May 17, 2016 (joint entry)\n",
    "\n",
    "We are starting to write code that uses the FFT to get some information on the signal, such as attack time. This will be helpful for our representation. We are also starting to implement the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturday, May 21, 2016\n",
    "\n",
    "Created the Hann window function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_signal = []\n",
    "for i in range(100):\n",
    "    test_signal.append(1)\n",
    "w = ap.hann_window(test_signal)\n",
    "au.graph_signal(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created code to compute the spectral centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wave = au.create_sine_wave(440, 1, 1)\n",
    "spectrum = fft.rfft(wave)\n",
    "print(\"before window: spectral centroid is \", ap.get_spectral_centroid(spectrum, 44100))\n",
    "au.graph_fft(spectrum[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_signal = ap.hann_window(wave)\n",
    "w_spectrum = fft.rfft(w_signal)\n",
    "print(\"after window: spectral centroid is \", ap.get_spectral_centroid(w_spectrum, 44100))\n",
    "au.graph_fft(w_spectrum[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sunday, May 22, 2016\n",
    "\n",
    "Now using the python wave library to read .wav files. Created the function read_wav_mono in audio_utils.py which returns a list of the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = au.read_wav_mono('audio_files/guitar_A4_very-long_forte_normal.wav')\n",
    "au.graph_signal(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step= int(44100 / 4)\n",
    "start = 44100\n",
    "chunk = f[start:start+step]\n",
    "s = fft.rfft(chunk)\n",
    "au.graph_fft(s, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = ap.hann_window(chunk)\n",
    "s = fft.rfft(w)\n",
    "au.graph_fft(s, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = ap.spectral_flux(f)\n",
    "au.graph_signal(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f2 = au.read_wav_mono('audio_files/saxophone_A4_15_forte_normal.wav')\n",
    "au.graph_signal(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s2 = ap.spectral_flux(f2)\n",
    "au.graph_signal(s2)\n",
    "for i in s2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Thursday, May 26\n",
    "\n",
    "We finished writing code for our neural network, and are now testing and de-bugging it. Our neural network has an initialize_network function that initializes the weights and nodes of the networks based on input length, the output length, the number of hidden units, and the number of layers of the network. It also has a set_hidden_units function that allows you to set the number of hidden units at any particular hidden layer, and changes the weights accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.10862947,  0.8522127 ,  0.69202717,  0.99432993],\n",
       "        [ 0.73346391,  0.50975582,  0.87500344,  0.36502353],\n",
       "        [ 0.06261863,  0.36317722,  0.92796805,  0.16657154]]),\n",
       " array([[ 0.87090001,  0.36034179,  0.47638214],\n",
       "        [ 0.07273478,  0.3241552 ,  0.56360695],\n",
       "        [ 0.33731554,  0.89614422,  0.11920964]])]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the initialize network function\n",
    "import neural_net as nn\n",
    "import numpy as np\n",
    "nn.initialize_network(4, 3, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.34369755,  0.84036295,  0.90568857,  0.72985319],\n",
       "        [ 0.15344001,  0.47214932,  0.30966574,  0.62191363],\n",
       "        [ 0.55701622,  0.981771  ,  0.99315375,  0.52207827],\n",
       "        [ 0.29762253,  0.94654795,  0.42537322,  0.43483845],\n",
       "        [ 0.72261829,  0.36450799,  0.82721509,  0.63033539]]),\n",
       " array([[ 0.25176506,  0.87716275,  0.90186555,  0.43185953,  0.95131715],\n",
       "        [ 0.26965406,  0.02369516,  0.86748153,  0.67230029,  0.13918175],\n",
       "        [ 0.47776563,  0.42815781,  0.68453952,  0.51677091,  0.52650537]])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the set hidden units function\n",
    "nn.set_hidden_units(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975855]\n",
      " [ 0.99469021]\n",
      " [ 0.99969593]\n",
      " [ 0.99685965]\n",
      " [ 0.99897228]]\n",
      "activation at layer:  2 [[ 0.98502238]\n",
      " [ 0.89097854]\n",
      " [ 0.9710648 ]]\n",
      "New weights at layer  1 [[ 0.25176726  0.87716495  0.90186776  0.43186173  0.95131936]\n",
      " [ 0.26878881  0.0228343   0.86661633  0.67143755  0.13831718]\n",
      " [ 0.47749285  0.42788641  0.68426676  0.51649892  0.5262328 ]]\n",
      "New weights at layer  0 [[ 0.34369746  0.84036277  0.90568831  0.72985285]\n",
      " [ 0.1534393   0.4721479   0.30966361  0.62191079]\n",
      " [ 0.55701593  0.98177043  0.99315289  0.52207713]\n",
      " [ 0.29762027  0.94654343  0.42536645  0.43482942]\n",
      " [ 0.72261802  0.36450745  0.82721428  0.63033432]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998947]\n",
      " [ 1.        ]\n",
      " [ 0.9999993 ]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98513014]\n",
      " [ 0.89073782]\n",
      " [ 0.97115239]]\n",
      "New weights at layer  1 [[ 0.25162296  0.87702064  0.90172345  0.43171743  0.95117505]\n",
      " [ 0.26889515  0.02294063  0.86672267  0.67154388  0.13842352]\n",
      " [ 0.47722078  0.42761434  0.68399468  0.51622685  0.52596073]]\n",
      "New weights at layer  0 [[ 0.34369746  0.84036277  0.90568831  0.72985285]\n",
      " [ 0.15343928  0.47214789  0.30966359  0.62191077]\n",
      " [ 0.55701593  0.98177043  0.99315289  0.52207713]\n",
      " [ 0.29762027  0.94654343  0.42536645  0.43482942]\n",
      " [ 0.72261802  0.36450745  0.82721428  0.63033432]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.9851176 ]\n",
      " [ 0.89079997]\n",
      " [ 0.97110676]]\n",
      "New weights at layer  1 [[ 0.25147853  0.87687621  0.90157902  0.431573    0.95103062]\n",
      " [ 0.26802862  0.02207411  0.86585614  0.67067735  0.13755699]\n",
      " [ 0.47722888  0.42762244  0.68400279  0.51623495  0.52596883]]\n",
      "New weights at layer  0 [[ 0.34369746  0.84036277  0.90568831  0.72985285]\n",
      " [ 0.15343928  0.47214789  0.30966359  0.62191077]\n",
      " [ 0.55701593  0.98177043  0.99315289  0.52207713]\n",
      " [ 0.29762027  0.94654343  0.42536645  0.43482942]\n",
      " [ 0.72261802  0.36450745  0.82721428  0.63033432]]\n",
      "[array([[ 0.34369746,  0.84036277,  0.90568831,  0.72985285],\n",
      "       [ 0.15343928,  0.47214789,  0.30966359,  0.62191077],\n",
      "       [ 0.55701593,  0.98177043,  0.99315289,  0.52207713],\n",
      "       [ 0.29762027,  0.94654343,  0.42536645,  0.43482942],\n",
      "       [ 0.72261802,  0.36450745,  0.82721428,  0.63033432]]), array([[ 0.25147853,  0.87687621,  0.90157902,  0.431573  ,  0.95103062],\n",
      "       [ 0.26802862,  0.02207411,  0.86585614,  0.67067735,  0.13755699],\n",
      "       [ 0.47722888,  0.42762244,  0.68400279,  0.51623495,  0.52596883]])]\n"
     ]
    }
   ],
   "source": [
    "#Testing the update weights function\n",
    "input = np.array([[1,2,3, 4], [5, 6, 7, 8], [9,10,11, 12]])\n",
    "output = np.array([[1, 0, 0], [0,1,0], [0,0,1]])\n",
    "np.array(output)\n",
    "nn.update_weights(input.T, output.T, 0.01)\n",
    "print(nn.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975855]\n",
      " [ 0.99469009]\n",
      " [ 0.99969593]\n",
      " [ 0.99685943]\n",
      " [ 0.99897228]]\n",
      "activation at layer:  2 [[ 0.98499704]\n",
      " [ 0.89002989]\n",
      " [ 0.97097439]]\n",
      "New weights at layer  1 [[ 0.25150069  0.87689826  0.90160119  0.4315951   0.95105277]\n",
      " [ 0.25931941  0.01340904  0.85714748  0.6619934   0.12885463]\n",
      " [ 0.47449304  0.42490047  0.68126711  0.51350704  0.52323514]]\n",
      "New weights at layer  0 [[ 0.3436966   0.84036106  0.90568573  0.72984942]\n",
      " [ 0.15343263  0.47213457  0.30964362  0.62188415]\n",
      " [ 0.5570131   0.98176477  0.9931444   0.52206581]\n",
      " [ 0.29759785  0.94649859  0.42529918  0.43473973]\n",
      " [ 0.72261542  0.36450225  0.82720648  0.63032392]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998947]\n",
      " [ 1.        ]\n",
      " [ 0.9999993 ]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98510669]\n",
      " [ 0.88509184]\n",
      " [ 0.97064451]]\n",
      "New weights at layer  1 [[ 0.25005539  0.87545298  0.90015589  0.4301498   0.94960747]\n",
      " [ 0.26048807  0.0145777   0.85831614  0.66316206  0.13002329]\n",
      " [ 0.47172731  0.42213477  0.67850139  0.51074131  0.52046941]]\n",
      "New weights at layer  0 [[ 0.3436966   0.84036106  0.90568573  0.72984942]\n",
      " [ 0.1534325   0.47213442  0.30964344  0.62188395]\n",
      " [ 0.5570131   0.98176477  0.9931444   0.52206581]\n",
      " [ 0.29759785  0.94649858  0.42529917  0.43473972]\n",
      " [ 0.72261542  0.36450225  0.82720648  0.63032392]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98497907]\n",
      " [ 0.88580312]\n",
      " [ 0.97016811]]\n",
      "New weights at layer  1 [[ 0.24859809  0.87399567  0.89869858  0.4286925   0.94815017]\n",
      " [ 0.25152765  0.00561727  0.84935571  0.65420163  0.12106287]\n",
      " [ 0.47181365  0.42222111  0.67858772  0.51082765  0.52055575]]\n",
      "New weights at layer  0 [[ 0.3436966   0.84036106  0.90568573  0.72984942]\n",
      " [ 0.1534325   0.47213442  0.30964344  0.62188394]\n",
      " [ 0.5570131   0.98176477  0.9931444   0.52206581]\n",
      " [ 0.29759785  0.94649858  0.42529917  0.43473972]\n",
      " [ 0.72261542  0.36450225  0.82720648  0.63032392]]\n",
      "Iteration:  1\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975855]\n",
      " [ 0.99468899]\n",
      " [ 0.99969591]\n",
      " [ 0.99685726]\n",
      " [ 0.99897219]]\n",
      "activation at layer:  2 [[ 0.98473989]\n",
      " [ 0.87998403]\n",
      " [ 0.97004674]]\n",
      "New weights at layer  1 [[ 0.24862101  0.87401848  0.89872151  0.42871536  0.94817307]\n",
      " [ 0.24223619 -0.00362707  0.84006484  0.64493714  0.11177872]\n",
      " [ 0.46899575  0.4194175   0.67577001  0.50801793  0.51774007]]\n",
      "New weights at layer  0 [[ 0.34369574  0.84035933  0.90568315  0.72984597]\n",
      " [ 0.15342654  0.4721225   0.30962556  0.6218601 ]\n",
      " [ 0.55701016  0.98175888  0.99313556  0.52205402]\n",
      " [ 0.29757461  0.94645211  0.42522947  0.43464679]\n",
      " [ 0.72261287  0.36449716  0.82719885  0.63031375]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998946]\n",
      " [ 1.        ]\n",
      " [ 0.9999993 ]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98485106]\n",
      " [ 0.8742575 ]\n",
      " [ 0.96969066]]\n",
      "New weights at layer  1 [[ 0.24715167  0.87254916  0.89725216  0.42724601  0.94670373]\n",
      " [ 0.24361849 -0.00224478  0.84144715  0.64631945  0.11316102]\n",
      " [ 0.46614577  0.41656754  0.67292002  0.50516795  0.51489008]]\n",
      "New weights at layer  0 [[ 0.34369574  0.84035933  0.90568315  0.72984597]\n",
      " [ 0.15342641  0.47212234  0.30962538  0.62185989]\n",
      " [ 0.55701016  0.98175888  0.99313556  0.52205402]\n",
      " [ 0.29757461  0.94645211  0.42522947  0.43464678]\n",
      " [ 0.72261287  0.36449716  0.82719885  0.63031375]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98471911]\n",
      " [ 0.87516647]\n",
      " [ 0.96918417]]\n",
      "New weights at layer  1 [[ 0.24566993  0.87106741  0.89577042  0.42576427  0.94522199]\n",
      " [ 0.23405729 -0.01180598  0.83188594  0.63675824  0.10359982]\n",
      " [ 0.4662378   0.41665958  0.67301206  0.50525998  0.51498212]]\n",
      "New weights at layer  0 [[ 0.34369574  0.84035933  0.90568315  0.72984597]\n",
      " [ 0.15342641  0.47212234  0.30962538  0.62185989]\n",
      " [ 0.55701016  0.98175888  0.99313556  0.52205402]\n",
      " [ 0.29757461  0.94645211  0.42522947  0.43464678]\n",
      " [ 0.72261287  0.36449716  0.82719885  0.63031375]]\n",
      "Iteration:  2\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975854]\n",
      " [ 0.994688  ]\n",
      " [ 0.99969588]\n",
      " [ 0.996855  ]\n",
      " [ 0.99897211]]\n",
      "activation at layer:  2 [[ 0.98447403]\n",
      " [ 0.86849419]\n",
      " [ 0.96906158]]\n",
      "New weights at layer  1 [[ 0.24569365  0.87109102  0.89579414  0.42578792  0.94524569]\n",
      " [ 0.22414044 -0.02167254  0.82196971  0.62687019  0.09369077]\n",
      " [ 0.46333314  0.41376965  0.67010757  0.50236376  0.51207974]]\n",
      "New weights at layer  0 [[ 0.34369488  0.84035761  0.90568057  0.72984253]\n",
      " [ 0.1534213   0.47211213  0.30961006  0.62183946]\n",
      " [ 0.55700709  0.98175275  0.99312637  0.52204177]\n",
      " [ 0.29755057  0.94640403  0.42515735  0.43455063]\n",
      " [ 0.72261041  0.36449225  0.82719147  0.63030391]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998946]\n",
      " [ 1.        ]\n",
      " [ 0.9999993 ]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98458677]\n",
      " [ 0.86183909]\n",
      " [ 0.96867676]]\n",
      "New weights at layer  1 [[ 0.24419948  0.86959686  0.89429997  0.42429375  0.94375152]\n",
      " [ 0.22578555 -0.02002744  0.82361483  0.6285153   0.09533588]\n",
      " [ 0.46039397  0.41083051  0.66716841  0.49942459  0.50914057]]\n",
      "New weights at layer  0 [[ 0.34369488  0.84035761  0.90568057  0.72984253]\n",
      " [ 0.15342117  0.47211197  0.30960987  0.62183925]\n",
      " [ 0.55700709  0.98175275  0.99312637  0.52204177]\n",
      " [ 0.29755057  0.94640403  0.42515735  0.43455062]\n",
      " [ 0.72261041  0.36449224  0.82719147  0.63030391]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98445027]\n",
      " [ 0.86301025]\n",
      " [ 0.96813738]]\n",
      "New weights at layer  1 [[ 0.24269249  0.86808987  0.89279298  0.42278676  0.94224453]\n",
      " [ 0.21558274 -0.03023026  0.81341201  0.61831249  0.08513307]\n",
      " [ 0.46049226  0.4109288   0.66726669  0.49952288  0.50923886]]\n",
      "New weights at layer  0 [[ 0.34369488  0.84035761  0.90568057  0.72984253]\n",
      " [ 0.15342117  0.47211197  0.30960987  0.62183925]\n",
      " [ 0.55700709  0.98175275  0.99312637  0.52204177]\n",
      " [ 0.29755057  0.94640403  0.42515735  0.43455062]\n",
      " [ 0.72261041  0.36449224  0.82719147  0.63030391]]\n",
      "Iteration:  3\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975853]\n",
      " [ 0.99468716]\n",
      " [ 0.99969585]\n",
      " [ 0.99685266]\n",
      " [ 0.99897203]]\n",
      "activation at layer:  2 [[ 0.98419903]\n",
      " [ 0.85534226]\n",
      " [ 0.96801366]]\n",
      "New weights at layer  1 [[ 0.24271705  0.86811431  0.89281754  0.42281125  0.94226907]\n",
      " [ 0.20500198 -0.04075734  0.80283192  0.60776249  0.07456064]\n",
      " [ 0.4574957   0.40794744  0.66427032  0.49653503  0.50624466]]\n",
      "New weights at layer  0 [[ 0.34369403  0.84035591  0.90567801  0.72983912]\n",
      " [ 0.1534171   0.47210383  0.30959766  0.62182297]\n",
      " [ 0.55700391  0.98174638  0.99311682  0.52202904]\n",
      " [ 0.29752575  0.94635439  0.42508289  0.43445135]\n",
      " [ 0.72260807  0.36448755  0.82718444  0.63029453]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998946]\n",
      " [ 1.        ]\n",
      " [ 0.9999993 ]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98431337]\n",
      " [ 0.84760126]\n",
      " [ 0.96759723]]\n",
      "New weights at layer  1 [[ 0.24119722  0.86659449  0.89129771  0.42129142  0.94074924]\n",
      " [ 0.20697057 -0.03878877  0.80480051  0.60973107  0.07652922]\n",
      " [ 0.45446201  0.40491378  0.66123663  0.49350134  0.50321097]]\n",
      "New weights at layer  0 [[ 0.34369403  0.84035591  0.90567801  0.72983912]\n",
      " [ 0.15341696  0.47210366  0.30959747  0.62182275]\n",
      " [ 0.55700391  0.98174638  0.99311682  0.52202904]\n",
      " [ 0.29752575  0.94635439  0.42508289  0.43445135]\n",
      " [ 0.72260807  0.36448755  0.82718444  0.63029453]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98417209]\n",
      " [ 0.84912073]\n",
      " [ 0.9670218 ]]\n",
      "New weights at layer  1 [[ 0.23966414  0.86506141  0.88976462  0.41975834  0.93921616]\n",
      " [ 0.19609208 -0.04966726  0.79392202  0.59885259  0.06565074]\n",
      " [ 0.45456718  0.40501895  0.6613418   0.49360651  0.50331614]]\n",
      "New weights at layer  0 [[ 0.34369403  0.84035591  0.90567801  0.72983912]\n",
      " [ 0.15341696  0.47210366  0.30959747  0.62182275]\n",
      " [ 0.55700391  0.98174638  0.99311682  0.52202904]\n",
      " [ 0.29752575  0.94635439  0.42508289  0.43445135]\n",
      " [ 0.72260807  0.36448755  0.82718444  0.63029453]]\n",
      "Iteration:  4\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975853]\n",
      " [ 0.99468648]\n",
      " [ 0.99969582]\n",
      " [ 0.99685025]\n",
      " [ 0.99897196]]\n",
      "activation at layer:  2 [[ 0.98391441]\n",
      " [ 0.84030133]\n",
      " [ 0.96689707]]\n",
      "New weights at layer  1 [[ 0.23968959  0.86508673  0.88979007  0.41978372  0.93924159]\n",
      " [ 0.18481838 -0.06088377  0.78264903  0.58761168  0.0543859 ]\n",
      " [ 0.45147316  0.40194063  0.65824798  0.4905215   0.50022456]]\n",
      "New weights at layer  0 [[ 0.34369319  0.84035423  0.90567549  0.72983576]\n",
      " [ 0.15341413  0.472098    0.30958898  0.62181143]\n",
      " [ 0.55700062  0.98173979  0.99310694  0.52201585]\n",
      " [ 0.29750021  0.94630331  0.42500628  0.4343492 ]\n",
      " [ 0.72260587  0.36448316  0.82717785  0.63028575]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998945]\n",
      " [ 1.        ]\n",
      " [ 0.9999993 ]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98403041]\n",
      " [ 0.83130513]\n",
      " [ 0.9664458 ]]\n",
      "New weights at layer  1 [[ 0.23814323  0.86354039  0.88824371  0.41823736  0.93769523]\n",
      " [ 0.18718411 -0.05851807  0.78501475  0.5899774   0.05675163]\n",
      " [ 0.44833914  0.39880665  0.65511396  0.48738748  0.49709054]]\n",
      "New weights at layer  0 [[ 0.34369319  0.84035423  0.90567549  0.72983576]\n",
      " [ 0.15341399  0.47209783  0.30958878  0.6218112 ]\n",
      " [ 0.55700062  0.98173979  0.99310694  0.52201585]\n",
      " [ 0.29750021  0.94630331  0.42500627  0.43434919]\n",
      " [ 0.72260587  0.36448316  0.82717785  0.63028575]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98388411]\n",
      " [ 0.83328632]\n",
      " [ 0.96583079]]\n",
      "New weights at layer  1 [[ 0.23658316  0.86198032  0.88668365  0.41667729  0.93613516]\n",
      " [ 0.17560807 -0.0700941   0.77343872  0.57840137  0.0451756 ]\n",
      " [ 0.44845191  0.39891941  0.65522673  0.48750024  0.4972033 ]]\n",
      "New weights at layer  0 [[ 0.34369319  0.84035423  0.90567549  0.72983576]\n",
      " [ 0.15341399  0.47209783  0.30958878  0.6218112 ]\n",
      " [ 0.55700062  0.98173979  0.99310694  0.52201585]\n",
      " [ 0.29750021  0.94630331  0.42500627  0.43434919]\n",
      " [ 0.72260587  0.36448316  0.82717785  0.63028575]]\n",
      "Iteration:  5\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975852]\n",
      " [ 0.99468601]\n",
      " [ 0.99969579]\n",
      " [ 0.99684776]\n",
      " [ 0.99897189]]\n",
      "activation at layer:  2 [[ 0.98361968]\n",
      " [ 0.82315463]\n",
      " [ 0.96570521]]\n",
      "New weights at layer  1 [[ 0.23660955  0.86200657  0.88671003  0.4167036   0.93616153]\n",
      " [ 0.16362822 -0.08201317  0.76145961  0.56645639  0.03320516]\n",
      " [ 0.44525439  0.39573812  0.65202941  0.48431204  0.4940083 ]]\n",
      "New weights at layer  0 [[ 0.34369237  0.8403526   0.90567305  0.7298325 ]\n",
      " [ 0.15341261  0.47209508  0.30958465  0.6218057 ]\n",
      " [ 0.55699721  0.98173299  0.99309673  0.52200224]\n",
      " [ 0.29747404  0.94625099  0.42492779  0.43424454]\n",
      " [ 0.72260387  0.36447915  0.82717183  0.63027772]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998945]\n",
      " [ 1.        ]\n",
      " [ 0.9999993 ]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98373739]\n",
      " [ 0.81273304]\n",
      " [ 0.96521546]]\n",
      "New weights at layer  1 [[ 0.23503575  0.8604328   0.88513624  0.41512981  0.93458773]\n",
      " [ 0.16647838 -0.07916304  0.76430978  0.56930656  0.03605533]\n",
      " [ 0.44201372  0.39249748  0.64878874  0.48107137  0.49076763]]\n",
      "New weights at layer  0 [[ 0.34369237  0.8403526   0.90567305  0.7298325 ]\n",
      " [ 0.15341246  0.4720949   0.30958444  0.62180546]\n",
      " [ 0.55699721  0.98173299  0.99309673  0.52200224]\n",
      " [ 0.29747404  0.94625098  0.42492778  0.43424454]\n",
      " [ 0.72260387  0.36447915  0.82717183  0.63027772]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98358578]\n",
      " [ 0.8153218 ]\n",
      " [ 0.96455685]]\n",
      "New weights at layer  1 [[ 0.23344777  0.85884482  0.88354826  0.41354183  0.93299975]\n",
      " [ 0.15420191 -0.09143951  0.7520333   0.55703008  0.02377885]\n",
      " [ 0.44213489  0.39261865  0.64890991  0.48119254  0.4908888 ]]\n",
      "New weights at layer  0 [[ 0.34369237  0.8403526   0.90567305  0.7298325 ]\n",
      " [ 0.15341246  0.4720949   0.30958444  0.62180546]\n",
      " [ 0.55699721  0.98173299  0.99309673  0.52200224]\n",
      " [ 0.29747404  0.94625098  0.42492778  0.43424454]\n",
      " [ 0.72260387  0.36447915  0.82717183  0.63027772]]\n",
      "Iteration:  6\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975852]\n",
      " [ 0.99468577]\n",
      " [ 0.99969576]\n",
      " [ 0.99684521]\n",
      " [ 0.99897183]]\n",
      "activation at layer:  2 [[ 0.98331429]\n",
      " [ 0.80372608]\n",
      " [ 0.96443061]]\n",
      "New weights at layer  1 [[ 0.23347514  0.85887205  0.88357563  0.41356912  0.9330271 ]\n",
      " [ 0.14152615 -0.10405095  0.73935834  0.54439126  0.01111307]\n",
      " [ 0.43882729  0.38932783  0.64560252  0.47789457  0.4875838 ]]\n",
      "New weights at layer  0 [[ 0.34369159  0.84035103  0.9056707   0.72982937]\n",
      " [ 0.15341275  0.47209548  0.30958531  0.62180662]\n",
      " [ 0.55699372  0.981726    0.99308625  0.52198827]\n",
      " [ 0.2974474   0.9461977   0.42484786  0.43413797]\n",
      " [ 0.72260209  0.3644756   0.82716651  0.63027062]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998945]\n",
      " [ 1.        ]\n",
      " [ 0.9999993 ]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98343377]\n",
      " [ 0.79172657]\n",
      " [ 0.96389824]]\n",
      "New weights at layer  1 [[ 0.23187295  0.85726987  0.88197344  0.41196693  0.93142491]\n",
      " [ 0.14496049 -0.10061665  0.74279268  0.5478256   0.01454741]\n",
      " [ 0.43547308  0.38597365  0.6422483   0.47454036  0.48422959]]\n",
      "New weights at layer  0 [[ 0.34369159  0.84035103  0.9056707   0.72982937]\n",
      " [ 0.15341259  0.47209529  0.30958509  0.62180636]\n",
      " [ 0.55699372  0.981726    0.99308625  0.52198827]\n",
      " [ 0.2974474   0.9461977   0.42484785  0.43413797]\n",
      " [ 0.72260209  0.3644756   0.82716651  0.63027062]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98327658]\n",
      " [ 0.79510387]\n",
      " [ 0.9631915 ]]\n",
      "New weights at layer  1 [[ 0.23025608  0.855653    0.88035656  0.41035005  0.92980804]\n",
      " [ 0.13200716 -0.11356998  0.72983935  0.53487227  0.00159408]\n",
      " [ 0.43560358  0.38610415  0.6423788   0.47467086  0.48436009]]\n",
      "New weights at layer  0 [[ 0.34369159  0.84035103  0.9056707   0.72982937]\n",
      " [ 0.15341259  0.47209529  0.30958509  0.62180636]\n",
      " [ 0.55699372  0.981726    0.99308625  0.52198827]\n",
      " [ 0.2974474   0.9461977   0.42484785  0.43413797]\n",
      " [ 0.72260209  0.3644756   0.82716651  0.63027062]]\n",
      "Iteration:  7\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975851]\n",
      " [ 0.9946858 ]\n",
      " [ 0.99969572]\n",
      " [ 0.99684261]\n",
      " [ 0.99897177]]\n",
      "activation at layer:  2 [[ 0.98299767]\n",
      " [ 0.78192311]\n",
      " [ 0.96306485]]\n",
      "New weights at layer  1 [[ 0.23028449  0.85568126  0.88038497  0.41037838  0.92983642]\n",
      " [ 0.11867707 -0.12683243  0.7165101   0.52158106 -0.01172551]\n",
      " [ 0.43217869  0.38269665  0.63895413  0.47125597  0.4809379 ]]\n",
      "New weights at layer  0 [[ 0.34369085  0.84034956  0.90566849  0.72982642]\n",
      " [ 0.15341473  0.47209956  0.3095915   0.62181491]\n",
      " [ 0.55699016  0.98171887  0.99307556  0.52197401]\n",
      " [ 0.29742046  0.94614383  0.42476705  0.43403024]\n",
      " [ 0.72260059  0.36447259  0.82716199  0.6302646 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998945]\n",
      " [ 1.        ]\n",
      " [ 0.99999929]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98311897]\n",
      " [ 0.7682373 ]\n",
      " [ 0.96248516]]\n",
      "New weights at layer  1 [[ 0.2286529   0.85404969  0.87875338  0.40874679  0.92820483]\n",
      " [ 0.12280358 -0.12270597  0.72063661  0.52570756 -0.00759901]\n",
      " [ 0.4287034   0.37922139  0.63547884  0.46778068  0.47746261]]\n",
      "New weights at layer  0 [[ 0.34369085  0.84034956  0.90566849  0.72982642]\n",
      " [ 0.15341456  0.47209936  0.30959126  0.62181464]\n",
      " [ 0.55699016  0.98171887  0.99307556  0.52197401]\n",
      " [ 0.29742046  0.94614383  0.42476705  0.43403024]\n",
      " [ 0.72260058  0.36447259  0.82716199  0.6302646 ]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98295589]\n",
      " [ 0.77261613]\n",
      " [ 0.96172512]]\n",
      "New weights at layer  1 [[ 0.22700609  0.85240289  0.87710657  0.40709999  0.92655803]\n",
      " [ 0.10923023 -0.13627932  0.70706325  0.51213421 -0.02117236]\n",
      " [ 0.42884429  0.37936228  0.63561973  0.46792157  0.4776035 ]]\n",
      "New weights at layer  0 [[ 0.34369085  0.84034956  0.90566849  0.72982642]\n",
      " [ 0.15341456  0.47209936  0.30959126  0.62181464]\n",
      " [ 0.55699016  0.98171887  0.99307556  0.52197401]\n",
      " [ 0.29742046  0.94614383  0.42476705  0.43403024]\n",
      " [ 0.72260058  0.36447259  0.82716199  0.6302646 ]]\n",
      "Iteration:  8\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.9997585 ]\n",
      " [ 0.99468614]\n",
      " [ 0.99969569]\n",
      " [ 0.99683998]\n",
      " [ 0.99897172]]\n",
      "activation at layer:  2 [[ 0.98266922]\n",
      " [ 0.75778769]\n",
      " [ 0.96159839]]\n",
      "New weights at layer  1 [[ 0.2270356   0.85243224  0.87713608  0.40712941  0.92658751]\n",
      " [ 0.09532473 -0.15011426  0.69315863  0.49826931 -0.03506691]\n",
      " [ 0.42529426  0.37583026  0.63206992  0.4643819   0.47405626]]\n",
      "New weights at layer  0 [[ 0.34369017  0.84034819  0.90566644  0.72982368]\n",
      " [ 0.15341868  0.47210759  0.30960361  0.6218311 ]\n",
      " [ 0.55698655  0.98171166  0.99306473  0.52195958]\n",
      " [ 0.29739348  0.94608986  0.42468609  0.43392229]\n",
      " [ 0.72259938  0.36447019  0.82715839  0.6302598 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998946]\n",
      " [ 1.        ]\n",
      " [ 0.99999929]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98279239]\n",
      " [ 0.74238284]\n",
      " [ 0.96096601]]\n",
      "New weights at layer  1 [[ 0.22537355  0.85077021  0.87547403  0.40546736  0.92492546]\n",
      " [ 0.10025167 -0.14518737  0.69808558  0.50319625 -0.03013997]\n",
      " [ 0.42168964  0.37222569  0.62846531  0.46077729  0.47045165]]\n",
      "New weights at layer  0 [[ 0.34369017  0.84034819  0.90566644  0.72982368]\n",
      " [ 0.15341849  0.47210737  0.30960335  0.62183081]\n",
      " [ 0.55698655  0.98171166  0.99306473  0.52195958]\n",
      " [ 0.29739348  0.94608986  0.42468609  0.43392229]\n",
      " [ 0.72259938  0.36447019  0.82715839  0.6302598 ]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98262309]\n",
      " [ 0.74799565]\n",
      " [ 0.96014678]]\n",
      "New weights at layer  1 [[ 0.22369573  0.84909239  0.8737962   0.40378953  0.92324764]\n",
      " [ 0.08615209 -0.15928695  0.683986    0.48909667 -0.04423955]\n",
      " [ 0.42184214  0.37237818  0.62861781  0.46092979  0.47060414]]\n",
      "New weights at layer  0 [[ 0.34369017  0.84034819  0.90566644  0.72982368]\n",
      " [ 0.15341849  0.47210737  0.30960335  0.62183081]\n",
      " [ 0.55698655  0.98171166  0.99306473  0.52195958]\n",
      " [ 0.29739348  0.94608986  0.42468609  0.43392229]\n",
      " [ 0.72259938  0.36447019  0.82715839  0.6302598 ]]\n",
      "Iteration:  9\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.9997585 ]\n",
      " [ 0.9946868 ]\n",
      " [ 0.99969565]\n",
      " [ 0.99683735]\n",
      " [ 0.99897168]]\n",
      "activation at layer:  2 [[ 0.98232825]\n",
      " [ 0.7315438 ]\n",
      " [ 0.96002037]]\n",
      "New weights at layer  1 [[ 0.2237264   0.8491229   0.87382687  0.40382011  0.92327828]\n",
      " [ 0.07178896 -0.17357722  0.66962376  0.4747755  -0.05859138]\n",
      " [ 0.41815835  0.36871308  0.62493425  0.45725676  0.46692325]]\n",
      "New weights at layer  0 [[ 0.34368955  0.84034695  0.90566458  0.72982121]\n",
      " [ 0.15342463  0.47211964  0.30962176  0.62185536]\n",
      " [ 0.55698293  0.98170442  0.99305387  0.5219451 ]\n",
      " [ 0.2973667   0.9460363   0.42460576  0.43381518]\n",
      " [ 0.72259851  0.36446844  0.82715577  0.63025631]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998946]\n",
      " [ 1.        ]\n",
      " [ 0.99999929]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98245337]\n",
      " [ 0.71449156]\n",
      " [ 0.95932915]]\n",
      "New weights at layer  1 [[ 0.22203277  0.84742929  0.87213325  0.40212649  0.92158466]\n",
      " [ 0.07761314 -0.1677531   0.67544795  0.48059968 -0.0527672 ]\n",
      " [ 0.41441536  0.36497013  0.62119126  0.45351377  0.46318026]]\n",
      "New weights at layer  0 [[ 0.34368955  0.84034695  0.90566458  0.72982121]\n",
      " [ 0.15342443  0.47211941  0.30962148  0.62185504]\n",
      " [ 0.55698293  0.98170442  0.99305387  0.5219451 ]\n",
      " [ 0.2973667   0.94603631  0.42460577  0.43381518]\n",
      " [ 0.72259851  0.36446844  0.82715577  0.6302563 ]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98227749]\n",
      " [ 0.72156607]\n",
      " [ 0.95844399]]\n",
      "New weights at layer  1 [[ 0.22032278  0.8457193   0.87042326  0.4004165   0.91987467]\n",
      " [ 0.06311627 -0.18224998  0.66095107  0.46610281 -0.06726407]\n",
      " [ 0.41458088  0.36513565  0.62135677  0.45367929  0.46334578]]\n",
      "New weights at layer  0 [[ 0.34368955  0.84034695  0.90566458  0.72982121]\n",
      " [ 0.15342443  0.47211941  0.30962148  0.62185504]\n",
      " [ 0.55698293  0.98170442  0.99305387  0.5219451 ]\n",
      " [ 0.2973667   0.94603631  0.42460577  0.43381518]\n",
      " [ 0.72259851  0.36446844  0.82715577  0.6302563 ]]\n",
      "Iteration:  10\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975849]\n",
      " [ 0.99468779]\n",
      " [ 0.99969562]\n",
      " [ 0.99683473]\n",
      " [ 0.99897165]]\n",
      "activation at layer:  2 [[ 0.98197408]\n",
      " [ 0.70362238]\n",
      " [ 0.95831841]]\n",
      "New weights at layer  1 [[ 0.22035468  0.84575104  0.87045515  0.40044831  0.91990654]\n",
      " [ 0.04844662 -0.19684522  0.64628234  0.45147606 -0.08192218]\n",
      " [ 0.41075387  0.36132805  0.61753001  0.44986348  0.45952179]]\n",
      "New weights at layer  0 [[ 0.343689    0.84034585  0.90566293  0.72981901]\n",
      " [ 0.15343253  0.4721356   0.30964577  0.62188742]\n",
      " [ 0.55697933  0.98169722  0.99304309  0.52193072]\n",
      " [ 0.29734041  0.94598371  0.42452688  0.43371   ]\n",
      " [ 0.72259797  0.36446736  0.82715414  0.63025414]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998947]\n",
      " [ 1.        ]\n",
      " [ 0.99999929]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98210119]\n",
      " [ 0.68511343]\n",
      " [ 0.95756129]]\n",
      "New weights at layer  1 [[ 0.2186283   0.84402468  0.86872877  0.39872193  0.91818016]\n",
      " [ 0.05523976 -0.19005215  0.65307549  0.45826919 -0.07512904]\n",
      " [ 0.40686257  0.35743679  0.6136387   0.44597217  0.45563048]]\n",
      "New weights at layer  0 [[ 0.343689    0.84034585  0.90566293  0.72981901]\n",
      " [ 0.15343231  0.47213534  0.30964547  0.62188708]\n",
      " [ 0.55697933  0.98169722  0.99304309  0.52193072]\n",
      " [ 0.29734041  0.94598372  0.42452688  0.43371001]\n",
      " [ 0.72259797  0.36446736  0.82715414  0.63025414]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98191836]\n",
      " [ 0.69383903]\n",
      " [ 0.95660246]]\n",
      "New weights at layer  1 [[ 0.21688493  0.84228131  0.86698541  0.39697856  0.9164368 ]\n",
      " [ 0.04050079 -0.20479113  0.63833651  0.44353022 -0.08986801]\n",
      " [ 0.40704273  0.35761695  0.61381887  0.44615233  0.45581064]]\n",
      "New weights at layer  0 [[ 0.343689    0.84034585  0.90566293  0.72981901]\n",
      " [ 0.15343231  0.47213534  0.30964547  0.62188708]\n",
      " [ 0.55697933  0.98169722  0.99304309  0.52193072]\n",
      " [ 0.29734041  0.94598372  0.42452688  0.43371001]\n",
      " [ 0.72259797  0.36446736  0.82715414  0.63025414]]\n",
      "Iteration:  11\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975849]\n",
      " [ 0.9946891 ]\n",
      " [ 0.99969559]\n",
      " [ 0.99683216]\n",
      " [ 0.99897164]]\n",
      "activation at layer:  2 [[ 0.98160593]\n",
      " [ 0.67464419]\n",
      " [ 0.95647835]]\n",
      "New weights at layer  1 [[ 0.21691814  0.84231435  0.86701861  0.39701167  0.91646997]\n",
      " [ 0.02569596 -0.21952088  0.62353262  0.42876873 -0.10466118]\n",
      " [ 0.40306211  0.35365651  0.6098385   0.44218336  0.45183315]]\n",
      "New weights at layer  0 [[ 0.34368852  0.8403449   0.9056615   0.7298171 ]\n",
      " [ 0.15344219  0.4721551   0.30967511  0.6219266 ]\n",
      " [ 0.55697579  0.98169014  0.99303246  0.52191656]\n",
      " [ 0.29731484  0.94593258  0.42445018  0.43360773]\n",
      " [ 0.72259774  0.36446691  0.82715347  0.63025324]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998948]\n",
      " [ 1.        ]\n",
      " [ 0.99999929]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98173511]\n",
      " [ 0.65497849]\n",
      " [ 0.95564718]]\n",
      "New weights at layer  1 [[ 0.21515776  0.84055399  0.86525823  0.39525129  0.9147096 ]\n",
      " [ 0.03349282 -0.21172411  0.63132947  0.43656558 -0.09686433]\n",
      " [ 0.39901154  0.34960598  0.60578792  0.4381328   0.44778258]]\n",
      "New weights at layer  0 [[ 0.34368852  0.8403449   0.9056615   0.7298171 ]\n",
      " [ 0.15344195  0.47215481  0.30967478  0.62192622]\n",
      " [ 0.55697579  0.98169014  0.99303246  0.52191656]\n",
      " [ 0.29731484  0.94593259  0.42445018  0.43360774]\n",
      " [ 0.72259774  0.36446691  0.82715347  0.63025324]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98154491]\n",
      " [ 0.6654717 ]\n",
      " [ 0.95460576]]\n",
      "New weights at layer  1 [[ 0.21337974  0.83877597  0.86348021  0.39347327  0.91293158]\n",
      " [ 0.01867814 -0.22653878  0.6165148   0.42175091 -0.111679  ]\n",
      " [ 0.39920825  0.34980269  0.60598463  0.4383295   0.44797929]]\n",
      "New weights at layer  0 [[ 0.34368852  0.8403449   0.9056615   0.7298171 ]\n",
      " [ 0.15344195  0.47215481  0.30967478  0.62192622]\n",
      " [ 0.55697579  0.98169014  0.99303246  0.52191656]\n",
      " [ 0.29731484  0.94593259  0.42445018  0.43360774]\n",
      " [ 0.72259774  0.36446691  0.82715347  0.63025324]]\n",
      "Iteration:  12\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975849]\n",
      " [ 0.9946907 ]\n",
      " [ 0.99969555]\n",
      " [ 0.99682965]\n",
      " [ 0.99897163]]\n",
      "activation at layer:  2 [[ 0.98122297]\n",
      " [ 0.64535309]\n",
      " [ 0.95448392]]\n",
      "New weights at layer  1 [[ 0.21341433  0.83881038  0.8635148   0.39350776  0.91296614]\n",
      " [ 0.00391135 -0.24123072  0.60174894  0.40702738 -0.12643417]\n",
      " [ 0.39506255  0.34567801  0.6018392   0.43419596  0.44383686]]\n",
      "New weights at layer  0 [[ 0.34368811  0.84034408  0.90566027  0.72981547]\n",
      " [ 0.15345335  0.47217761  0.30970898  0.62197182]\n",
      " [ 0.55697234  0.98168323  0.9930221   0.52190274]\n",
      " [ 0.2972902   0.94588329  0.42437624  0.43350915]\n",
      " [ 0.7225978   0.36446703  0.82715365  0.63025348]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998948]\n",
      " [ 1.        ]\n",
      " [ 0.99999929]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98135429]\n",
      " [ 0.62490381]\n",
      " [ 0.95356929]]\n",
      "New weights at layer  1 [[ 0.21161864  0.83701471  0.86171911  0.39171207  0.91117045]\n",
      " [ 0.01270357 -0.23243859  0.61054116  0.41581959 -0.11764195]\n",
      " [ 0.39084063  0.34145614  0.59761728  0.42997404  0.43961494]]\n",
      "New weights at layer  0 [[ 0.34368811  0.84034408  0.90566027  0.72981547]\n",
      " [ 0.15345309  0.4721773   0.30970861  0.6219714 ]\n",
      " [ 0.55697234  0.98168323  0.9930221   0.52190274]\n",
      " [ 0.2972902   0.9458833   0.42437625  0.43350916]\n",
      " [ 0.7225978   0.36446702  0.82715364  0.63025348]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98115629]\n",
      " [ 0.63718482]\n",
      " [ 0.95243495]]\n",
      "New weights at layer  1 [[ 0.20980462  0.83520069  0.85990509  0.38989805  0.90935643]\n",
      " [-0.00202689 -0.24716905  0.5958107   0.40108913 -0.13237241]\n",
      " [ 0.39105612  0.34167162  0.59783276  0.43018952  0.43983043]]\n",
      "New weights at layer  0 [[ 0.34368811  0.84034408  0.90566027  0.72981547]\n",
      " [ 0.15345309  0.4721773   0.30970861  0.6219714 ]\n",
      " [ 0.55697234  0.98168323  0.9930221   0.52190274]\n",
      " [ 0.2972902   0.9458833   0.42437625  0.43350916]\n",
      " [ 0.7225978   0.36446702  0.82715364  0.63025348]]\n",
      "Iteration:  13\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975848]\n",
      " [ 0.99469255]\n",
      " [ 0.99969552]\n",
      " [ 0.99682724]\n",
      " [ 0.99897163]]\n",
      "activation at layer:  2 [[ 0.98082433]\n",
      " [ 0.616514  ]\n",
      " [ 0.95231638]]\n",
      "New weights at layer  1 [[ 0.20984067  0.83523656  0.85994114  0.389934    0.90939246]\n",
      " [-0.01659927 -0.26166759  0.58123924  0.38655948 -0.14693332]\n",
      " [ 0.3867327   0.33737011  0.59350962  0.42587879  0.43551041]]\n",
      "New weights at layer  0 [[ 0.34368777  0.8403434   0.90565924  0.72981409]\n",
      " [ 0.15346568  0.47220248  0.30974639  0.62202177]\n",
      " [ 0.55696899  0.98167653  0.99301205  0.52188933]\n",
      " [ 0.2972666   0.9458361   0.42430545  0.43341476]\n",
      " [ 0.7225981   0.36446762  0.82715454  0.63025467]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998949]\n",
      " [ 1.        ]\n",
      " [ 0.99999929]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98095787]\n",
      " [ 0.5956752 ]\n",
      " [ 0.95130736]]\n",
      "New weights at layer  1 [[ 0.20800829  0.8334042   0.85810876  0.38810162  0.90756007]\n",
      " [-0.00686126 -0.25192968  0.59097725  0.39629748 -0.13719531]\n",
      " [ 0.38232609  0.33296355  0.58910301  0.42147217  0.4311038 ]]\n",
      "New weights at layer  0 [[ 0.34368777  0.8403434   0.90565924  0.72981409]\n",
      " [ 0.1534654   0.47220214  0.30974599  0.62202131]\n",
      " [ 0.55696899  0.98167653  0.99301205  0.52188933]\n",
      " [ 0.29726661  0.9458361   0.42430545  0.43341476]\n",
      " [ 0.7225981   0.36446762  0.82715454  0.63025467]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98075158]\n",
      " [ 0.60966459]\n",
      " [ 0.9500681 ]]\n",
      "New weights at layer  1 [[ 0.20615684  0.83155275  0.8562573   0.38625016  0.90570862]\n",
      " [-0.02136967 -0.26643809  0.57646884  0.38178907 -0.15170372]\n",
      " [ 0.38256296  0.33320042  0.58933988  0.42170904  0.43134067]]\n",
      "New weights at layer  0 [[ 0.34368777  0.8403434   0.90565924  0.72981409]\n",
      " [ 0.1534654   0.47220214  0.30974599  0.62202131]\n",
      " [ 0.55696899  0.98167653  0.99301205  0.52188933]\n",
      " [ 0.29726661  0.9458361   0.42430545  0.43341476]\n",
      " [ 0.7225981   0.36446762  0.82715454  0.63025467]]\n",
      "Iteration:  14\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975848]\n",
      " [ 0.99469459]\n",
      " [ 0.99969549]\n",
      " [ 0.99682492]\n",
      " [ 0.99897164]]\n",
      "activation at layer:  2 [[ 0.98040904]\n",
      " [ 0.58880779]\n",
      " [ 0.94995408]]\n",
      "New weights at layer  1 [[ 0.20619445  0.83159017  0.85629492  0.38628767  0.90574621]\n",
      " [-0.03562204 -0.28061827  0.56221737  0.36757852 -0.16594487]\n",
      " [ 0.37804784  0.32870817  0.58482505  0.41720718  0.42682911]]\n",
      "New weights at layer  0 [[ 0.34368748  0.84034282  0.90565838  0.72981294]\n",
      " [ 0.15347884  0.47222903  0.30978631  0.62207508]\n",
      " [ 0.55696575  0.98167006  0.99300234  0.5218764 ]\n",
      " [ 0.2972441   0.9457911   0.42423795  0.43332475]\n",
      " [ 0.72259858  0.36446859  0.827156    0.63025661]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.9999895 ]\n",
      " [ 1.        ]\n",
      " [ 0.99999928]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98054488]\n",
      " [ 0.56794292]\n",
      " [ 0.94883791]]\n",
      "New weights at layer  1 [[ 0.20432391  0.82971965  0.85442437  0.38441713  0.90387566]\n",
      " [-0.02502006 -0.27001641  0.57281935  0.37818049 -0.15534289]\n",
      " [ 0.37344175  0.32410213  0.58021896  0.41260109  0.42222302]]\n",
      "New weights at layer  0 [[ 0.34368748  0.84034282  0.90565838  0.72981294]\n",
      " [ 0.15347853  0.47222865  0.30978588  0.62207459]\n",
      " [ 0.55696575  0.98167006  0.99300234  0.5218764 ]\n",
      " [ 0.29724411  0.9457911   0.42423795  0.43332476]\n",
      " [ 0.72259858  0.36446859  0.82715599  0.63025661]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.98032979]\n",
      " [ 0.58347917]\n",
      " [ 0.94747973]]\n",
      "New weights at layer  1 [[ 0.20243351  0.82782925  0.85253397  0.38252673  0.90198526]\n",
      " [-0.03920042 -0.28419677  0.55863898  0.36400012 -0.16952326]\n",
      " [ 0.3737031   0.32436348  0.58048031  0.41286244  0.42248437]]\n",
      "New weights at layer  0 [[ 0.34368748  0.84034282  0.90565838  0.72981294]\n",
      " [ 0.15347853  0.47222865  0.30978588  0.62207459]\n",
      " [ 0.55696575  0.98167006  0.99300234  0.5218764 ]\n",
      " [ 0.29724411  0.9457911   0.42423795  0.43332476]\n",
      " [ 0.72259858  0.36446859  0.82715599  0.63025661]]\n",
      "Iteration:  15\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975848]\n",
      " [ 0.99469676]\n",
      " [ 0.99969546]\n",
      " [ 0.99682271]\n",
      " [ 0.99897166]]\n",
      "activation at layer:  2 [[ 0.97997606]\n",
      " [ 0.56275591]\n",
      " [ 0.94737188]]\n",
      "New weights at layer  1 [[ 0.20247279  0.82786833  0.85257326  0.38256589  0.90202451]\n",
      " [-0.05304435 -0.2979706   0.54479593  0.35019685 -0.18335629]\n",
      " [ 0.3689808   0.31966508  0.5757583   0.408154    0.41776578]]\n",
      "New weights at layer  0 [[ 0.34368724  0.84034234  0.90565765  0.72981198]\n",
      " [ 0.1534925   0.4722566   0.3098278   0.62213048]\n",
      " [ 0.55696264  0.98166384  0.992993    0.52186394]\n",
      " [ 0.29722269  0.94574827  0.4241737   0.43323909]\n",
      " [ 0.7225992   0.36446983  0.82715785  0.63025908]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998951]\n",
      " [ 1.        ]\n",
      " [ 0.99999928]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.98011429]\n",
      " [ 0.54216179]\n",
      " [ 0.94613364]]\n",
      "New weights at layer  1 [[ 0.20056252  0.82595808  0.85066299  0.38065563  0.90011424]\n",
      " [-0.04167978 -0.28660615  0.5561605   0.36156141 -0.17199172]\n",
      " [ 0.36415885  0.31484319  0.57093635  0.40333206  0.41294383]]\n",
      "New weights at layer  0 [[ 0.34368724  0.84034234  0.90565765  0.72981198]\n",
      " [ 0.15349217  0.4722562   0.30982733  0.62212994]\n",
      " [ 0.55696264  0.98166384  0.992993    0.52186394]\n",
      " [ 0.2972227   0.94574828  0.42417371  0.4332391 ]\n",
      " [ 0.7225992   0.36446982  0.82715785  0.63025908]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97988984]\n",
      " [ 0.55903166]\n",
      " [ 0.94464016]]\n",
      "New weights at layer  1 [[ 0.19863158  0.82402714  0.84873204  0.37872468  0.8981833 ]\n",
      " [-0.05546076 -0.30038714  0.54237952  0.34778043 -0.1857727 ]\n",
      " [ 0.36444835  0.31513269  0.57122586  0.40362156  0.41323334]]\n",
      "New weights at layer  0 [[ 0.34368724  0.84034234  0.90565765  0.72981198]\n",
      " [ 0.15349217  0.4722562   0.30982733  0.62212994]\n",
      " [ 0.55696264  0.98166384  0.992993    0.52186394]\n",
      " [ 0.2972227   0.94574828  0.42417371  0.4332391 ]\n",
      " [ 0.7225992   0.36446982  0.82715785  0.63025908]]\n",
      "Iteration:  16\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975848]\n",
      " [ 0.99469902]\n",
      " [ 0.99969543]\n",
      " [ 0.99682061]\n",
      " [ 0.99897168]]\n",
      "activation at layer:  2 [[ 0.97952428]\n",
      " [ 0.53869073]\n",
      " [ 0.94454053]]\n",
      "New weights at layer  1 [[ 0.19867263  0.82406799  0.84877309  0.37876562  0.89822432]\n",
      " [-0.06884416 -0.3137028   0.52899696  0.33443636 -0.19914557]\n",
      " [ 0.3595017   0.31021107  0.56627951  0.39868944  0.40829057]]\n",
      "New weights at layer  0 [[ 0.34368704  0.84034193  0.90565704  0.72981116]\n",
      " [ 0.1535064   0.47228466  0.30987002  0.62218686]\n",
      " [ 0.55695964  0.98165784  0.992984    0.52185195]\n",
      " [ 0.29720231  0.94570749  0.42411254  0.43315754]\n",
      " [ 0.7225999   0.36447123  0.82715995  0.63026188]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998952]\n",
      " [ 1.        ]\n",
      " [ 0.99999928]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.97966499]\n",
      " [ 0.51858268]\n",
      " [ 0.94316269]]\n",
      "New weights at layer  1 [[ 0.19672099  0.82211637  0.84682146  0.37681398  0.89627268]\n",
      " [-0.05682535 -0.30168412  0.54101577  0.34645516 -0.18712676]\n",
      " [ 0.3544457   0.30515512  0.56122351  0.39363345  0.40323458]]\n",
      "New weights at layer  0 [[ 0.34368704  0.84034193  0.90565704  0.72981116]\n",
      " [ 0.15350604  0.47228423  0.30986952  0.62218629]\n",
      " [ 0.55695964  0.98165784  0.992984    0.52185195]\n",
      " [ 0.29720231  0.9457075   0.42411254  0.43315755]\n",
      " [ 0.7225999   0.36447123  0.82715995  0.63026188]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97943057]\n",
      " [ 0.53655333]\n",
      " [ 0.94151464]]\n",
      "New weights at layer  1 [[ 0.1947478   0.82014317  0.84484826  0.37484079  0.89429949]\n",
      " [-0.07016749 -0.31502626  0.52767363  0.33311302 -0.2004689 ]\n",
      " [ 0.35476775  0.30547717  0.56154556  0.3939555   0.40355662]]\n",
      "New weights at layer  0 [[ 0.34368704  0.84034193  0.90565704  0.72981116]\n",
      " [ 0.15350604  0.47228423  0.30986952  0.62218629]\n",
      " [ 0.55695964  0.98165784  0.992984    0.52185195]\n",
      " [ 0.29720231  0.9457075   0.42411254  0.43315755]\n",
      " [ 0.7225999   0.36447123  0.82715995  0.63026188]]\n",
      "Iteration:  17\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975848]\n",
      " [ 0.99470132]\n",
      " [ 0.9996954 ]\n",
      " [ 0.99681861]\n",
      " [ 0.9989717 ]]\n",
      "activation at layer:  2 [[ 0.97905246]\n",
      " [ 0.51676712]\n",
      " [ 0.94142585]]\n",
      "New weights at layer  1 [[ 0.19479075  0.82018591  0.84489121  0.37488361  0.89434241]\n",
      " [-0.08306902 -0.32786253  0.51477291  0.32024943 -0.21336028]\n",
      " [ 0.34957768  0.30031335  0.55635582  0.38878069  0.39837064]]\n",
      "New weights at layer  0 [[ 0.34368686  0.84034157  0.90565651  0.72981045]\n",
      " [ 0.15352031  0.47231277  0.30991233  0.62224336]\n",
      " [ 0.55695675  0.98165206  0.99297533  0.52184038]\n",
      " [ 0.29718286  0.94566859  0.42405418  0.43307972]\n",
      " [ 0.72260064  0.36447271  0.82716218  0.63026485]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998953]\n",
      " [ 1.        ]\n",
      " [ 0.99999928]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.97919576]\n",
      " [ 0.49728174]\n",
      " [ 0.93988774]]\n",
      "New weights at layer  1 [[ 0.19279599  0.81819117  0.84289645  0.37288885  0.89234765]\n",
      " [-0.07050144 -0.31529508  0.5273405   0.332817   -0.20079269]\n",
      " [ 0.34426743  0.29500316  0.55104557  0.38347044  0.39306039]]\n",
      "New weights at layer  0 [[ 0.34368686  0.84034157  0.90565651  0.72981045]\n",
      " [ 0.15351994  0.47231232  0.3099118   0.62224276]\n",
      " [ 0.55695675  0.98165206  0.99297533  0.52184038]\n",
      " [ 0.29718286  0.94566859  0.42405418  0.43307973]\n",
      " [ 0.72260064  0.36447271  0.82716218  0.63026485]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97895072]\n",
      " [ 0.51612666]\n",
      " [ 0.93806237]]\n",
      "New weights at layer  1 [[ 0.19077874  0.81617392  0.8408792   0.37087161  0.8903304 ]\n",
      " [-0.08339118 -0.32818482  0.51445075  0.31992726 -0.21368244]\n",
      " [ 0.34462729  0.29536302  0.55140543  0.38383031  0.39342025]]\n",
      "New weights at layer  0 [[ 0.34368686  0.84034157  0.90565651  0.72981045]\n",
      " [ 0.15351994  0.47231232  0.3099118   0.62224276]\n",
      " [ 0.55695675  0.98165206  0.99297533  0.52184038]\n",
      " [ 0.29718286  0.94566859  0.42405418  0.43307973]\n",
      " [ 0.72260064  0.36447271  0.82716218  0.63026485]]\n",
      "Iteration:  18\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975847]\n",
      " [ 0.99470362]\n",
      " [ 0.99969537]\n",
      " [ 0.99681669]\n",
      " [ 0.99897172]]\n",
      "activation at layer:  2 [[ 0.9785593]\n",
      " [ 0.4969984]\n",
      " [ 0.9379878]]\n",
      "New weights at layer  1 [[ 0.19082372  0.81621867  0.84092417  0.37091645  0.89037534]\n",
      " [-0.09581269 -0.34054353  0.50203003  0.3075423  -0.22609417]\n",
      " [ 0.33917264  0.28993596  0.54595113  0.37839171  0.3879699 ]]\n",
      "New weights at layer  0 [[ 0.3436867   0.84034126  0.90565604  0.72980982]\n",
      " [ 0.15353409  0.47234062  0.30995425  0.62229937]\n",
      " [ 0.55695395  0.98164647  0.99296695  0.5218292 ]\n",
      " [ 0.29716424  0.94563135  0.42399832  0.43300524]\n",
      " [ 0.72260139  0.36447422  0.82716443  0.63026786]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998954]\n",
      " [ 1.        ]\n",
      " [ 0.99999928]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.9787053 ]\n",
      " [ 0.47820675]\n",
      " [ 0.93626493]]\n",
      "New weights at layer  1 [[ 0.18878398  0.81417895  0.83888443  0.36887671  0.8883356 ]\n",
      " [-0.08279264 -0.32752362  0.51505008  0.32056234 -0.21307413]\n",
      " [ 0.33358568  0.28434905  0.54036417  0.37280475  0.38238293]]\n",
      "New weights at layer  0 [[ 0.3436867   0.84034126  0.90565604  0.72980982]\n",
      " [ 0.15353369  0.47234015  0.3099537   0.62229874]\n",
      " [ 0.55695395  0.98164647  0.99296695  0.5218292 ]\n",
      " [ 0.29716424  0.94563135  0.42399832  0.43300525]\n",
      " [ 0.72260139  0.36447421  0.82716443  0.63026786]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97844892]\n",
      " [ 0.49772215]\n",
      " [ 0.9342353 ]]\n",
      "New weights at layer  1 [[ 0.18672076  0.81211573  0.83682121  0.36681349  0.88627238]\n",
      " [-0.09523544 -0.33996641  0.50260728  0.30811954 -0.22551692]\n",
      " [ 0.33398974  0.28475311  0.54076822  0.3732088   0.38278699]]\n",
      "New weights at layer  0 [[ 0.3436867   0.84034126  0.90565604  0.72980982]\n",
      " [ 0.1535337   0.47234015  0.3099537   0.62229874]\n",
      " [ 0.55695395  0.98164647  0.99296695  0.5218292 ]\n",
      " [ 0.29716424  0.94563135  0.42399832  0.43300525]\n",
      " [ 0.72260139  0.36447421  0.82716443  0.63026786]]\n",
      "Iteration:  19\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975847]\n",
      " [ 0.99470591]\n",
      " [ 0.99969535]\n",
      " [ 0.99681486]\n",
      " [ 0.99897175]]\n",
      "activation at layer:  2 [[ 0.97804336]\n",
      " [ 0.47929983]\n",
      " [ 0.93417931]]\n",
      "New weights at layer  1 [[ 0.1867679   0.81216263  0.83686835  0.36686049  0.88631948]\n",
      " [-0.10719451 -0.35186504  0.49064897  0.29619569 -0.23746658]\n",
      " [ 0.32824701  0.2790394   0.53502586  0.36748299  0.37704878]]\n",
      "New weights at layer  0 [[ 0.34368656  0.84034097  0.9056556   0.72980924]\n",
      " [ 0.15354762  0.472368    0.30999548  0.62235444]\n",
      " [ 0.55695124  0.98164104  0.99295881  0.52181835]\n",
      " [ 0.29714635  0.94559556  0.42394463  0.43293366]\n",
      " [ 0.72260213  0.36447569  0.82716664  0.6302708 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998955]\n",
      " [ 1.        ]\n",
      " [ 0.99999928]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.97819217]\n",
      " [ 0.46122396]\n",
      " [ 0.9322426 ]]\n",
      "New weights at layer  1 [[ 0.18468119  0.81007595  0.83478165  0.36477379  0.88423278]\n",
      " [-0.09380612 -0.33847679  0.50403736  0.30958407 -0.22407819]\n",
      " [ 0.32235838  0.27315083  0.52913723  0.36159436  0.37116015]]\n",
      "New weights at layer  0 [[ 0.34368656  0.84034097  0.9056556   0.72980924]\n",
      " [ 0.15354721  0.47236751  0.30999491  0.62235379]\n",
      " [ 0.55695124  0.98164104  0.99295881  0.52181835]\n",
      " [ 0.29714635  0.94559557  0.42394464  0.43293367]\n",
      " [ 0.72260213  0.36447568  0.82716664  0.6302708 ]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97792367]\n",
      " [ 0.48123644]\n",
      " [ 0.92997665]]\n",
      "New weights at layer  1 [[ 0.18256996  0.80796471  0.83267041  0.36266255  0.88212154]\n",
      " [-0.10582008 -0.35049076  0.49202339  0.2975701  -0.23609216]\n",
      " [ 0.32281437  0.27360682  0.52959322  0.36205035  0.37161614]]\n",
      "New weights at layer  0 [[ 0.34368656  0.84034097  0.9056556   0.72980924]\n",
      " [ 0.15354721  0.47236751  0.30999491  0.62235379]\n",
      " [ 0.55695124  0.98164104  0.99295881  0.52181835]\n",
      " [ 0.29714635  0.94559557  0.42394464  0.43293367]\n",
      " [ 0.72260213  0.36447568  0.82716664  0.6302708 ]]\n",
      "Iteration:  20\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975847]\n",
      " [ 0.99470815]\n",
      " [ 0.99969532]\n",
      " [ 0.9968131 ]\n",
      " [ 0.99897177]]\n",
      "activation at layer:  2 [[ 0.97750306]\n",
      " [ 0.46352777]\n",
      " [ 0.92994492]]\n",
      "New weights at layer  1 [[ 0.18261942  0.80801392  0.83271987  0.36271186  0.88217096]\n",
      " [-0.11734384 -0.3619563   0.48050037  0.2860803  -0.24760684]\n",
      " [ 0.31675749  0.26758054  0.52353672  0.35601131  0.36556402]]\n",
      "New weights at layer  0 [[ 0.34368642  0.8403407   0.9056552   0.7298087 ]\n",
      " [ 0.15356085  0.47239479  0.31003582  0.62240834]\n",
      " [ 0.5569486   0.98163576  0.99295089  0.52180779]\n",
      " [ 0.29712908  0.94556103  0.42389283  0.43286459]\n",
      " [ 0.72260283  0.36447709  0.82716874  0.63027361]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998956]\n",
      " [ 1.        ]\n",
      " [ 0.99999928]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.97765482]\n",
      " [ 0.44615633]\n",
      " [ 0.92775969]]\n",
      "New weights at layer  1 [[ 0.18048364  0.80587817  0.83058409  0.36057609  0.88003519]\n",
      " [-0.10365831 -0.34827092  0.49418589  0.29976581 -0.23392132]\n",
      " [ 0.31053949  0.2613626   0.51731872  0.34979332  0.35934603]]\n",
      "New weights at layer  0 [[ 0.34368642  0.8403407   0.9056552   0.7298087 ]\n",
      " [ 0.15356043  0.47239428  0.31003523  0.62240767]\n",
      " [ 0.5569486   0.98163576  0.99295089  0.52180779]\n",
      " [ 0.29712908  0.94556103  0.42389283  0.4328646 ]\n",
      " [ 0.72260283  0.36447709  0.82716874  0.6302736 ]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97737334]\n",
      " [ 0.46652424]\n",
      " [ 0.92521921]]\n",
      "New weights at layer  1 [[ 0.17832221  0.80371674  0.82842266  0.35841466  0.87787376]\n",
      " [-0.11526914 -0.35988174  0.48257507  0.28815499 -0.24553214]\n",
      " [ 0.31105689  0.26188     0.51783612  0.35031072  0.35986342]]\n",
      "New weights at layer  0 [[ 0.34368642  0.8403407   0.9056552   0.7298087 ]\n",
      " [ 0.15356043  0.47239428  0.31003523  0.62240767]\n",
      " [ 0.5569486   0.98163576  0.99295089  0.52180779]\n",
      " [ 0.29712908  0.94556103  0.42389283  0.4328646 ]\n",
      " [ 0.72260283  0.36447709  0.82716874  0.6302736 ]]\n",
      "Iteration:  21\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975847]\n",
      " [ 0.99471034]\n",
      " [ 0.9996953 ]\n",
      " [ 0.9968114 ]\n",
      " [ 0.99897179]]\n",
      "activation at layer:  2 [[ 0.97693669]\n",
      " [ 0.44951004]\n",
      " [ 0.92521915]]\n",
      "New weights at layer  1 [[ 0.17837417  0.80376843  0.82847461  0.35846646  0.87792567]\n",
      " [-0.12638961 -0.37094606  0.47145529  0.27706729 -0.25664386]\n",
      " [ 0.30465696  0.25551239  0.5114366   0.34392966  0.35346854]]\n",
      "New weights at layer  0 [[ 0.34368629  0.84034044  0.90565481  0.72980819]\n",
      " [ 0.15357375  0.47242093  0.3100752   0.62246096]\n",
      " [ 0.55694602  0.9816306   0.99294315  0.52179747]\n",
      " [ 0.29711235  0.94552756  0.42384263  0.43279766]\n",
      " [ 0.72260348  0.3644784   0.8271707   0.63027622]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998957]\n",
      " [ 1.        ]\n",
      " [ 0.99999928]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.97709154]\n",
      " [ 0.43281084]\n",
      " [ 0.92274396]]\n",
      "New weights at layer  1 [[ 0.17618708  0.80158136  0.82628752  0.35627937  0.87573858]\n",
      " [-0.11246593 -0.35702253  0.48537897  0.29099096 -0.24272019]\n",
      " [ 0.29807895  0.24893445  0.50485858  0.33735165  0.34689052]]\n",
      "New weights at layer  0 [[ 0.34368629  0.84034044  0.90565481  0.72980819]\n",
      " [ 0.15357332  0.4724204   0.31007459  0.62246026]\n",
      " [ 0.55694602  0.9816306   0.99294315  0.52179747]\n",
      " [ 0.29711235  0.94552757  0.42384264  0.43279767]\n",
      " [ 0.72260348  0.36447839  0.8271707   0.63027622]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97679616]\n",
      " [ 0.45342198]\n",
      " [ 0.91988323]]\n",
      "New weights at layer  1 [[ 0.17397313  0.79936741  0.82407357  0.35406542  0.87352463]\n",
      " [-0.12370311 -0.36825971  0.47414179  0.27975378 -0.25395737]\n",
      " [ 0.29866939  0.24952489  0.50544903  0.33794209  0.34748097]]\n",
      "New weights at layer  0 [[ 0.34368629  0.84034044  0.90565481  0.72980819]\n",
      " [ 0.15357332  0.4724204   0.31007459  0.62246026]\n",
      " [ 0.55694602  0.9816306   0.99294315  0.52179747]\n",
      " [ 0.29711235  0.94552757  0.42384264  0.43279767]\n",
      " [ 0.72260348  0.36447839  0.8271707   0.63027622]]\n",
      "Iteration:  22\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975847]\n",
      " [ 0.99471248]\n",
      " [ 0.99969527]\n",
      " [ 0.99680975]\n",
      " [ 0.99897181]]\n",
      "activation at layer:  2 [[ 0.9763424 ]\n",
      " [ 0.43706661]\n",
      " [ 0.91992462]]\n",
      "New weights at layer  1 [[ 0.17402776  0.79942177  0.8241282   0.35411989  0.87357922]\n",
      " [-0.13445407 -0.37895641  0.46339151  0.26903453 -0.26469987]\n",
      " [ 0.29189456  0.24278425  0.49867462  0.33118724  0.34071147]]\n",
      "New weights at layer  0 [[ 0.34368617  0.84034019  0.90565443  0.72980768]\n",
      " [ 0.15358633  0.47244642  0.31011362  0.6225123 ]\n",
      " [ 0.55694348  0.98162553  0.99293554  0.52178733]\n",
      " [ 0.29709608  0.94549502  0.42379381  0.43273256]\n",
      " [ 0.72260408  0.3644796   0.82717251  0.63027863]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998958]\n",
      " [ 1.        ]\n",
      " [ 0.99999928]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.97650048]\n",
      " [ 0.42099591]\n",
      " [ 0.91710983]]\n",
      "New weights at layer  1 [[ 0.17178695  0.79718099  0.8218874   0.35187909  0.87133841]\n",
      " [-0.12034037 -0.36484285  0.47750522  0.28314823 -0.25058616]\n",
      " [ 0.28492275  0.23581251  0.49170281  0.32421543  0.33373965]]\n",
      "New weights at layer  0 [[ 0.34368617  0.84034019  0.90565443  0.72980768]\n",
      " [ 0.15358588  0.47244589  0.310113    0.62251159]\n",
      " [ 0.55694348  0.98162553  0.99293554  0.52178733]\n",
      " [ 0.29709608  0.94549502  0.42379382  0.43273257]\n",
      " [ 0.72260408  0.36447959  0.8271725   0.63027862]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97619017]\n",
      " [ 0.44176349]\n",
      " [ 0.91387411]]\n",
      "New weights at layer  1 [[ 0.169518    0.79491204  0.81961844  0.34961014  0.86906946]\n",
      " [-0.13123463 -0.37573711  0.46661095  0.27225396 -0.26148043]\n",
      " [ 0.28560063  0.23649039  0.49238069  0.32489332  0.33441753]]\n",
      "New weights at layer  0 [[ 0.34368617  0.84034019  0.90565443  0.72980768]\n",
      " [ 0.15358588  0.47244589  0.310113    0.62251159]\n",
      " [ 0.55694348  0.98162553  0.99293554  0.52178733]\n",
      " [ 0.29709608  0.94549502  0.42379382  0.43273257]\n",
      " [ 0.72260408  0.36447959  0.8271725   0.63027862]]\n",
      "Iteration:  23\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975847]\n",
      " [ 0.99471457]\n",
      " [ 0.99969525]\n",
      " [ 0.99680815]\n",
      " [ 0.99897183]]\n",
      "activation at layer:  2 [[ 0.97571813]\n",
      " [ 0.42602241]\n",
      " [ 0.91396988]]\n",
      "New weights at layer  1 [[ 0.16957552  0.79496926  0.81967596  0.34966749  0.86912693]\n",
      " [-0.14164953 -0.38609946  0.45619672  0.2618698  -0.27188713]\n",
      " [ 0.27841592  0.22934193  0.48519644  0.31772981  0.32723848]]\n",
      "New weights at layer  0 [[ 0.34368604  0.84033994  0.90565406  0.72980718]\n",
      " [ 0.1535986   0.47247133  0.31015116  0.62256248]\n",
      " [ 0.55694099  0.98162054  0.99292806  0.52177735]\n",
      " [ 0.2970802   0.94546326  0.42374617  0.43266905]\n",
      " [ 0.72260463  0.36448068  0.82717414  0.6302808 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998959]\n",
      " [ 1.        ]\n",
      " [ 0.99999928]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.9758796 ]\n",
      " [ 0.41053127]\n",
      " [ 0.91075601]]\n",
      "New weights at layer  1 [[ 0.16727843  0.7926722   0.81737887  0.3473704   0.86682985]\n",
      " [-0.12738466 -0.37183474  0.47046159  0.27613466 -0.25762226]\n",
      " [ 0.27101334  0.22193943  0.47779386  0.31032723  0.3198359 ]]\n",
      "New weights at layer  0 [[ 0.34368604  0.84033994  0.90565406  0.72980718]\n",
      " [ 0.15359815  0.47247078  0.31015053  0.62256175]\n",
      " [ 0.55694099  0.98162054  0.99292806  0.52177735]\n",
      " [ 0.2970802   0.94546326  0.42374618  0.43266905]\n",
      " [ 0.72260462  0.36448068  0.82717414  0.6302808 ]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97555325]\n",
      " [ 0.43138948]\n",
      " [ 0.9070797 ]]\n",
      "New weights at layer  1 [[ 0.16495183  0.79034559  0.81505226  0.34504379  0.86450324]\n",
      " [-0.13796632 -0.38241641  0.45987992  0.265553   -0.26820392]\n",
      " [ 0.27179653  0.22272262  0.47857704  0.31111042  0.32061909]]\n",
      "New weights at layer  0 [[ 0.34368604  0.84033994  0.90565406  0.72980718]\n",
      " [ 0.15359815  0.47247079  0.31015053  0.62256175]\n",
      " [ 0.55694099  0.98162054  0.99292806  0.52177735]\n",
      " [ 0.2970802   0.94546326  0.42374618  0.43266905]\n",
      " [ 0.72260462  0.36448068  0.82717414  0.6302808 ]]\n",
      "Iteration:  24\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975847]\n",
      " [ 0.99471661]\n",
      " [ 0.99969522]\n",
      " [ 0.99680658]\n",
      " [ 0.99897185]]\n",
      "activation at layer:  2 [[ 0.97506163]\n",
      " [ 0.41621447]\n",
      " [ 0.9072471 ]]\n",
      "New weights at layer  1 [[ 0.16501245  0.79040591  0.81511289  0.34510424  0.86456382]\n",
      " [-0.14807706 -0.39247615  0.44976983  0.25547211 -0.2783067 ]\n",
      " [ 0.2641639   0.21512849  0.47094491  0.30350033  0.31299247]]\n",
      "New weights at layer  0 [[ 0.34368592  0.84033969  0.90565369  0.72980669]\n",
      " [ 0.15361063  0.47249575  0.31018797  0.62261167]\n",
      " [ 0.55693852  0.98161561  0.99292066  0.52176748]\n",
      " [ 0.29706467  0.9454322   0.42369958  0.43260692]\n",
      " [ 0.72260511  0.36448166  0.82717561  0.63028276]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.9999896 ]\n",
      " [ 1.        ]\n",
      " [ 0.99999928]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.97522667]\n",
      " [ 0.40125279]\n",
      " [ 0.9035629 ]]\n",
      "New weights at layer  1 [[ 0.16265634  0.78804983  0.81275678  0.34274813  0.86220771]\n",
      " [-0.13369222 -0.37809146  0.46415467  0.26985695 -0.26392186]\n",
      " [ 0.25629053  0.20725519  0.46307153  0.29562697  0.30511909]]\n",
      "New weights at layer  0 [[ 0.34368592  0.84033969  0.90565369  0.72980669]\n",
      " [ 0.15361016  0.47249519  0.31018732  0.62261093]\n",
      " [ 0.55693852  0.98161561  0.99292066  0.52176748]\n",
      " [ 0.29706467  0.9454322   0.42369958  0.43260692]\n",
      " [ 0.72260511  0.36448166  0.82717561  0.63028276]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97488304]\n",
      " [ 0.42215262]\n",
      " [ 0.89936755]]\n",
      "New weights at layer  1 [[ 0.16026923  0.78566272  0.81036967  0.34036103  0.8598206 ]\n",
      " [-0.1439902  -0.38838944  0.45385669  0.25955896 -0.27421984]\n",
      " [ 0.25720131  0.20816597  0.46398231  0.29653775  0.30602987]]\n",
      "New weights at layer  0 [[ 0.34368592  0.84033969  0.90565369  0.72980669]\n",
      " [ 0.15361016  0.47249519  0.31018732  0.62261093]\n",
      " [ 0.55693852  0.98161561  0.99292066  0.52176748]\n",
      " [ 0.29706467  0.9454322   0.42369958  0.43260692]\n",
      " [ 0.72260511  0.36448166  0.82717561  0.63028276]]\n",
      "Iteration:  25\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975847]\n",
      " [ 0.99471861]\n",
      " [ 0.9996952 ]\n",
      " [ 0.99680505]\n",
      " [ 0.99897187]]\n",
      "activation at layer:  2 [[ 0.97437044]\n",
      " [ 0.40749505]\n",
      " [ 0.89962969]]\n",
      "New weights at layer  1 [[ 0.16033322  0.78572639  0.81043365  0.34042483  0.85988454]\n",
      " [-0.1538265  -0.39817616  0.44402101  0.24975172 -0.2840484 ]\n",
      " [ 0.24907996  0.20008557  0.45586148  0.28844039  0.29791492]]\n",
      "New weights at layer  0 [[ 0.3436858   0.84033945  0.90565333  0.72980621]\n",
      " [ 0.15362247  0.4725198   0.31022424  0.62266015]\n",
      " [ 0.55693608  0.98161072  0.99291333  0.52175771]\n",
      " [ 0.29704946  0.94540176  0.42365393  0.43254605]\n",
      " [ 0.72260555  0.36448254  0.82717693  0.63028453]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998961]\n",
      " [ 1.        ]\n",
      " [ 0.99999928]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.97453925]\n",
      " [ 0.39301412]\n",
      " [ 0.8953901 ]]\n",
      "New weights at layer  1 [[ 0.15791515  0.78330834  0.80801558  0.33800675  0.85746646]\n",
      " [-0.1393466  -0.38369642  0.4585009   0.2642316  -0.26956851]\n",
      " [ 0.24069314  0.19169884  0.44747466  0.28005358  0.2895281 ]]\n",
      "New weights at layer  0 [[ 0.3436858   0.84033945  0.90565333  0.72980621]\n",
      " [ 0.153622    0.47251924  0.31022358  0.6226594 ]\n",
      " [ 0.55693608  0.98161072  0.99291333  0.52175771]\n",
      " [ 0.29704946  0.94540177  0.42365393  0.43254605]\n",
      " [ 0.72260555  0.36448254  0.82717693  0.63028452]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97417698]\n",
      " [ 0.41391972]\n",
      " [ 0.89058229]]\n",
      "New weights at layer  1 [[ 0.15546449  0.78085768  0.80556492  0.33555609  0.85501581]\n",
      " [-0.14938789 -0.3937377   0.44845961  0.25419032 -0.2796098 ]\n",
      " [ 0.24175937  0.19276506  0.44854089  0.2811198   0.29059433]]\n",
      "New weights at layer  0 [[ 0.3436858   0.84033945  0.90565333  0.72980621]\n",
      " [ 0.153622    0.47251924  0.31022358  0.6226594 ]\n",
      " [ 0.55693608  0.98161072  0.99291333  0.52175771]\n",
      " [ 0.29704946  0.94540177  0.42365393  0.43254605]\n",
      " [ 0.72260555  0.36448254  0.82717693  0.63028452]]\n",
      "Iteration:  26\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975847]\n",
      " [ 0.99472057]\n",
      " [ 0.99969518]\n",
      " [ 0.99680354]\n",
      " [ 0.99897188]]\n",
      "activation at layer:  2 [[ 0.97364184]\n",
      " [ 0.39973259]\n",
      " [ 0.89097022]]\n",
      "New weights at layer  1 [[ 0.15553212  0.78092497  0.80563254  0.33562352  0.85508338]\n",
      " [-0.15897702 -0.40327851  0.4388711   0.24462953 -0.28919138]\n",
      " [ 0.23310637  0.18415567  0.43988844  0.27249238  0.28194813]]\n",
      "New weights at layer  0 [[ 0.34368568  0.84033922  0.90565298  0.72980574]\n",
      " [ 0.15363422  0.47254368  0.31026024  0.62270828]\n",
      " [ 0.55693365  0.98160587  0.99290605  0.521748  ]\n",
      " [ 0.29703454  0.94537193  0.42360918  0.43248638]\n",
      " [ 0.72260595  0.36448335  0.82717813  0.63028613]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998962]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.97381461]\n",
      " [ 0.38568646]\n",
      " [ 0.88607431]]\n",
      "New weights at layer  1 [[ 0.15304892  0.77844179  0.80314934  0.33314032  0.85260018]\n",
      " [-0.14442194 -0.38872358  0.45342618  0.2591846  -0.2746363 ]\n",
      " [ 0.22416175  0.17521114  0.43094381  0.26354777  0.27300351]]\n",
      "New weights at layer  0 [[ 0.34368568  0.84033922  0.90565298  0.72980574]\n",
      " [ 0.15363374  0.47254311  0.31025957  0.62270752]\n",
      " [ 0.55693365  0.98160587  0.99290605  0.521748  ]\n",
      " [ 0.29703454  0.94537193  0.42360918  0.43248638]\n",
      " [ 0.72260595  0.36448334  0.82717813  0.63028613]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97343221]\n",
      " [ 0.40657211]\n",
      " [ 0.8805437 ]]\n",
      "New weights at layer  1 [[ 0.15053143  0.77592431  0.80063186  0.33062284  0.8500827 ]\n",
      " [-0.15423135 -0.39853299  0.44361676  0.24937519 -0.28444572]\n",
      " [ 0.22541827  0.17646766  0.43220033  0.26480429  0.27426003]]\n",
      "New weights at layer  0 [[ 0.34368568  0.84033922  0.90565298  0.72980574]\n",
      " [ 0.15363374  0.47254311  0.31025957  0.62270752]\n",
      " [ 0.55693365  0.98160587  0.99290605  0.521748  ]\n",
      " [ 0.29703454  0.94537193  0.42360918  0.43248638]\n",
      " [ 0.72260595  0.36448334  0.82717813  0.63028613]]\n",
      "Iteration:  27\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975846]\n",
      " [ 0.99472253]\n",
      " [ 0.99969516]\n",
      " [ 0.99680207]\n",
      " [ 0.99897189]]\n",
      "activation at layer:  2 [[ 0.97287282]\n",
      " [ 0.3928112 ]\n",
      " [ 0.88109931]]\n",
      "New weights at layer  1 [[ 0.15060301  0.77599552  0.80070343  0.3306942   0.85015422]\n",
      " [-0.16359805 -0.40785251  0.43425065  0.24003619 -0.29380505]\n",
      " [ 0.21618981  0.16728568  0.42297246  0.25560312  0.26503883]]\n",
      "New weights at layer  0 [[ 0.34368557  0.840339    0.90565265  0.72980531]\n",
      " [ 0.15364599  0.4725676   0.31029631  0.6227565 ]\n",
      " [ 0.55693124  0.98160104  0.99289881  0.52173835]\n",
      " [ 0.29701993  0.9453427   0.42356534  0.43242793]\n",
      " [ 0.72260633  0.3644841   0.82717926  0.63028764]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998963]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.97304979]\n",
      " [ 0.37915733]\n",
      " [ 0.87542862]]\n",
      "New weights at layer  1 [[ 0.14805129  0.77344383  0.79815171  0.32814249  0.8476025 ]\n",
      " [-0.1489836  -0.39323821  0.44886511  0.25465063 -0.27919059]\n",
      " [ 0.20664297  0.15773894  0.41342562  0.24605628  0.25549199]]\n",
      "New weights at layer  0 [[ 0.34368557  0.840339    0.90565265  0.72980531]\n",
      " [ 0.15364551  0.47256702  0.31029564  0.62275573]\n",
      " [ 0.55693124  0.98160104  0.99289881  0.52173835]\n",
      " [ 0.29701993  0.94534271  0.42356534  0.43242793]\n",
      " [ 0.72260633  0.3644841   0.82717926  0.63028763]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.9726456 ]\n",
      " [ 0.40000508]\n",
      " [ 0.86904666]]\n",
      "New weights at layer  1 [[ 0.14546346  0.770856    0.79556388  0.32555465  0.84501467]\n",
      " [-0.15858376 -0.40283837  0.43926495  0.24505047 -0.28879076]\n",
      " [ 0.20813328  0.15922925  0.41491592  0.24754659  0.2569823 ]]\n",
      "New weights at layer  0 [[ 0.34368557  0.840339    0.90565265  0.72980531]\n",
      " [ 0.15364551  0.47256702  0.31029564  0.62275573]\n",
      " [ 0.55693124  0.98160104  0.99289881  0.52173835]\n",
      " [ 0.29701993  0.94534271  0.42356534  0.43242793]\n",
      " [ 0.72260633  0.3644841   0.82717926  0.63028763]]\n",
      "Iteration:  28\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975846]\n",
      " [ 0.99472449]\n",
      " [ 0.99969513]\n",
      " [ 0.99680062]\n",
      " [ 0.9989719 ]]\n",
      "activation at layer:  2 [[ 0.97206007]\n",
      " [ 0.38662945]\n",
      " [ 0.86982639]]\n",
      "New weights at layer  1 [[ 0.14553932  0.77093148  0.79563974  0.32563029  0.84509047]\n",
      " [-0.16775035 -0.41195881  0.43009893  0.235911   -0.29795014]\n",
      " [ 0.19828675  0.1494323   0.40507002  0.23772919  0.24714352]]\n",
      "New weights at layer  0 [[ 0.34368548  0.84033881  0.90565236  0.72980492]\n",
      " [ 0.15365792  0.47259183  0.31033285  0.62280535]\n",
      " [ 0.55692884  0.98159624  0.99289161  0.52172876]\n",
      " [ 0.29700564  0.94531413  0.42352248  0.43237078]\n",
      " [ 0.7226067   0.36448484  0.82718037  0.63028912]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998964]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.97224149]\n",
      " [ 0.37332902]\n",
      " [ 0.86324427]]\n",
      "New weights at layer  1 [[ 0.14291544  0.76830763  0.79301586  0.32300641  0.84246659]\n",
      " [-0.1530891  -0.39729771  0.44476018  0.25057223 -0.28328889]\n",
      " [ 0.18809584  0.13924149  0.39487911  0.22753829  0.23695261]]\n",
      "New weights at layer  0 [[ 0.34368548  0.84033881  0.90565236  0.72980492]\n",
      " [ 0.15365744  0.47259126  0.31033218  0.62280459]\n",
      " [ 0.55692884  0.98159624  0.99289161  0.52172876]\n",
      " [ 0.29700564  0.94531413  0.42352248  0.43237079]\n",
      " [ 0.7226067   0.36448484  0.82718037  0.63028912]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97181366]\n",
      " [ 0.39412674]\n",
      " [ 0.85586444]]\n",
      "New weights at layer  1 [[ 0.14025346  0.76564565  0.79035388  0.32034443  0.83980461]\n",
      " [-0.16250049 -0.4067091   0.4353488   0.24116085 -0.29270027]\n",
      " [ 0.1898739   0.14101956  0.39665717  0.22931635  0.23873067]]\n",
      "New weights at layer  0 [[ 0.34368548  0.84033881  0.90565236  0.72980492]\n",
      " [ 0.15365744  0.47259126  0.31033218  0.62280459]\n",
      " [ 0.55692884  0.98159624  0.99289161  0.52172876]\n",
      " [ 0.29700564  0.94531413  0.42352248  0.43237079]\n",
      " [ 0.7226067   0.36448484  0.82718037  0.63028912]]\n",
      "Iteration:  29\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975846]\n",
      " [ 0.99472647]\n",
      " [ 0.99969511]\n",
      " [ 0.99679921]\n",
      " [ 0.99897192]]\n",
      "activation at layer:  2 [[ 0.9711999 ]\n",
      " [ 0.38109896]\n",
      " [ 0.85694398]]\n",
      "New weights at layer  1 [[ 0.140334    0.76572578  0.79043441  0.32042473  0.83988508]\n",
      " [-0.17148702 -0.41565039  0.42636284  0.23220092 -0.30167973]\n",
      " [ 0.17937108  0.1305696   0.38615501  0.21884462  0.22823611]]\n",
      "New weights at layer  0 [[ 0.3436854   0.84033865  0.90565212  0.7298046 ]\n",
      " [ 0.15367016  0.47261671  0.31037036  0.6228555 ]\n",
      " [ 0.55692646  0.98159147  0.99288446  0.52171922]\n",
      " [ 0.29699173  0.94528631  0.42348075  0.43231514]\n",
      " [ 0.72260709  0.36448562  0.82718155  0.63029069]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998965]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.97138603]\n",
      " [ 0.36811679]\n",
      " [ 0.84929701]]\n",
      "New weights at layer  1 [[ 0.13763401  0.76302582  0.78773442  0.31772475  0.83718509]\n",
      " [-0.15678898 -0.40095251  0.44106087  0.24689894 -0.2869817 ]\n",
      " [ 0.16850079  0.11969942  0.37528473  0.20797434  0.21736582]]\n",
      "New weights at layer  0 [[ 0.3436854   0.84033865  0.90565212  0.7298046 ]\n",
      " [ 0.15366968  0.47261614  0.31036969  0.62285473]\n",
      " [ 0.55692646  0.98159147  0.99288446  0.52171922]\n",
      " [ 0.29699173  0.94528631  0.42348075  0.43231514]\n",
      " [ 0.72260709  0.36448562  0.82718155  0.63029068]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97093253]\n",
      " [ 0.38885673]\n",
      " [ 0.84075788]]\n",
      "New weights at layer  1 [[ 0.13489379  0.7602856   0.7849942   0.31498453  0.83444487]\n",
      " [-0.16603005 -0.41019358  0.4318198   0.23765787 -0.29622277]\n",
      " [ 0.17063279  0.12183142  0.37741672  0.21010633  0.21949782]]\n",
      "New weights at layer  0 [[ 0.3436854   0.84033865  0.90565212  0.7298046 ]\n",
      " [ 0.15366969  0.47261614  0.31036969  0.62285473]\n",
      " [ 0.55692646  0.98159147  0.99288446  0.52171922]\n",
      " [ 0.29699173  0.94528631  0.42348075  0.43231514]\n",
      " [ 0.72260709  0.36448562  0.82718155  0.63029068]]\n",
      "Iteration:  30\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975846]\n",
      " [ 0.9947285 ]\n",
      " [ 0.99969509]\n",
      " [ 0.99679784]\n",
      " [ 0.99897193]]\n",
      "activation at layer:  2 [[ 0.97028823]\n",
      " [ 0.37614281]\n",
      " [ 0.84223772]]\n",
      "New weights at layer  1 [[ 0.13497942  0.7603708   0.78507983  0.31506991  0.83453044]\n",
      " [-0.17485446 -0.4189736   0.42299595  0.22885959 -0.30504024]\n",
      " [ 0.1594444   0.11069932  0.36622904  0.19895108  0.20831823]]\n",
      "New weights at layer  0 [[ 0.34368534  0.84033854  0.90565195  0.72980437]\n",
      " [ 0.15368292  0.47264261  0.3104094   0.62290768]\n",
      " [ 0.55692409  0.98158674  0.99287736  0.52170975]\n",
      " [ 0.29697827  0.94525938  0.42344034  0.43226127]\n",
      " [ 0.72260753  0.36448651  0.82718288  0.63029246]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998966]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.97047938]\n",
      " [ 0.36344722]\n",
      " [ 0.83336063]]\n",
      "New weights at layer  1 [[ 0.13219908  0.75759049  0.78229949  0.31228957  0.8317501 ]\n",
      " [-0.1601276  -0.40424689  0.43772281  0.24358644 -0.29031338]\n",
      " [ 0.14787146  0.0991265   0.35465611  0.18737815  0.1967453 ]]\n",
      "New weights at layer  0 [[ 0.34368534  0.84033854  0.90565195  0.72980437]\n",
      " [ 0.15368245  0.47264204  0.31040874  0.62290692]\n",
      " [ 0.55692409  0.98158674  0.99287736  0.52170975]\n",
      " [ 0.29697827  0.94525938  0.42344035  0.43226127]\n",
      " [ 0.72260753  0.36448651  0.82718288  0.63029246]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.96999792]\n",
      " [ 0.3841249 ]\n",
      " [ 0.82349355]]\n",
      "New weights at layer  1 [[ 0.1293762   0.75476761  0.77947661  0.30946669  0.82892722]\n",
      " [-0.16921496 -0.41333424  0.42863545  0.23449909 -0.29940073]\n",
      " [ 0.15043701  0.10169206  0.35722166  0.1899437   0.19931085]]\n",
      "New weights at layer  0 [[ 0.34368534  0.84033854  0.90565195  0.72980437]\n",
      " [ 0.15368245  0.47264204  0.31040874  0.62290692]\n",
      " [ 0.55692409  0.98158674  0.99287736  0.52170975]\n",
      " [ 0.29697827  0.94525938  0.42344035  0.43226127]\n",
      " [ 0.72260753  0.36448651  0.82718288  0.63029246]]\n",
      "Iteration:  31\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975846]\n",
      " [ 0.99473062]\n",
      " [ 0.99969507]\n",
      " [ 0.9967965 ]\n",
      " [ 0.99897194]]\n",
      "activation at layer:  2 [[ 0.96932049]\n",
      " [ 0.37169413]\n",
      " [ 0.8255047 ]]\n",
      "New weights at layer  1 [[ 0.12946741  0.75485836  0.77956781  0.30955763  0.82901836]\n",
      " [-0.17789332 -0.42196896  0.41995765  0.22584644 -0.30807227]\n",
      " [ 0.13854876  0.08986359  0.34533416  0.17809067  0.18743195]]\n",
      "New weights at layer  0 [[ 0.34368532  0.84033849  0.90565189  0.72980429]\n",
      " [ 0.15369641  0.47266996  0.31045062  0.62296275]\n",
      " [ 0.55692175  0.98158206  0.99287033  0.52170038]\n",
      " [ 0.29696534  0.94523351  0.42340155  0.43220954]\n",
      " [ 0.72260807  0.36448758  0.82718448  0.6302946 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998967]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.96951698]\n",
      " [ 0.35925666]\n",
      " [ 0.81523073]]\n",
      "New weights at layer  1 [[ 0.12660212  0.7519931   0.77670252  0.30669234  0.82615307]\n",
      " [-0.16314396 -0.40721976  0.434707    0.24059578 -0.29332291]\n",
      " [ 0.12626898  0.07758393  0.33305438  0.1658109   0.17515217]]\n",
      "New weights at layer  0 [[ 0.34368532  0.84033849  0.90565189  0.72980429]\n",
      " [ 0.15369593  0.4726694   0.31044996  0.622962  ]\n",
      " [ 0.55692175  0.98158206  0.99287033  0.52170038]\n",
      " [ 0.29696534  0.94523352  0.42340155  0.43220955]\n",
      " [ 0.72260806  0.36448757  0.82718448  0.63029459]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.96900502]\n",
      " [ 0.37986998]\n",
      " [ 0.80387409]]\n",
      "New weights at layer  1 [[ 0.12369178  0.74908276  0.77379218  0.303782    0.82324273]\n",
      " [-0.17209252 -0.41616831  0.42575845  0.23164723 -0.30227146]\n",
      " [ 0.12936111  0.08067607  0.33614651  0.16890303  0.1782443 ]]\n",
      "New weights at layer  0 [[ 0.34368532  0.84033849  0.90565189  0.72980429]\n",
      " [ 0.15369594  0.4726694   0.31044996  0.622962  ]\n",
      " [ 0.55692175  0.98158206  0.99287033  0.52170038]\n",
      " [ 0.29696534  0.94523352  0.42340155  0.43220955]\n",
      " [ 0.72260806  0.36448757  0.82718448  0.63029459]]\n",
      "Iteration:  32\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975846]\n",
      " [ 0.99473285]\n",
      " [ 0.99969504]\n",
      " [ 0.99679522]\n",
      " [ 0.99897196]]\n",
      "activation at layer:  2 [[ 0.96829159]\n",
      " [ 0.36769471]\n",
      " [ 0.80658254]]\n",
      "New weights at layer  1 [[ 0.12378911  0.7491796   0.77388951  0.30387904  0.82333998]\n",
      " [-0.18063918 -0.42467201  0.41721233  0.2231259  -0.3108114 ]\n",
      " [ 0.11678088  0.06815908  0.32356708  0.15636009  0.16567397]]\n",
      "New weights at layer  0 [[ 0.34368534  0.84033853  0.90565195  0.72980437]\n",
      " [ 0.15371084  0.47269921  0.31049469  0.62302164]\n",
      " [ 0.55691944  0.98157745  0.99286342  0.52169116]\n",
      " [ 0.29695305  0.94520895  0.4233647   0.43216041]\n",
      " [ 0.72260873  0.36448891  0.82718649  0.63029728]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998968]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.9684938 ]\n",
      " [ 0.35548989]\n",
      " [ 0.79476072]]\n",
      "New weights at layer  1 [[ 0.12083389  0.74622441  0.77093429  0.30092383  0.82038476]\n",
      " [-0.16587237 -0.40990535  0.43197914  0.2378927  -0.29604459]\n",
      " [ 0.10381706  0.05519538  0.31060325  0.14339627  0.15271014]]\n",
      "New weights at layer  0 [[ 0.34368534  0.84033853  0.90565195  0.72980437]\n",
      " [ 0.15371038  0.47269866  0.31049404  0.6230209 ]\n",
      " [ 0.55691944  0.98157745  0.99286342  0.52169116]\n",
      " [ 0.29695306  0.94520895  0.4233647   0.43216041]\n",
      " [ 0.72260873  0.36448891  0.82718649  0.63029727]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.96794848]\n",
      " [ 0.37603848]\n",
      " [ 0.78178221]]\n",
      "New weights at layer  1 [[ 0.11783091  0.74322143  0.76793131  0.29792084  0.81738178]\n",
      " [-0.17469549 -0.41872847  0.42315601  0.22906958 -0.30486772]\n",
      " [ 0.10753983  0.05891815  0.31432602  0.14711904  0.15643291]]\n",
      "New weights at layer  0 [[ 0.34368534  0.84033853  0.90565195  0.72980437]\n",
      " [ 0.15371038  0.47269866  0.31049404  0.6230209 ]\n",
      " [ 0.55691944  0.98157745  0.99286342  0.52169116]\n",
      " [ 0.29695306  0.94520895  0.4233647   0.43216041]\n",
      " [ 0.72260873  0.36448891  0.82718649  0.63029727]]\n",
      "Iteration:  33\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975846]\n",
      " [ 0.99473524]\n",
      " [ 0.99969502]\n",
      " [ 0.99679401]\n",
      " [ 0.99897198]]\n",
      "activation at layer:  2 [[ 0.96719584]\n",
      " [ 0.36409385]\n",
      " [ 0.78538945]]\n",
      "New weights at layer  1 [[ 0.11793497  0.74332496  0.76803535  0.29802459  0.81748575]\n",
      " [-0.1831233  -0.42711394  0.41472874  0.22066675 -0.3132889 ]\n",
      " [ 0.09430506  0.04574988  0.3010921   0.13392352  0.14320856]]\n",
      "New weights at layer  0 [[ 0.34368541  0.84033868  0.90565217  0.72980467]\n",
      " [ 0.15372647  0.47273084  0.31054231  0.62308526]\n",
      " [ 0.55691719  0.98157293  0.99285665  0.52168213]\n",
      " [ 0.29694155  0.94518593  0.42333017  0.43211437]\n",
      " [ 0.72260959  0.36449062  0.82718904  0.63030068]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998969]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.96740417]\n",
      " [ 0.35209886]\n",
      " [ 0.77190932]]\n",
      "New weights at layer  1 [[ 0.11488442  0.74027445  0.76498481  0.29497404  0.81443521]\n",
      " [-0.16834304 -0.41233383  0.429509    0.235447   -0.29850864]\n",
      " [ 0.08071441  0.03215938  0.28750145  0.12033288  0.12961791]]\n",
      "New weights at layer  0 [[ 0.34368541  0.84033868  0.90565217  0.72980467]\n",
      " [ 0.15372602  0.47273029  0.31054167  0.62308453]\n",
      " [ 0.55691719  0.98157293  0.99285665  0.52168213]\n",
      " [ 0.29694155  0.94518593  0.42333018  0.43211437]\n",
      " [ 0.72260958  0.36449061  0.82718904  0.63030068]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.9668223 ]\n",
      " [ 0.37258361]\n",
      " [ 0.75723517]]\n",
      "New weights at layer  1 [[ 0.11178315  0.73717318  0.76188354  0.29187277  0.81133394]\n",
      " [-0.17705275 -0.42104353  0.4207993   0.2267373  -0.30721834]\n",
      " [ 0.08517716  0.03662213  0.2919642   0.12479563  0.13408066]]\n",
      "New weights at layer  0 [[ 0.34368541  0.84033868  0.90565217  0.72980467]\n",
      " [ 0.15372602  0.47273029  0.31054167  0.62308453]\n",
      " [ 0.55691719  0.98157293  0.99285665  0.52168213]\n",
      " [ 0.29694155  0.94518593  0.42333018  0.43211437]\n",
      " [ 0.72260958  0.36449061  0.82718904  0.63030068]]\n",
      "Iteration:  34\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975846]\n",
      " [ 0.99473781]\n",
      " [ 0.999695  ]\n",
      " [ 0.99679287]\n",
      " [ 0.99897201]]\n",
      "activation at layer:  2 [[ 0.96602683]\n",
      " [ 0.3608473 ]\n",
      " [ 0.76197114]]\n",
      "New weights at layer  1 [[ 0.11189462  0.73728409  0.761995    0.29198391  0.81144532]\n",
      " [-0.18537319 -0.4293222   0.41247938  0.21844154 -0.31553224]\n",
      " [ 0.07136054  0.02287489  0.27814846  0.11101999  0.12027491]]\n",
      "New weights at layer  0 [[ 0.34368555  0.84033896  0.90565259  0.72980522]\n",
      " [ 0.1537435   0.47276525  0.31059411  0.62315445]\n",
      " [ 0.55691499  0.98156855  0.99285007  0.52167336]\n",
      " [ 0.29693094  0.94516471  0.42329834  0.43207192]\n",
      " [ 0.72261067  0.36449278  0.82719229  0.63030501]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.9999897 ]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.96624175]\n",
      " [ 0.34904173]\n",
      " [ 0.74679209]]\n",
      "New weights at layer  1 [[ 0.10874287  0.73413237  0.75884325  0.28883217  0.80829357]\n",
      " [-0.17058267 -0.41453182  0.4272699   0.23323205 -0.30074171]\n",
      " [ 0.05723918  0.00875367  0.26402709  0.09689864  0.10615355]]\n",
      "New weights at layer  0 [[ 0.34368555  0.84033896  0.90565259  0.72980522]\n",
      " [ 0.15374306  0.47276472  0.31059349  0.62315374]\n",
      " [ 0.55691499  0.98156855  0.99285007  0.52167336]\n",
      " [ 0.29693094  0.94516471  0.42329834  0.43207193]\n",
      " [ 0.72261066  0.36449278  0.82719229  0.63030501]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.9656197 ]\n",
      " [ 0.36946438]\n",
      " [ 0.73043844]]\n",
      "New weights at layer  1 [[ 0.10553718  0.73092668  0.75563756  0.28562647  0.80508788]\n",
      " [-0.17918972 -0.42313888  0.41866285  0.22462499 -0.30934877]\n",
      " [ 0.06254679  0.01406129  0.26933471  0.10220626  0.11146116]]\n",
      "New weights at layer  0 [[ 0.34368555  0.84033896  0.90565259  0.72980522]\n",
      " [ 0.15374306  0.47276472  0.31059349  0.62315374]\n",
      " [ 0.55691499  0.98156855  0.99285007  0.52167336]\n",
      " [ 0.29693094  0.94516471  0.42329834  0.43207193]\n",
      " [ 0.72261066  0.36449278  0.82719229  0.63030501]]\n",
      "Iteration:  35\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975846]\n",
      " [ 0.99474062]\n",
      " [ 0.99969498]\n",
      " [ 0.99679181]\n",
      " [ 0.99897204]]\n",
      "activation at layer:  2 [[ 0.96477736]\n",
      " [ 0.35791635]\n",
      " [ 0.73654353]]\n",
      "New weights at layer  1 [[  1.05656840e-01   7.31045741e-01   7.55757214e-01   2.85745783e-01\n",
      "    8.05207447e-01]\n",
      " [ -1.87413093e-01  -4.31320976e-01   4.10439999e-01   2.16426026e-01\n",
      "   -3.17565674e-01]\n",
      " [  4.82578276e-02  -1.55959263e-04   2.55046650e-01   8.79596909e-02\n",
      "    9.71834366e-02]]\n",
      "New weights at layer  0 [[ 0.34368576  0.84033937  0.90565321  0.72980605]\n",
      " [ 0.15376209  0.47280278  0.31065058  0.62322986]\n",
      " [ 0.55691288  0.98156432  0.99284373  0.52166491]\n",
      " [ 0.29692134  0.94514551  0.42326953  0.43203352]\n",
      " [ 0.72261202  0.36449549  0.82719635  0.63031043]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998972]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.96499937]\n",
      " [ 0.34628192]\n",
      " [ 0.71972196]]\n",
      "New weights at layer  1 [[ 0.1023975   0.72778643  0.75249787  0.28248644  0.80194811]\n",
      " [-0.17261483 -0.41652286  0.42523826  0.23122428 -0.30276741]\n",
      " [ 0.03373943 -0.0146742   0.24052826  0.07344131  0.08266504]]\n",
      "New weights at layer  0 [[ 0.34368576  0.84033937  0.90565321  0.72980605]\n",
      " [ 0.15376166  0.47280227  0.31064998  0.62322918]\n",
      " [ 0.55691288  0.98156432  0.99284373  0.52166491]\n",
      " [ 0.29692134  0.94514551  0.42326954  0.43203352]\n",
      " [ 0.72261202  0.36449549  0.82719635  0.63031043]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.96433308]\n",
      " [ 0.36664482]\n",
      " [ 0.70181855]]\n",
      "New weights at layer  1 [[ 0.0990807   0.72446963  0.74918107  0.27916964  0.7986313 ]\n",
      " [-0.18112892 -0.42503696  0.41672417  0.22271019 -0.3112815 ]\n",
      " [ 0.03997946 -0.00843418  0.24676828  0.07968133  0.08890506]]\n",
      "New weights at layer  0 [[ 0.34368576  0.84033937  0.90565321  0.72980605]\n",
      " [ 0.15376166  0.47280227  0.31064998  0.62322918]\n",
      " [ 0.55691288  0.98156432  0.99284373  0.52166491]\n",
      " [ 0.29692134  0.94514551  0.42326954  0.43203352]\n",
      " [ 0.72261202  0.36449549  0.82719635  0.63031043]]\n",
      "Iteration:  36\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975847]\n",
      " [ 0.99474367]\n",
      " [ 0.99969496]\n",
      " [ 0.99679086]\n",
      " [ 0.99897209]]\n",
      "activation at layer:  2 [[ 0.96343928]\n",
      " [ 0.35526707]\n",
      " [ 0.70951406]]\n",
      "New weights at layer  1 [[ 0.09920945  0.72459773  0.74930981  0.27929801  0.79875995]\n",
      " [-0.18926443 -0.43313166  0.40858918  0.21459882 -0.31941062]\n",
      " [ 0.02535963 -0.02298068  0.23214938  0.0651049   0.07429674]]\n",
      "New weights at layer  0 [[ 0.34368604  0.84033994  0.90565407  0.72980719]\n",
      " [ 0.15378233  0.47284362  0.31071201  0.62331187]\n",
      " [ 0.55691086  0.98156028  0.99283767  0.52165683]\n",
      " [ 0.29691283  0.94512848  0.42324399  0.43199946]\n",
      " [ 0.72261368  0.3644988   0.82720133  0.63031706]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998973]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.96366899]\n",
      " [ 0.34378744]\n",
      " [ 0.69121799]]\n",
      "New weights at layer  1 [[ 0.09583554  0.72122386  0.7459359   0.2759241   0.79538604]\n",
      " [-0.17446043 -0.41832781  0.42339318  0.22940281 -0.30460662]\n",
      " [ 0.01060657 -0.03773358  0.21739632  0.05035185  0.05954368]]\n",
      "New weights at layer  0 [[ 0.34368604  0.84033994  0.90565407  0.72980719]\n",
      " [ 0.15378192  0.47284312  0.31071143  0.62331121]\n",
      " [ 0.55691086  0.98156028  0.99283767  0.52165683]\n",
      " [ 0.29691283  0.94512849  0.423244    0.43199947]\n",
      " [ 0.72261368  0.3644988   0.82720133  0.63031706]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.96295383]\n",
      " [ 0.36409331]\n",
      " [ 0.67201247]]\n",
      "New weights at layer  1 [[ 0.09240032  0.71778864  0.74250069  0.27248889  0.79195083]\n",
      " [-0.18289026 -0.42675764  0.41496335  0.22097298 -0.31303644]\n",
      " [ 0.0178358  -0.03050435  0.22462555  0.05758108  0.06677291]]\n",
      "New weights at layer  0 [[ 0.34368604  0.84033994  0.90565407  0.72980719]\n",
      " [ 0.15378192  0.47284312  0.31071143  0.62331121]\n",
      " [ 0.55691086  0.98156028  0.99283767  0.52165683]\n",
      " [ 0.29691283  0.94512849  0.423244    0.43199947]\n",
      " [ 0.72261368  0.3644988   0.82720133  0.63031706]]\n",
      "Iteration:  37\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975847]\n",
      " [ 0.99474699]\n",
      " [ 0.99969494]\n",
      " [ 0.99679002]\n",
      " [ 0.99897214]]\n",
      "activation at layer:  2 [[ 0.96200339]\n",
      " [ 0.35286968]\n",
      " [ 0.68146459]]\n",
      "New weights at layer  1 [[ 0.09253918  0.7179268   0.74263953  0.27262733  0.79208957]\n",
      " [-0.19094619 -0.43477319  0.40690793  0.21294098 -0.32108604]\n",
      " [ 0.00304678 -0.04521924  0.20983747  0.04283597  0.05199552]]\n",
      "New weights at layer  0 [[ 0.34368641  0.84034067  0.90565516  0.72980865]\n",
      " [ 0.15380424  0.47288777  0.31077839  0.6234005 ]\n",
      " [ 0.55690894  0.98155645  0.99283192  0.52164917]\n",
      " [ 0.29690544  0.94511369  0.42322181  0.43196988]\n",
      " [ 0.72261565  0.36450276  0.82720727  0.63032498]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998975]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.96224145]\n",
      " [ 0.34153016]\n",
      " [ 0.66196652]]\n",
      "New weights at layer  1 [[ 0.08904308  0.71443074  0.73914344  0.26913123  0.78859348]\n",
      " [-0.17613804 -0.41996519  0.42171608  0.22774912 -0.30627789]\n",
      " [-0.01176584 -0.06003171  0.19502485  0.02802337  0.03718291]]\n",
      "New weights at layer  0 [[ 0.34368641  0.84034067  0.90565516  0.72980865]\n",
      " [ 0.15380384  0.47288729  0.31077783  0.62339986]\n",
      " [ 0.55690894  0.98155645  0.99283192  0.52164917]\n",
      " [ 0.29690544  0.9451137   0.42322182  0.43196989]\n",
      " [ 0.72261565  0.36450276  0.82720726  0.63032498]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.96147216]\n",
      " [ 0.36178196]\n",
      " [ 0.64180203]]\n",
      "New weights at layer  1 [[ 0.08548145  0.71086912  0.73558181  0.26556961  0.78503185]\n",
      " [-0.18449143 -0.42831858  0.41336269  0.21939572 -0.31463128]\n",
      " [-0.00353115 -0.05179701  0.20325955  0.03625806  0.0454176 ]]\n",
      "New weights at layer  0 [[ 0.34368641  0.84034067  0.90565516  0.72980865]\n",
      " [ 0.15380384  0.47288729  0.31077783  0.62339986]\n",
      " [ 0.55690894  0.98155645  0.99283192  0.52164917]\n",
      " [ 0.29690544  0.9451137   0.42322182  0.43196989]\n",
      " [ 0.72261565  0.36450276  0.82720726  0.63032498]]\n",
      "Iteration:  38\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975847]\n",
      " [ 0.99475057]\n",
      " [ 0.99969492]\n",
      " [ 0.99678928]\n",
      " [ 0.9989722 ]]\n",
      "activation at layer:  2 [[ 0.9604592 ]\n",
      " [ 0.35069793]\n",
      " [ 0.65309007]]\n",
      "New weights at layer  1 [[ 0.08563158  0.71101849  0.73573193  0.26571929  0.78518186]\n",
      " [-0.19247521 -0.43626236  0.40537942  0.21143566 -0.32260877]\n",
      " [-0.0183242  -0.06651597  0.18846743  0.02150893  0.03063617]]\n",
      "New weights at layer  0 [[ 0.34368685  0.84034155  0.90565648  0.72981041]\n",
      " [ 0.15382773  0.47293506  0.3108495   0.62349542]\n",
      " [ 0.55690714  0.98155284  0.99282651  0.52164195]\n",
      " [ 0.29689915  0.94510111  0.42320294  0.43194472]\n",
      " [ 0.72261795  0.36450736  0.82721417  0.63033418]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998976]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.96070638]\n",
      " [ 0.33948537]\n",
      " [ 0.63273582]]\n",
      "New weights at layer  1 [[ 0.08200495  0.7073919   0.7321053   0.26209266  0.78155523]\n",
      " [-0.17766415 -0.42145146  0.42019048  0.2262467  -0.30779772]\n",
      " [-0.0330278  -0.08121941  0.17376384  0.00680535  0.01593258]]\n",
      "New weights at layer  0 [[ 0.34368685  0.84034155  0.90565648  0.72981041]\n",
      " [ 0.15382734  0.4729346   0.31084895  0.62349479]\n",
      " [ 0.55690714  0.98155284  0.99282651  0.52164195]\n",
      " [ 0.29689916  0.94510112  0.42320295  0.43194473]\n",
      " [ 0.72261795  0.36450736  0.82721416  0.63033418]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.95987696]\n",
      " [ 0.35968616]\n",
      " [ 0.61200536]]\n",
      "New weights at layer  1 [[ 0.07830816  0.70369511  0.72840851  0.25839587  0.77785844]\n",
      " [-0.18594816 -0.42973547  0.41190647  0.2179627  -0.31608173]\n",
      " [-0.02381468 -0.07200629  0.18297695  0.01601847  0.0251457 ]]\n",
      "New weights at layer  0 [[ 0.34368685  0.84034155  0.90565648  0.72981041]\n",
      " [ 0.15382734  0.4729346   0.31084895  0.62349479]\n",
      " [ 0.55690714  0.98155284  0.99282651  0.52164195]\n",
      " [ 0.29689916  0.94510112  0.42320295  0.43194473]\n",
      " [ 0.72261795  0.36450736  0.82721416  0.63033418]]\n",
      "Iteration:  39\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975847]\n",
      " [ 0.99475441]\n",
      " [ 0.99969491]\n",
      " [ 0.99678866]\n",
      " [ 0.99897227]]\n",
      "activation at layer:  2 [[ 0.95879478]\n",
      " [ 0.34872865]\n",
      " [ 0.62510616]]\n",
      "New weights at layer  1 [[ 0.07847091  0.70385705  0.72857125  0.25855814  0.77802106]\n",
      " [-0.19386646 -0.43761414  0.40398867  0.21006791 -0.32399381]\n",
      " [-0.0384604  -0.08657872  0.16833216  0.00141625  0.01051149]]\n",
      "New weights at layer  0 [[ 0.34368736  0.84034257  0.90565801  0.72981245]\n",
      " [ 0.15385264  0.4729852   0.31092486  0.623596  ]\n",
      " [ 0.55690545  0.98154946  0.99282143  0.52163518]\n",
      " [ 0.2968939   0.94509061  0.42318718  0.4319237 ]\n",
      " [ 0.72262056  0.36451257  0.82722198  0.6303446 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998978]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.95905197]\n",
      " [ 0.33763124]\n",
      " [ 0.60426797]]\n",
      "New weights at layer  1 [[ 0.07470459  0.70009076  0.72480493  0.25479182  0.77425474]\n",
      " [-0.17905349 -0.42280132  0.41880164  0.22488088 -0.30918083]\n",
      " [-0.05291015 -0.10102832  0.15388241 -0.01303349 -0.00393826]]\n",
      "New weights at layer  0 [[ 0.34368736  0.84034257  0.90565801  0.72981245]\n",
      " [ 0.15385226  0.47298474  0.31092433  0.62359539]\n",
      " [ 0.55690545  0.98154946  0.99282143  0.52163518]\n",
      " [ 0.29689391  0.94509062  0.42318719  0.43192371]\n",
      " [ 0.72262056  0.36451257  0.82722198  0.6303446 ]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.95815555]\n",
      " [ 0.35778411]\n",
      " [ 0.58335983]]\n",
      "New weights at layer  1 [[ 0.07086301  0.69624918  0.72096335  0.25095024  0.77041316]\n",
      " [-0.18727446 -0.43102229  0.41058067  0.21665991 -0.3174018 ]\n",
      " [-0.04278367 -0.09090183  0.16400889 -0.002907    0.00618823]]\n",
      "New weights at layer  0 [[ 0.34368736  0.84034257  0.90565801  0.72981245]\n",
      " [ 0.15385226  0.47298474  0.31092433  0.62359539]\n",
      " [ 0.55690545  0.98154946  0.99282143  0.52163518]\n",
      " [ 0.29689391  0.94509062  0.42318719  0.43192371]\n",
      " [ 0.72262056  0.36451257  0.82722198  0.6303446 ]]\n",
      "Iteration:  40\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975848]\n",
      " [ 0.99475847]\n",
      " [ 0.99969489]\n",
      " [ 0.99678814]\n",
      " [ 0.99897236]]\n",
      "activation at layer:  2 [[ 0.95699646]\n",
      " [ 0.34694136]\n",
      " [ 0.59815414]]\n",
      "New weights at layer  1 [[ 0.07103995  0.69642523  0.72114027  0.25112665  0.77058996]\n",
      " [-0.19513332 -0.43884184  0.40272231  0.2088244  -0.32525448]\n",
      " [-0.05715777 -0.10520405  0.1496357  -0.0172384  -0.00817458]]\n",
      "New weights at layer  0 [[ 0.34368793  0.84034372  0.90565972  0.72981473]\n",
      " [ 0.15387878  0.47303778  0.31100388  0.62370146]\n",
      " [ 0.55690387  0.98154629  0.99281669  0.52162885]\n",
      " [ 0.29688959  0.94508198  0.42317423  0.43190644]\n",
      " [ 0.72262344  0.36451834  0.82723064  0.63035614]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.9999898 ]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.95726468]\n",
      " [ 0.33594847]\n",
      " [ 0.57718301]]\n",
      "New weights at layer  1 [[ 0.06712387  0.6925092   0.7172242   0.24721058  0.76667388]\n",
      " [-0.18031919 -0.42402786  0.41753645  0.22363852 -0.31044035]\n",
      " [-0.07124351 -0.11928964  0.13554997 -0.03132413 -0.02226031]]\n",
      "New weights at layer  0 [[ 0.34368793  0.84034372  0.90565972  0.72981473]\n",
      " [ 0.1538784   0.47303733  0.31100335  0.62370086]\n",
      " [ 0.55690387  0.98154629  0.99281669  0.52162885]\n",
      " [ 0.2968896   0.94508199  0.42317424  0.43190645]\n",
      " [ 0.72262344  0.36451834  0.82723063  0.63035614]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.95629338]\n",
      " [ 0.35605647]\n",
      " [ 0.55643466]]\n",
      "New weights at layer  1 [[ 0.06312691  0.68851224  0.71322724  0.24321362  0.76267693]\n",
      " [-0.18848286 -0.43219153  0.40937277  0.21547485 -0.31860402]\n",
      " [-0.06029564 -0.10834178  0.14649783 -0.02037626 -0.01131245]]\n",
      "New weights at layer  0 [[ 0.34368793  0.84034372  0.90565972  0.72981473]\n",
      " [ 0.1538784   0.47303733  0.31100335  0.62370086]\n",
      " [ 0.55690387  0.98154629  0.99281669  0.52162885]\n",
      " [ 0.2968896   0.94508199  0.42317424  0.43190645]\n",
      " [ 0.72262344  0.36451834  0.82723063  0.63035614]]\n",
      "Iteration:  41\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975848]\n",
      " [ 0.99476273]\n",
      " [ 0.99969487]\n",
      " [ 0.99678771]\n",
      " [ 0.99897245]]\n",
      "activation at layer:  2 [[ 0.95504857]\n",
      " [ 0.34531789]\n",
      " [ 0.57273193]]\n",
      "New weights at layer  1 [[ 0.06331985  0.68870421  0.71342016  0.24340598  0.76286971]\n",
      " [-0.19628769 -0.43995737  0.40156844  0.2076932  -0.32640272]\n",
      " [-0.07430759 -0.1222837   0.13248678 -0.03434657 -0.02531337]]\n",
      "New weights at layer  0 [[ 0.34368855  0.84034496  0.9056616   0.72981723]\n",
      " [ 0.15390592  0.47309236  0.3110859   0.62381092]\n",
      " [ 0.55690238  0.98154333  0.99281224  0.52162293]\n",
      " [ 0.2968861   0.94507499  0.42316374  0.43189245]\n",
      " [ 0.72262657  0.3645246   0.82724003  0.63036866]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998982]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.95532904]\n",
      " [ 0.33441993]\n",
      " [ 0.55192254]]\n",
      "New weights at layer  1 [[ 0.05924294  0.68462734  0.70934325  0.23932908  0.7587928 ]\n",
      " [-0.181473   -0.42514282  0.41638313  0.22250789 -0.31158802]\n",
      " [-0.08795685 -0.13593283  0.11883751 -0.04799583 -0.03896264]]\n",
      "New weights at layer  0 [[ 0.34368855  0.84034496  0.9056616   0.72981723]\n",
      " [ 0.15390555  0.47309192  0.31108538  0.62381033]\n",
      " [ 0.55690238  0.98154333  0.99281224  0.52162293]\n",
      " [ 0.29688611  0.94507501  0.42316376  0.43189247]\n",
      " [ 0.72262657  0.3645246   0.82724003  0.63036866]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.95427375]\n",
      " [ 0.35448607]\n",
      " [ 0.53159407]]\n",
      "New weights at layer  1 [[ 0.05507893  0.68046333  0.70517924  0.23516507  0.75462879]\n",
      " [-0.18958455 -0.43325437  0.40827158  0.21439634 -0.31969958]\n",
      " [-0.07629346 -0.12426944  0.13050091 -0.03633243 -0.02729925]]\n",
      "New weights at layer  0 [[ 0.34368855  0.84034496  0.9056616   0.72981723]\n",
      " [ 0.15390555  0.47309192  0.31108538  0.62381033]\n",
      " [ 0.55690238  0.98154333  0.99281224  0.52162293]\n",
      " [ 0.29688611  0.94507501  0.42316376  0.43189247]\n",
      " [ 0.72262657  0.3645246   0.82724003  0.63036866]]\n",
      "Iteration:  42\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975849]\n",
      " [ 0.99476715]\n",
      " [ 0.99969486]\n",
      " [ 0.99678736]\n",
      " [ 0.99897255]]\n",
      "activation at layer:  2 [[ 0.95293306]\n",
      " [ 0.34384208]\n",
      " [ 0.54916534]]\n",
      "New weights at layer  1 [[ 0.05528998  0.68067333  0.70539028  0.23537549  0.75483968]\n",
      " [-0.19734026 -0.44097136  0.40051636  0.20666368 -0.32744919]\n",
      " [-0.08988656 -0.13779468  0.11690867 -0.04988514 -0.04088166]]\n",
      "New weights at layer  0 [[ 0.34368922  0.8403463   0.9056636   0.7298199 ]\n",
      " [ 0.15393386  0.47314853  0.31117031  0.62392356]\n",
      " [ 0.556901    0.98154056  0.99280808  0.52161738]\n",
      " [ 0.29688331  0.9450694   0.42315535  0.43188126]\n",
      " [ 0.72262991  0.36453128  0.82725005  0.63038203]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998984]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.95322719]\n",
      " [ 0.33303044]\n",
      " [ 0.52873943]]\n",
      "New weights at layer  1 [[ 0.05104001  0.6764234   0.70114031  0.23112552  0.7505897 ]\n",
      " [-0.18252545 -0.4261567   0.41533117  0.22147847 -0.31263438]\n",
      " [-0.10306138 -0.15096936  0.10373385 -0.06305995 -0.05405648]]\n",
      "New weights at layer  0 [[ 0.34368922  0.8403463   0.9056636   0.7298199 ]\n",
      " [ 0.15393349  0.47314809  0.31116979  0.62392298]\n",
      " [ 0.556901    0.98154056  0.99280808  0.52161738]\n",
      " [ 0.29688332  0.94506941  0.42315537  0.43188128]\n",
      " [ 0.72262991  0.36453128  0.82725005  0.63038202]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.95207738]\n",
      " [ 0.35305757]\n",
      " [ 0.50900762]]\n",
      "New weights at layer  1 [[ 0.04669605  0.67207945  0.69679635  0.22678157  0.74624575]\n",
      " [-0.19058957 -0.43422082  0.40726706  0.21341436 -0.3206985 ]\n",
      " [-0.09079055 -0.13869853  0.11600468 -0.05078912 -0.04178565]]\n",
      "New weights at layer  0 [[ 0.34368922  0.8403463   0.9056636   0.7298199 ]\n",
      " [ 0.15393349  0.47314809  0.31116979  0.62392298]\n",
      " [ 0.556901    0.98154056  0.99280808  0.52161738]\n",
      " [ 0.29688332  0.94506941  0.42315537  0.43188128]\n",
      " [ 0.72262991  0.36453128  0.82725005  0.63038202]]\n",
      "Iteration:  43\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975849]\n",
      " [ 0.99477169]\n",
      " [ 0.99969485]\n",
      " [ 0.99678709]\n",
      " [ 0.99897266]]\n",
      "activation at layer:  2 [[ 0.95062906]\n",
      " [ 0.34249955]\n",
      " [ 0.52761615]]\n",
      "New weights at layer  1 [[ 0.04692771  0.67230995  0.697028    0.22701254  0.74647723]\n",
      " [-0.19830057 -0.44189337  0.39955654  0.20572627 -0.32840344]\n",
      " [-0.10393754 -0.15177995  0.10285853 -0.06389704 -0.05492231]]\n",
      "New weights at layer  0 [[ 0.34368992  0.8403477   0.9056657   0.72982271]\n",
      " [ 0.15396241  0.47320593  0.31125655  0.62403865]\n",
      " [ 0.55689969  0.98153795  0.99280417  0.52161217]\n",
      " [ 0.2968811   0.94506497  0.4231487   0.43187239]\n",
      " [ 0.72263343  0.36453832  0.8272606   0.6303961 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998986]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.95093855]\n",
      " [ 0.33176645]\n",
      " [ 0.5077223 ]]\n",
      "New weights at layer  1 [[ 0.04249116  0.66787345  0.69259145  0.22257599  0.74204068]\n",
      " [-0.18348601 -0.42707895  0.41437111  0.22054083 -0.31358888]\n",
      " [-0.11662757 -0.16446985  0.0901685  -0.07658706 -0.06761234]]\n",
      "New weights at layer  0 [[ 0.34368992  0.8403477   0.9056657   0.72982271]\n",
      " [ 0.15396205  0.47320549  0.31125604  0.62403806]\n",
      " [ 0.55689969  0.98153795  0.99280417  0.52161217]\n",
      " [ 0.29688111  0.94506498  0.42314872  0.43187241]\n",
      " [ 0.72263343  0.36453832  0.8272606   0.63039609]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.94968197]\n",
      " [ 0.35175733]\n",
      " [ 0.48868943]]\n",
      "New weights at layer  1 [[ 0.037953    0.66333529  0.68805329  0.21803783  0.73750251]\n",
      " [-0.19150692 -0.43509986  0.40635019  0.21251991 -0.32160979]\n",
      " [-0.10385135 -0.15169362  0.10294472 -0.06381083 -0.05483611]]\n",
      "New weights at layer  0 [[ 0.34368992  0.8403477   0.9056657   0.72982271]\n",
      " [ 0.15396205  0.47320549  0.31125603  0.62403806]\n",
      " [ 0.55689969  0.98153795  0.99280417  0.52161217]\n",
      " [ 0.29688111  0.94506498  0.42314872  0.43187241]\n",
      " [ 0.72263343  0.36453832  0.8272606   0.63039609]]\n",
      "Iteration:  44\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.9997585 ]\n",
      " [ 0.99477632]\n",
      " [ 0.99969483]\n",
      " [ 0.99678687]\n",
      " [ 0.99897277]]\n",
      "activation at layer:  2 [[ 0.94811236]\n",
      " [ 0.34127748]\n",
      " [ 0.50811298]]\n",
      "New weights at layer  1 [[ 0.0382082   0.66358921  0.68830847  0.21829227  0.73775752]\n",
      " [-0.19917723 -0.44273195  0.39868037  0.2048724  -0.32927407]\n",
      " [-0.11654776 -0.16432677  0.09024912 -0.07646951 -0.06752255]]\n",
      "New weights at layer  0 [[ 0.34369065  0.84034916  0.90566789  0.72982562]\n",
      " [ 0.15399142  0.47326424  0.31134416  0.62415556]\n",
      " [ 0.55689847  0.98153549  0.99280048  0.52160725]\n",
      " [ 0.29687936  0.94506149  0.42314348  0.43186543]\n",
      " [ 0.72263709  0.36454565  0.8272716   0.63041076]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998988]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.94843924]\n",
      " [ 0.33061587]\n",
      " [ 0.48883669]]\n",
      "New weights at layer  1 [[ 0.03357012  0.65895118  0.68367039  0.2136542   0.73311943]\n",
      " [-0.18436316 -0.42791802  0.41349445  0.21968646 -0.31446   ]\n",
      " [-0.12876259 -0.17654147  0.07803429 -0.08868432 -0.07973737]]\n",
      "New weights at layer  0 [[ 0.34369065  0.84034916  0.90566789  0.72982562]\n",
      " [ 0.15399105  0.4732638   0.31134365  0.62415498]\n",
      " [ 0.55689847  0.98153549  0.99280048  0.52160725]\n",
      " [ 0.29687938  0.94506151  0.4231435   0.43186544]\n",
      " [ 0.72263709  0.36454565  0.8272716   0.63041076]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.9470616 ]\n",
      " [ 0.35057312]\n",
      " [ 0.47054634]]\n",
      "New weights at layer  1 [[ 0.02882194  0.654203    0.67892221  0.20890601  0.72837125]\n",
      " [-0.19234471 -0.43589958  0.40551289  0.21170491 -0.32244155]\n",
      " [-0.11557218 -0.16335106  0.0912247  -0.07549391 -0.06654696]]\n",
      "New weights at layer  0 [[ 0.34369065  0.84034916  0.90566789  0.72982562]\n",
      " [ 0.15399105  0.4732638   0.31134365  0.62415498]\n",
      " [ 0.55689847  0.98153549  0.99280048  0.52160725]\n",
      " [ 0.29687938  0.94506151  0.4231435   0.43186544]\n",
      " [ 0.72263709  0.36454565  0.8272716   0.63041076]]\n",
      "Iteration:  45\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.9997585 ]\n",
      " [ 0.99478102]\n",
      " [ 0.99969482]\n",
      " [ 0.99678669]\n",
      " [ 0.99897288]]\n",
      "activation at layer:  2 [[ 0.94535477]\n",
      " [ 0.34016439]\n",
      " [ 0.49058981]]\n",
      "New weights at layer  1 [[ 0.02910417  0.65448382  0.67920442  0.2091874   0.72865326]\n",
      " [-0.19997794 -0.44349481  0.39788014  0.20409436 -0.33006879]\n",
      " [-0.12782962 -0.17554747  0.07896804 -0.08771492 -0.07879477]]\n",
      "New weights at layer  0 [[ 0.3436914   0.84035066  0.90567014  0.72982862]\n",
      " [ 0.15402077  0.47332323  0.31143279  0.62427383]\n",
      " [ 0.5568973   0.98153316  0.99279699  0.5216026 ]\n",
      " [ 0.29687802  0.94505879  0.42313943  0.43186002]\n",
      " [ 0.72264088  0.36455322  0.82728296  0.63042591]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.9999899 ]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.9457015 ]\n",
      " [ 0.3295679 ]\n",
      " [ 0.47196777]]\n",
      "New weights at layer  1 [[ 0.02424797  0.64962767  0.67434822  0.20433121  0.72379706]\n",
      " [-0.18516455 -0.42868156  0.41269354  0.21890775 -0.3152554 ]\n",
      " [-0.13959172 -0.18730946  0.06720594 -0.09947702 -0.09055688]]\n",
      "New weights at layer  0 [[ 0.3436914   0.84035066  0.90567014  0.72982862]\n",
      " [ 0.1540204   0.47332278  0.31143227  0.62427324]\n",
      " [ 0.5568973   0.98153316  0.99279699  0.5216026 ]\n",
      " [ 0.29687803  0.94505881  0.42313944  0.43186004]\n",
      " [ 0.72264088  0.36455322  0.82728296  0.63042591]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.94418608]\n",
      " [ 0.34949399]\n",
      " [ 0.45442163]]\n",
      "New weights at layer  1 [[ 0.01927223  0.64465193  0.66937248  0.19935547  0.71882132]\n",
      " [-0.19311023 -0.43662724  0.40474786  0.21096207 -0.32320107]\n",
      " [-0.1260656  -0.17378334  0.08073206 -0.08595089 -0.07703076]]\n",
      "New weights at layer  0 [[ 0.3436914   0.84035066  0.90567014  0.72982862]\n",
      " [ 0.1540204   0.47332278  0.31143227  0.62427324]\n",
      " [ 0.5568973   0.98153316  0.99279699  0.5216026 ]\n",
      " [ 0.29687803  0.94505881  0.42313944  0.43186004]\n",
      " [ 0.72264088  0.36455322  0.82728296  0.63042591]]\n",
      "Iteration:  46\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975851]\n",
      " [ 0.99478577]\n",
      " [ 0.99969481]\n",
      " [ 0.99678656]\n",
      " [ 0.998973  ]]\n",
      "activation at layer:  2 [[ 0.94232337]\n",
      " [ 0.33914999]\n",
      " [ 0.47492181]]\n",
      "New weights at layer  1 [[ 0.01958563  0.64496377  0.66968586  0.19966794  0.71913447]\n",
      " [-0.20070967 -0.44418888  0.3971489   0.20338522 -0.33079454]\n",
      " [-0.13790592 -0.18556476  0.06889249 -0.09775601 -0.08886177]]\n",
      "New weights at layer  0 [[ 0.34369217  0.84035219  0.90567243  0.72983168]\n",
      " [ 0.15405036  0.47338271  0.31152215  0.62439308]\n",
      " [ 0.5568962   0.98153095  0.99279368  0.52159817]\n",
      " [ 0.29687699  0.94505672  0.42313631  0.43185586]\n",
      " [ 0.72264477  0.364561    0.82729463  0.63044147]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998993]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.94269293]\n",
      " [ 0.32861288]\n",
      " [ 0.45695608]]\n",
      "New weights at layer  1 [[ 0.01449292  0.63987112  0.66459315  0.19457523  0.71404177]\n",
      " [-0.18589709 -0.42937645  0.41196148  0.21819779 -0.31598197]\n",
      " [-0.14924516 -0.19690389  0.05755326 -0.10909524 -0.10020101]]\n",
      "New weights at layer  0 [[ 0.34369217  0.84035219  0.90567243  0.72983168]\n",
      " [ 0.15404999  0.47338226  0.31152163  0.62439249]\n",
      " [ 0.5568962   0.98153095  0.99279368  0.52159817]\n",
      " [ 0.296877    0.94505674  0.42313633  0.43185588]\n",
      " [ 0.72264477  0.364561    0.82729463  0.63044147]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.94102006]\n",
      " [ 0.34851013]\n",
      " [ 0.4401286 ]]\n",
      "New weights at layer  1 [[ 0.00927014  0.63464833  0.65937037  0.18935245  0.70881898]\n",
      " [-0.19381004 -0.4372894   0.40404853  0.21028484 -0.32389492]\n",
      " [-0.13544906 -0.18310779  0.07134935 -0.09529915 -0.08640491]]\n",
      "New weights at layer  0 [[ 0.34369217  0.84035219  0.90567243  0.72983168]\n",
      " [ 0.15404999  0.47338226  0.31152163  0.62439249]\n",
      " [ 0.5568962   0.98153095  0.99279368  0.52159817]\n",
      " [ 0.296877    0.94505674  0.42313633  0.43185588]\n",
      " [ 0.72264477  0.364561    0.82729463  0.63044147]]\n",
      "Iteration:  47\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975851]\n",
      " [ 0.99479056]\n",
      " [ 0.9996948 ]\n",
      " [ 0.99678646]\n",
      " [ 0.99897313]]\n",
      "activation at layer:  2 [[ 0.93897953]\n",
      " [ 0.33822508]\n",
      " [ 0.46095371]]\n",
      "New weights at layer  1 [[ 0.00961968  0.63499614  0.65971989  0.18970095  0.70916825]\n",
      " [-0.20137867 -0.44482042  0.39648039  0.20273871 -0.3314576 ]\n",
      " [-0.14689986 -0.19450169  0.05989928 -0.10671591 -0.09784672]]\n",
      "New weights at layer  0 [[ 0.34369294  0.84035374  0.90567476  0.72983478]\n",
      " [ 0.15408013  0.47344255  0.31161207  0.62451307]\n",
      " [ 0.55689514  0.98152884  0.99279051  0.52159395]\n",
      " [ 0.29687621  0.94505516  0.42313397  0.43185273]\n",
      " [ 0.72264875  0.36456896  0.82730656  0.63045738]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998995]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.93937561]\n",
      " [ 0.32774211]\n",
      " [ 0.44362363]]\n",
      "New weights at layer  1 [[ 0.00427002  0.62964654  0.65437023  0.1843513   0.70381859]\n",
      " [-0.186567   -0.4300089   0.41129206  0.21755037 -0.31664593]\n",
      " [-0.15784946 -0.20545117  0.04894969 -0.11766549 -0.10879631]]\n",
      "New weights at layer  0 [[ 0.34369294  0.84035374  0.90567476  0.72983478]\n",
      " [ 0.15407976  0.4734421   0.31161155  0.62451247]\n",
      " [ 0.55689514  0.98152884  0.99279051  0.52159395]\n",
      " [ 0.29687622  0.94505518  0.42313398  0.43185275]\n",
      " [ 0.72264874  0.36456895  0.82730656  0.63045738]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.93752206]\n",
      " [ 0.34761271]\n",
      " [ 0.4274732 ]]\n",
      "New weights at layer  1 [[-0.00122146  0.62415505  0.64887875  0.17885982  0.69832711]\n",
      " [-0.19445009 -0.43789199  0.40340896  0.20966727 -0.32452902]\n",
      " [-0.14383744 -0.19143916  0.0629617  -0.10365348 -0.0947843 ]]\n",
      "New weights at layer  0 [[ 0.34369294  0.84035374  0.90567476  0.72983478]\n",
      " [ 0.15407976  0.4734421   0.31161155  0.62451247]\n",
      " [ 0.55689514  0.98152884  0.99279051  0.52159395]\n",
      " [ 0.29687622  0.94505518  0.42313398  0.43185275]\n",
      " [ 0.72264874  0.36456895  0.82730656  0.63045738]]\n",
      "Iteration:  48\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975852]\n",
      " [ 0.99479538]\n",
      " [ 0.99969479]\n",
      " [ 0.99678638]\n",
      " [ 0.99897325]]\n",
      "activation at layer:  2 [[ 0.93527783]\n",
      " [ 0.33738136]\n",
      " [ 0.44851969]]\n",
      "New weights at layer  1 [[-0.00082977  0.6245448   0.64927041  0.17925034  0.69871849]\n",
      " [-0.20199061 -0.44539507  0.39586893  0.20214918 -0.33206361]\n",
      " [-0.15492889 -0.20247555  0.05187096 -0.11471195 -0.10586703]]\n",
      "New weights at layer  0 [[ 0.34369372  0.8403553   0.9056771   0.72983791]\n",
      " [ 0.15411005  0.47350268  0.31170242  0.62463363]\n",
      " [ 0.55689413  0.98152682  0.99278748  0.52158992]\n",
      " [ 0.29687564  0.94505401  0.42313224  0.43185042]\n",
      " [ 0.7226528   0.36457706  0.82731872  0.63047359]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998997]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.93570499]\n",
      " [ 0.32694781]\n",
      " [ 0.43179099]]\n",
      "New weights at layer  1 [[-0.00645908  0.61891554  0.6436411   0.17362103  0.69308918]\n",
      " [-0.18717989 -0.43058451  0.41067964  0.21695988 -0.3172529 ]\n",
      " [-0.16552277 -0.21306932  0.04127708 -0.12530583 -0.11646092]]\n",
      "New weights at layer  0 [[ 0.34369372  0.8403553   0.9056771   0.72983791]\n",
      " [ 0.15410967  0.47350222  0.31170188  0.62463302]\n",
      " [ 0.55689413  0.98152682  0.99278748  0.52158992]\n",
      " [ 0.29687565  0.94505403  0.42313225  0.43185044]\n",
      " [ 0.7226528   0.36457706  0.82731872  0.63047359]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.93364324]\n",
      " [ 0.34679379]\n",
      " [ 0.41626793]]\n",
      "New weights at layer  1 [[-0.01224333  0.61313129  0.63785685  0.16783678  0.68730493]\n",
      " [-0.19503574 -0.43844036  0.40282379  0.20910403 -0.32510875]\n",
      " [-0.15133873 -0.19888528  0.05546112 -0.11112179 -0.10227687]]\n",
      "New weights at layer  0 [[ 0.34369372  0.8403553   0.9056771   0.72983791]\n",
      " [ 0.15410966  0.47350222  0.31170188  0.62463302]\n",
      " [ 0.55689413  0.98152682  0.99278748  0.52158992]\n",
      " [ 0.29687565  0.94505403  0.42313225  0.43185044]\n",
      " [ 0.7226528   0.36457706  0.82731872  0.63047359]]\n",
      "Iteration:  49\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99975853]\n",
      " [ 0.99480021]\n",
      " [ 0.99969478]\n",
      " [ 0.99678632]\n",
      " [ 0.99897338]]\n",
      "activation at layer:  2 [[ 0.93116464]\n",
      " [ 0.33661137]\n",
      " [ 0.4374561 ]]\n",
      "New weights at layer  1 [[-0.01180222  0.61357021  0.63829793  0.16827658  0.68774569]\n",
      " [-0.2025506  -0.44591794  0.39530942  0.20161152 -0.3326177 ]\n",
      " [-0.16210141 -0.20959458  0.04469912 -0.12185247 -0.1130311 ]]\n",
      "New weights at layer  0 [[ 0.34369451  0.84035688  0.90567947  0.72984106]\n",
      " [ 0.15414007  0.47356304  0.31179311  0.62475466]\n",
      " [ 0.55689316  0.98152489  0.99278458  0.52158605]\n",
      " [ 0.29687524  0.9450532   0.42313101  0.43184878]\n",
      " [ 0.72265692  0.36458531  0.82733109  0.63049008]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99998999]\n",
      " [ 1.        ]\n",
      " [ 0.99999927]\n",
      " [ 0.99999996]]\n",
      "activation at layer:  2 [[ 0.93162862]\n",
      " [ 0.32622294]\n",
      " [ 0.42128741]]\n",
      "New weights at layer  1 [[-0.01773639  0.6076361   0.63236376  0.16234242  0.68181152]\n",
      " [-0.18774087 -0.43110837  0.41011914  0.21642123 -0.31780798]\n",
      " [-0.17237258 -0.21986565  0.03442796 -0.13212363 -0.12330227]]\n",
      "New weights at layer  0 [[ 0.34369451  0.84035688  0.90567947  0.72984106]\n",
      " [ 0.15413969  0.47356258  0.31179257  0.62475404]\n",
      " [ 0.55689316  0.98152489  0.99278458  0.52158605]\n",
      " [ 0.29687525  0.94505321  0.42313103  0.4318488 ]\n",
      " [ 0.72265692  0.36458531  0.82733109  0.63049008]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 0.99999998]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.92932596]\n",
      " [ 0.3460462 ]\n",
      " [ 0.40633921]]\n",
      "New weights at layer  1 [[-0.02384013  0.60153236  0.62626002  0.15623867  0.67570778]\n",
      " [-0.19557184 -0.43893933  0.40228818  0.20859026 -0.32563894]\n",
      " [-0.15805184 -0.20554491  0.0487487  -0.11780289 -0.10898153]]\n",
      "New weights at layer  0 [[ 0.34369451  0.84035688  0.90567947  0.72984106]\n",
      " [ 0.15413969  0.47356258  0.31179257  0.62475404]\n",
      " [ 0.55689316  0.98152489  0.99278458  0.52158605]\n",
      " [ 0.29687525  0.94505321  0.42313103  0.4318488 ]\n",
      " [ 0.72265692  0.36458531  0.82733109  0.63049008]]\n",
      "[array([[ 0.34369451,  0.84035688,  0.90567947,  0.72984106],\n",
      "       [ 0.15413969,  0.47356258,  0.31179257,  0.62475404],\n",
      "       [ 0.55689316,  0.98152489,  0.99278458,  0.52158605],\n",
      "       [ 0.29687525,  0.94505321,  0.42313103,  0.4318488 ],\n",
      "       [ 0.72265692,  0.36458531,  0.82733109,  0.63049008]]), array([[-0.02384013,  0.60153236,  0.62626002,  0.15623867,  0.67570778],\n",
      "       [-0.19557184, -0.43893933,  0.40228818,  0.20859026, -0.32563894],\n",
      "       [-0.15805184, -0.20554491,  0.0487487 , -0.11780289, -0.10898153]])]\n"
     ]
    }
   ],
   "source": [
    "(weights, bias) = nn.train_network(input.T, output.T, nn.update_weights, 50, 0.1)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.33930455]\n",
      " [ 0.37194926]\n",
      " [ 0.36259774]]\n"
     ]
    }
   ],
   "source": [
    "#Testing predict network function\n",
    "inp = np.array([[1,2,3, 4]])\n",
    "print(nn.predict_network(inp.T, nn.logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.15419501e-02   2.72108844e-02   5.07936508e-03   2.11337868e-02\n",
      "    2.10430839e-02   5.49659864e-02   1.86848073e-02   1.17913832e-02\n",
      "    2.79365079e-02   2.43990930e-02   2.39455782e-02   1.32426304e-02\n",
      "    1.24263039e-02   6.14058957e-02   2.14058957e-02   2.64852608e-02\n",
      "    1.61451247e-02   2.53968254e-03   2.37641723e-02   2.29478458e-02\n",
      "    3.83673469e-02   4.11791383e-02   1.41496599e-02   2.32199546e-02\n",
      "    1.83219955e-02   5.99546485e-02   2.08616780e-02   6.98412698e-03\n",
      "    2.44897959e-02   1.90476190e-02   2.57596372e-02   2.32199546e-02\n",
      "    9.88662132e-03   2.88435374e-02   2.14965986e-02   2.43990930e-02\n",
      "    1.80498866e-02   4.89795918e-03   6.65759637e-02   2.59410431e-02\n",
      "    3.93650794e-02   1.50566893e-02   2.04081633e-02   2.76643991e-02\n",
      "    2.19501134e-02   4.25396825e-02   3.16553288e-02   6.80272109e-03\n",
      "    2.73922902e-02   1.92290249e-02   2.17687075e-02   2.28571429e-02\n",
      "    1.98639456e-02   5.50566893e-02   2.49433107e-02   3.02947846e-02\n",
      "    2.35827664e-02   1.17006803e-02   2.09523810e-02   2.86621315e-02\n",
      "    3.74603175e-02   1.58730159e-02   1.06122449e-02   2.16780045e-02\n",
      "    2.22222222e-02   4.01814059e-02   1.42403628e-02   1.72335601e-03\n",
      "    6.54875283e-02   3.49206349e-02   2.33106576e-02   4.38095238e-02\n",
      "    1.04308390e-02   1.99546485e-02   2.48526077e-02   2.50340136e-02\n",
      "    3.23809524e-02   1.17913832e-03   2.42176871e-02   2.13151927e-02\n",
      "    8.04535147e-02   1.98639456e-02   3.43764172e-02   3.29251701e-02\n",
      "    2.52154195e-02   4.35374150e-02   1.73242630e-02   1.78684807e-02\n",
      "    4.06349206e-02   2.42176871e-02   3.50113379e-02   1.35147392e-02\n",
      "    1.11564626e-02   4.35374150e-02   2.93877551e-02   4.93424036e-02\n",
      "    5.17006803e-03   1.11564626e-02   3.28344671e-02   2.90249433e-02\n",
      "    3.65532880e-02   1.45124717e-02   1.08843537e-02   7.39229025e-02\n",
      "    3.41950113e-02   3.97278912e-02   3.31972789e-02   1.08843537e-02\n",
      "    8.00000000e-02   2.39455782e-02   5.06122449e-02   2.72108844e-03\n",
      "    2.47619048e-02   2.51247166e-02   3.26530612e-02   1.98639456e-02\n",
      "    4.03628118e-02   2.63945578e-02   2.30385488e-02   3.88208617e-02\n",
      "    2.12244898e-02   2.48526077e-02   2.81179138e-03   4.28117914e-02\n",
      "    2.77551020e-02   3.32879819e-02   2.06802721e-02   2.22222222e-02\n",
      "    6.35827664e-02   2.48526077e-02   6.06802721e-02   2.84807256e-02\n",
      "    2.81179138e-03   2.00453515e-02   3.48299320e-02   2.02267574e-02\n",
      "    5.16099773e-02   1.44217687e-02   4.25396825e-02   2.42176871e-02\n",
      "    2.07709751e-02   2.78458050e-02   1.08843537e-03   7.96371882e-02\n",
      "    2.13151927e-02   3.76417234e-02   4.20861678e-02   1.28798186e-02\n",
      "    2.35827664e-02   2.21315193e-02   5.77777778e-02   3.71882086e-02\n",
      "    2.11337868e-02   3.35600907e-02   1.85941043e-02   2.00453515e-02\n",
      "    3.31972789e-02   7.80045351e-03   2.31292517e-02   3.48299320e-02\n",
      "    2.05895692e-02   4.84353741e-02   1.09750567e-02   3.31972789e-02\n",
      "    4.29024943e-02   2.66666667e-02   2.97505669e-02   4.73469388e-02\n",
      "    4.63492063e-02   2.78458050e-02   5.65986395e-02   3.50113379e-02\n",
      "    2.35827664e-03   3.93650794e-02   2.38548753e-02   2.05895692e-02\n",
      "    3.83673469e-02   1.25170068e-02   2.60317460e-02   2.59410431e-02\n",
      "    2.01360544e-02   5.37868481e-02   1.17913832e-03   8.61678005e-02\n",
      "    2.68480726e-02   7.30158730e-02   4.62585034e-02   2.81179138e-02\n",
      "    8.98866213e-02   2.84807256e-02   2.58503401e-02   4.52607710e-02\n",
      "    1.27891156e-02   3.67346939e-02   2.62131519e-02   3.96371882e-02\n",
      "    3.83673469e-02   1.34240363e-02   9.98639456e-02   3.27437642e-02\n",
      "    3.82766440e-02   3.65532880e-02   3.17460317e-03   1.11746032e-01\n",
      "    3.02040816e-02   2.96598639e-02   4.38095238e-02   9.79591837e-03\n",
      "    4.09070295e-02   3.47392290e-02   6.51247166e-02   2.44897959e-03\n",
      "    1.28798186e-02   3.37414966e-02   2.54875283e-02   6.86621315e-02\n",
      "    4.89795918e-03   2.30385488e-02   6.02267574e-02   4.31746032e-02\n",
      "    4.06349206e-02   5.35147392e-03   1.90476190e-03   1.13832200e-01\n",
      "    3.91836735e-02   7.07482993e-02   2.58503401e-02   9.70521542e-03\n",
      "    3.72789116e-02   2.80272109e-02   7.94557823e-02   4.18140590e-02\n",
      "    2.60317460e-02   3.60997732e-02   3.48299320e-02   5.63265306e-02\n",
      "    4.69841270e-02   3.17460317e-03   1.00770975e-01   2.30385488e-02\n",
      "    3.99092971e-02   2.81179138e-02   1.26984127e-02   1.10929705e-01\n",
      "    3.50113379e-02   7.60090703e-02   2.15873016e-02   1.34240363e-02\n",
      "    3.80952381e-02   4.02721088e-02   6.05895692e-02   2.52154195e-02\n",
      "    1.09750567e-02   3.80952381e-02   1.99546485e-02   7.50113379e-02\n",
      "    3.13832200e-02   1.40589569e-02   4.08163265e-02   4.01814059e-02\n",
      "    4.02721088e-02   3.71882086e-03   2.77551020e-02   3.42857143e-02\n",
      "    5.44217687e-03   4.67120181e-02   1.85034014e-02   3.01133787e-02\n",
      "    6.26757370e-02   1.04217687e-01   4.56235828e-02   4.48072562e-02\n",
      "    1.25170068e-02   2.74829932e-02   6.65759637e-02   7.63718821e-02\n",
      "    4.33560091e-02   1.40589569e-02   2.28571429e-02   6.09523810e-02\n",
      "    4.02721088e-02   6.32199546e-02   6.34920635e-03   3.90022676e-02\n",
      "    4.29931973e-02   5.46938776e-02   8.42630385e-02   1.63265306e-02\n",
      "    6.07709751e-02   6.38548753e-02   1.41133787e-01   5.27891156e-02\n",
      "    4.80725624e-03   2.21315193e-02   7.55555556e-02   1.30975057e-01\n",
      "    4.82539683e-02   1.07029478e-02   3.47392290e-02   8.98866213e-02\n",
      "    8.03628118e-02   7.02947846e-02   1.54195011e-03   6.27664399e-02\n",
      "    9.56916100e-02   7.85487528e-02   9.36054422e-02   1.99546485e-02\n",
      "    2.35827664e-02   7.31065760e-02   1.45668934e-01   6.67573696e-02\n",
      "    5.53287982e-03   2.36734694e-02   6.80272109e-02   1.40136054e-01\n",
      "    5.76870748e-02   1.46031746e-02   5.01587302e-02   9.41496599e-02\n",
      "    7.94557823e-02   7.26530612e-02   1.81405896e-03   2.16780045e-02\n",
      "    8.35374150e-02   8.01814059e-02   7.51020408e-02   4.75283447e-02\n",
      "    2.62131519e-02   8.68027211e-02   1.06122449e-01   5.68707483e-02\n",
      "    1.90476190e-03   3.66439909e-02   9.07936508e-02   1.42947846e-01\n",
      "    6.44897959e-02   4.41723356e-02   4.23582766e-02   7.47392290e-02\n",
      "    8.10884354e-02   4.70748299e-02   4.29931973e-02   5.65986395e-02\n",
      "    9.08843537e-02   7.80045351e-02   6.81179138e-02   5.98639456e-03\n",
      "    4.58956916e-02   8.78004535e-02   1.08662132e-01   7.31972789e-02\n",
      "    2.24943311e-02   8.25396825e-02   3.67346939e-02   1.38775510e-01\n",
      "    8.25396825e-02   8.61678005e-03   6.46712018e-02   5.95011338e-02\n",
      "    8.01814059e-02   3.58276644e-02   1.28798186e-02   2.29478458e-02\n",
      "    1.14195011e-01   7.84580499e-02   8.93424036e-02   1.90476190e-03\n",
      "    3.70068027e-02   7.91836735e-02   4.60770975e-02   3.08390023e-03\n",
      "    1.28798186e-02   4.69841270e-02   4.99773243e-02   5.16099773e-02\n",
      "    2.78458050e-02   1.50566893e-02   8.19954649e-02   6.28571429e-02\n",
      "    1.43310658e-02   1.60544218e-02   1.64172336e-02   2.91156463e-02\n",
      "    4.31746032e-02   1.71428571e-02   1.75056689e-02   1.57823129e-02\n",
      "    5.73242630e-02   8.12698413e-02   5.72335601e-02   1.18820862e-02\n",
      "    1.43310658e-02   6.96598639e-02   7.61904762e-02   4.75283447e-02\n",
      "    1.63265306e-02   7.16553288e-03   2.29478458e-02   5.65079365e-02\n",
      "    1.47845805e-02   4.25396825e-02   1.82312925e-02   3.41043084e-02\n",
      "    8.20861678e-02   1.68707483e-02   1.35147392e-02   1.44217687e-02\n",
      "    7.01133787e-02   4.97959184e-02   3.61904762e-02   1.96825397e-02\n",
      "    1.58730159e-02   3.37414966e-02   1.07664399e-01   4.53514739e-02\n",
      "    1.28798186e-02   1.19727891e-02   2.24036281e-02   1.43945578e-01\n",
      "    1.15192744e-02   1.68707483e-02   1.65986395e-02   4.00907029e-02\n",
      "    1.00498866e-01   1.57823129e-02   4.02721088e-02   6.80272109e-03\n",
      "    3.92743764e-02   3.75510204e-02   4.61678005e-02   1.47845805e-02\n",
      "    7.98185941e-03   7.52834467e-02   1.40589569e-01   4.64399093e-02\n",
      "    1.26984127e-02   5.35147392e-03   4.74376417e-02   1.05034014e-01\n",
      "    1.08843537e-02   1.67800454e-02   1.74149660e-02   6.82086168e-02\n",
      "    8.22675737e-02   1.67800454e-02   2.08616780e-02   6.80272109e-03\n",
      "    1.11473923e-01   1.47029478e-01   5.10657596e-02   1.38775510e-02\n",
      "    1.12471655e-02   8.36281179e-02   1.23809524e-01   5.82312925e-02\n",
      "    4.33560091e-02   1.25170068e-02   1.13741497e-01   1.38866213e-01\n",
      "    2.20408163e-02   1.52380952e-02   2.54875283e-02   4.16326531e-02\n",
      "    1.68798186e-01   2.57596372e-02   1.98639456e-02   4.62585034e-03\n",
      "    1.22358277e-01   1.08934240e-01   5.01587302e-02   1.82312925e-02\n",
      "    1.54195011e-03   1.20362812e-01   1.08934240e-01   6.34920635e-02\n",
      "    1.77777778e-02   1.82312925e-02   4.14512472e-02   1.29342404e-01\n",
      "    2.12244898e-02   5.44217687e-03   1.39682540e-02   7.51020408e-02\n",
      "    8.64399093e-02   2.21315193e-02   2.95691610e-02   1.54195011e-03\n",
      "    1.17732426e-01   2.00544218e-01   3.90929705e-02   2.88435374e-02\n",
      "    5.26077098e-03   1.03945578e-01   1.30249433e-01   6.03174603e-02\n",
      "    3.12018141e-02   3.35600907e-03   7.36507937e-02   1.51473923e-01]\n",
      " [  2.16159063e+03   1.99864258e+03   1.39296663e+03   1.36467265e+03\n",
      "    1.54176813e+03   2.07207058e+03   1.65788278e+03   1.06003554e+03\n",
      "    1.74711298e+03   2.09564284e+03   1.53437355e+03   1.86355988e+03\n",
      "    7.17722798e+02   2.63633438e+03   1.77236161e+03   1.45982307e+03\n",
      "    1.63752842e+03   1.90498822e+03   1.47264980e+03   1.76576693e+03\n",
      "    2.19141242e+03   2.26395600e+03   1.04275649e+03   1.40186706e+03\n",
      "    2.20677769e+03   1.94017113e+03   2.28866415e+03   1.34149568e+03\n",
      "    1.40845045e+03   1.38252886e+03   1.67655105e+03   1.85883005e+03\n",
      "    7.80202327e+02   1.58139931e+03   2.32053456e+03   1.70901206e+03\n",
      "    1.80872559e+03   1.43262015e+03   2.55256448e+03   2.03625121e+03\n",
      "    2.02133526e+03   2.32910383e+03   2.14464868e+03   1.38391746e+03\n",
      "    2.27689426e+03   2.02667265e+03   2.47252212e+03   1.47545631e+03\n",
      "    1.64150503e+03   1.99084851e+03   1.35493140e+03   2.01619493e+03\n",
      "    1.09768154e+03   2.63022151e+03   2.65258129e+03   1.86160606e+03\n",
      "    1.86906039e+03   1.05523957e+03   1.92489355e+03   2.24366274e+03\n",
      "    2.04252865e+03   1.98244266e+03   9.51847126e+02   1.41808011e+03\n",
      "    2.45282960e+03   2.16922621e+03   1.29712400e+03   2.25775003e+03\n",
      "    2.69825042e+03   2.48114828e+03   1.51344762e+03   2.46178479e+03\n",
      "    7.53172413e+02   1.58505744e+03   1.56763011e+03   1.56099642e+03\n",
      "    1.79987985e+03   1.60023376e+03   1.56229463e+03   2.01136982e+03\n",
      "    2.43140094e+03   1.80364985e+03   1.49792584e+03   2.08036623e+03\n",
      "    1.99315584e+03   1.66402430e+03   1.85180931e+03   1.43548269e+03\n",
      "    1.88894395e+03   1.85725391e+03   1.69822314e+03   1.45915793e+03\n",
      "    9.40841552e+02   2.48542624e+03   2.48014271e+03   3.15438667e+03\n",
      "    3.26874903e+03   9.90116908e+02   1.12357954e+03   2.34745535e+03\n",
      "    1.48090091e+03   1.53697471e+03   1.01841911e+03   2.23872284e+03\n",
      "    2.01399138e+03   2.14462410e+03   2.34112773e+03   1.60647874e+03\n",
      "    2.68586456e+03   1.76422177e+03   1.86278816e+03   3.01931567e+03\n",
      "    1.81256807e+03   1.52756455e+03   2.14404361e+03   1.85449592e+03\n",
      "    2.28423779e+03   2.67966190e+03   1.86307258e+03   2.25569094e+03\n",
      "    2.15078943e+03   2.27216090e+03   1.31790772e+03   1.72188165e+03\n",
      "    1.62636483e+03   1.57477987e+03   2.19165797e+03   1.37474060e+03\n",
      "    2.46808143e+03   2.10914643e+03   1.91717273e+03   2.66687452e+03\n",
      "    2.16536135e+03   1.20610817e+03   2.27412635e+03   2.37123670e+03\n",
      "    2.47183072e+03   1.71847060e+03   1.94601926e+03   1.94637521e+03\n",
      "    1.75826742e+03   2.07368467e+03   1.79203222e+03   2.72243094e+03\n",
      "    2.47442970e+03   2.27261697e+03   2.50900427e+03   1.26741952e+03\n",
      "    1.48859719e+03   1.94495934e+03   2.22428134e+03   2.90261062e+03\n",
      "    1.18113818e+03   1.76325470e+03   1.71040582e+03   2.41407536e+03\n",
      "    2.17735255e+03   1.57987787e+03   1.77123004e+03   2.35129408e+03\n",
      "    2.30672091e+03   2.86405710e+03   1.87010174e+03   1.47775829e+03\n",
      "    2.75649806e+03   1.57975217e+03   2.64271095e+03   1.84084199e+03\n",
      "    1.89721378e+03   2.51017651e+03   1.80956456e+03   2.50264011e+03\n",
      "    2.48688594e+03   3.47424654e+03   2.01339013e+03   2.04770954e+03\n",
      "    2.35979591e+03   9.79995006e+02   2.64252848e+03   1.80514093e+03\n",
      "    1.90352371e+03   2.10911256e+03   2.04107301e+03   2.82284656e+03\n",
      "    1.97388856e+03   2.04383666e+03   2.19121255e+03   1.17714228e+03\n",
      "    3.24870835e+03   1.90513264e+03   1.85147436e+03   2.24199282e+03\n",
      "    8.30400721e+02   1.61646995e+03   2.59611667e+03   1.85140443e+03\n",
      "    2.42803603e+03   7.21103488e+02   3.01137892e+03   2.33530900e+03\n",
      "    1.36057722e+03   2.39466555e+03   2.01836746e+03   3.44322851e+03\n",
      "    2.11240234e+03   2.42311939e+03   2.56062575e+03   1.55350059e+03\n",
      "    2.62927683e+03   2.42595871e+03   2.29650025e+03   3.02621470e+03\n",
      "    1.11218868e+03   2.35455064e+03   2.43241952e+03   2.31784236e+03\n",
      "    2.95468891e+03   1.58169610e+03   3.27524610e+03   2.95795520e+03\n",
      "    2.46286897e+03   3.65134026e+03   1.90000196e+03   3.57050464e+03\n",
      "    2.58234930e+03   2.18197382e+03   2.02863805e+03   1.25450060e+03\n",
      "    2.16427476e+03   1.91087573e+03   2.93603682e+03   2.64729420e+03\n",
      "    1.22893319e+03   2.50950313e+03   2.15482493e+03   2.44416497e+03\n",
      "    3.13639577e+03   2.09935273e+03   3.11172005e+03   2.16966084e+03\n",
      "    2.68313415e+03   1.86872652e+03   8.18962402e+02   3.56502233e+03\n",
      "    2.39693789e+03   2.92780692e+03   2.27320401e+03   1.35846116e+03\n",
      "    2.13470386e+03   1.94050296e+03   2.13221258e+03   2.23144768e+03\n",
      "    2.15813140e+03   1.63429098e+03   1.68399450e+03   2.94865138e+03\n",
      "    1.82389753e+03   1.74607451e+03   2.12654039e+03   2.14927342e+03\n",
      "    1.58803166e+03   3.16198519e+03   1.47464214e+03   1.97257411e+03\n",
      "    2.60192498e+03   2.20376385e+03   3.50800513e+03   1.70837397e+03\n",
      "    2.69667186e+03   4.54613887e+03   2.38590206e+03   4.01122047e+03\n",
      "    1.26993146e+03   1.67579389e+03   3.81405694e+03   2.48339544e+03\n",
      "    3.23588559e+03   8.14350182e+02   1.21579247e+03   3.55775685e+03\n",
      "    2.24610035e+03   3.59908658e+03   8.27655767e+02   2.06287677e+03\n",
      "    3.75025104e+03   2.41348461e+03   4.18189470e+03   1.54121743e+03\n",
      "    2.30066149e+03   2.90606952e+03   3.59223254e+03   4.38645694e+03\n",
      "    2.08002979e+03   1.72365140e+03   4.57516772e+03   3.44969586e+03\n",
      "    3.17595017e+03   9.36700488e+02   2.05018062e+03   5.00703664e+03\n",
      "    2.95102898e+03   2.95346277e+03   2.05091469e+03   2.33359197e+03\n",
      "    4.51481399e+03   3.22403402e+03   3.71139118e+03   1.39858324e+03\n",
      "    1.14238423e+03   4.17499318e+03   3.93968308e+03   3.42845192e+03\n",
      "    1.03664332e+03   1.89444318e+03   4.13773007e+03   3.60617990e+03\n",
      "    2.81097831e+03   1.18739112e+03   2.96659358e+03   4.34225002e+03\n",
      "    2.64350650e+03   2.54769620e+03   2.40274903e+03   1.53537417e+03\n",
      "    4.74044086e+03   3.33699293e+03   3.89640289e+03   2.34664268e+03\n",
      "    1.66969727e+03   4.69927735e+03   3.43531128e+03   3.91056489e+03\n",
      "    2.31965062e+03   1.86546513e+03   3.69386854e+03   3.96811634e+03\n",
      "    4.06910288e+03   1.63995915e+03   1.83763700e+03   4.78666501e+03\n",
      "    3.21424154e+03   3.58475440e+03   1.63880453e+03   1.79066674e+03\n",
      "    4.52878639e+03   3.06907865e+03   3.20715466e+03   2.65900072e+03\n",
      "    2.33026780e+03   4.72029435e+03   3.55680930e+03   3.65640834e+03\n",
      "    3.18911218e+03   1.84928105e+03   3.36529658e+03   4.07515807e+03\n",
      "    3.57606205e+03   1.51483563e+03   2.54487024e+03   3.46847221e+03\n",
      "    3.63450254e+03   3.70811247e+03   8.59727393e+02   2.01935504e+03\n",
      "    4.06418364e+03   3.37372765e+03   3.98741000e+03   2.08155000e+03\n",
      "    1.99814258e+03   4.24239210e+03   2.48702626e+03   3.76083496e+03\n",
      "    9.77070084e+02   1.96522905e+03   4.11512306e+03   2.21535551e+03\n",
      "    1.94232417e+03   1.70465874e+03   2.18312328e+03   4.57311006e+03\n",
      "    1.30786135e+03   1.67767331e+03   1.70076305e+03   1.13497062e+03\n",
      "    2.95111854e+03   1.30549995e+03   1.63625245e+03   1.34387643e+03\n",
      "    2.25598670e+03   4.74606319e+03   1.61023013e+03   2.39270655e+03\n",
      "    1.67797311e+03   2.35328692e+03   3.32254812e+03   1.96594590e+03\n",
      "    1.56377630e+03   1.57242494e+03   2.16136462e+03   3.57721180e+03\n",
      "    8.64436443e+02   2.54383898e+03   1.71313773e+03   1.98697623e+03\n",
      "    4.13416049e+03   1.00745307e+03   1.75264588e+03   1.40426475e+03\n",
      "    1.71523832e+03   3.25380600e+03   1.56380227e+03   1.63647285e+03\n",
      "    1.62282442e+03   1.34547801e+03   4.78560712e+03   2.45607603e+03\n",
      "    2.00491023e+03   1.88679682e+03   1.38228911e+03   5.56149807e+03\n",
      "    2.41807872e+03   1.17802152e+03   1.44911163e+03   1.43361583e+03\n",
      "    4.96686213e+03   1.22864769e+03   1.99486030e+03   2.42222760e+03\n",
      "    1.29819243e+03   2.43066387e+03   1.57204356e+03   1.56340220e+03\n",
      "    1.44251725e+03   2.84789120e+03   4.73414705e+03   1.97093606e+03\n",
      "    2.09143875e+03   1.13356016e+03   2.97782447e+03   4.15709064e+03\n",
      "    2.35915092e+03   1.74859723e+03   1.34307392e+03   2.17912676e+03\n",
      "    4.74271570e+03   1.15301880e+03   1.90709615e+03   1.14332573e+03\n",
      "    3.18529210e+03   5.70183568e+03   1.75868321e+03   2.15521303e+03\n",
      "    1.19876284e+03   3.29932359e+03   5.81537350e+03   1.90170625e+03\n",
      "    2.12799987e+03   1.58648809e+03   2.83034759e+03   5.26729252e+03\n",
      "    1.80204533e+03   1.99216715e+03   2.47915766e+03   1.87016192e+03\n",
      "    5.82365919e+03   1.42427745e+03   1.92594845e+03   1.51912212e+03\n",
      "    3.31814818e+03   4.35901863e+03   2.25530171e+03   1.86503526e+03\n",
      "    2.02895489e+03   3.41270030e+03   4.67959045e+03   2.45427217e+03\n",
      "    1.97586886e+03   1.53530762e+03   1.98402839e+03   5.80664266e+03\n",
      "    2.23209824e+03   1.97254829e+03   1.29649949e+03   2.30443660e+03\n",
      "    3.95043108e+03   2.02268897e+03   2.66015677e+03   1.92247990e+03\n",
      "    3.13577475e+03   6.43761548e+03   2.15019767e+03   2.69824374e+03\n",
      "    1.88778015e+03   3.50176931e+03   5.62704487e+03   2.27325868e+03\n",
      "    3.05233700e+03   1.96588757e+03   2.22019541e+03   5.53611428e+03]]\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " ..., \n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n",
      "Iteration:  0\n",
      "Iteration:  1\n",
      "Iteration:  2\n",
      "Iteration:  3\n",
      "Iteration:  4\n",
      "Iteration:  5\n",
      "Iteration:  6\n",
      "Iteration:  7\n",
      "Iteration:  8\n",
      "Iteration:  9\n",
      "Iteration:  10\n",
      "Iteration:  11\n",
      "Iteration:  12\n",
      "Iteration:  13\n",
      "Iteration:  14\n",
      "Iteration:  15\n",
      "Iteration:  16\n",
      "Iteration:  17\n",
      "Iteration:  18\n",
      "Iteration:  19\n",
      "Iteration:  20\n",
      "Iteration:  21\n",
      "Iteration:  22\n",
      "Iteration:  23\n",
      "Iteration:  24\n",
      "Iteration:  25\n",
      "Iteration:  26\n",
      "Iteration:  27\n",
      "Iteration:  28\n",
      "Iteration:  29\n",
      "Iteration:  30\n",
      "Iteration:  31\n",
      "Iteration:  32\n",
      "Iteration:  33\n",
      "Iteration:  34\n",
      "Iteration:  35\n",
      "Iteration:  36\n",
      "Iteration:  37\n",
      "Iteration:  38\n",
      "Iteration:  39\n",
      "Iteration:  40\n",
      "Iteration:  41\n",
      "Iteration:  42\n",
      "Iteration:  43\n",
      "Iteration:  44\n",
      "Iteration:  45\n",
      "Iteration:  46\n",
      "Iteration:  47\n",
      "Iteration:  48\n",
      "Iteration:  49\n",
      "Iteration:  50\n",
      "Iteration:  51\n",
      "Iteration:  52\n",
      "Iteration:  53\n",
      "Iteration:  54\n",
      "Iteration:  55\n",
      "Iteration:  56\n",
      "Iteration:  57\n",
      "Iteration:  58\n",
      "Iteration:  59\n",
      "Iteration:  60\n",
      "Iteration:  61\n",
      "Iteration:  62\n",
      "Iteration:  63\n",
      "Iteration:  64\n",
      "Iteration:  65\n",
      "Iteration:  66\n",
      "Iteration:  67\n",
      "Iteration:  68\n",
      "Iteration:  69\n",
      "Iteration:  70\n",
      "Iteration:  71\n",
      "Iteration:  72\n",
      "Iteration:  73\n",
      "Iteration:  74\n",
      "Iteration:  75\n",
      "Iteration:  76\n",
      "Iteration:  77\n",
      "Iteration:  78\n",
      "Iteration:  79\n",
      "Iteration:  80\n",
      "Iteration:  81\n",
      "Iteration:  82\n",
      "Iteration:  83\n",
      "Iteration:  84\n",
      "Iteration:  85\n",
      "Iteration:  86\n",
      "Iteration:  87\n",
      "Iteration:  88\n",
      "Iteration:  89\n",
      "Iteration:  90\n",
      "Iteration:  91\n",
      "Iteration:  92\n",
      "Iteration:  93\n",
      "Iteration:  94\n",
      "Iteration:  95\n",
      "Iteration:  96\n",
      "Iteration:  97\n",
      "Iteration:  98\n",
      "Iteration:  99\n",
      "Iteration:  100\n",
      "Iteration:  101\n",
      "Iteration:  102\n",
      "Iteration:  103\n",
      "Iteration:  104\n",
      "Iteration:  105\n",
      "Iteration:  106\n",
      "Iteration:  107\n",
      "Iteration:  108\n",
      "Iteration:  109\n",
      "Iteration:  110\n",
      "Iteration:  111\n",
      "Iteration:  112\n",
      "Iteration:  113\n",
      "Iteration:  114\n",
      "Iteration:  115\n",
      "Iteration:  116\n",
      "Iteration:  117\n",
      "Iteration:  118\n",
      "Iteration:  119\n",
      "Iteration:  120\n",
      "Iteration:  121\n",
      "Iteration:  122\n",
      "Iteration:  123\n",
      "Iteration:  124\n",
      "Iteration:  125\n",
      "Iteration:  126\n",
      "Iteration:  127\n",
      "Iteration:  128\n",
      "Iteration:  129\n",
      "Iteration:  130\n",
      "Iteration:  131\n",
      "Iteration:  132\n",
      "Iteration:  133\n",
      "Iteration:  134\n",
      "Iteration:  135\n",
      "Iteration:  136\n",
      "Iteration:  137\n",
      "Iteration:  138\n",
      "Iteration:  139\n",
      "Iteration:  140\n",
      "Iteration:  141\n",
      "Iteration:  142\n",
      "Iteration:  143\n",
      "Iteration:  144\n",
      "Iteration:  145\n",
      "Iteration:  146\n",
      "Iteration:  147\n",
      "Iteration:  148\n",
      "Iteration:  149\n",
      "Iteration:  150\n",
      "Iteration:  151\n",
      "Iteration:  152\n",
      "Iteration:  153\n",
      "Iteration:  154\n",
      "Iteration:  155\n",
      "Iteration:  156\n",
      "Iteration:  157\n",
      "Iteration:  158\n",
      "Iteration:  159\n",
      "Iteration:  160\n",
      "Iteration:  161\n",
      "Iteration:  162\n",
      "Iteration:  163\n",
      "Iteration:  164\n",
      "Iteration:  165\n",
      "Iteration:  166\n",
      "Iteration:  167\n",
      "Iteration:  168\n",
      "Iteration:  169\n",
      "Iteration:  170\n",
      "Iteration:  171\n",
      "Iteration:  172\n",
      "Iteration:  173\n",
      "Iteration:  174\n",
      "Iteration:  175\n",
      "Iteration:  176\n",
      "Iteration:  177\n",
      "Iteration:  178\n",
      "Iteration:  179\n",
      "Iteration:  180\n",
      "Iteration:  181\n",
      "Iteration:  182\n",
      "Iteration:  183\n",
      "Iteration:  184\n",
      "Iteration:  185\n",
      "Iteration:  186\n",
      "Iteration:  187\n",
      "Iteration:  188\n",
      "Iteration:  189\n",
      "Iteration:  190\n",
      "Iteration:  191\n",
      "Iteration:  192\n",
      "Iteration:  193\n",
      "Iteration:  194\n",
      "Iteration:  195\n",
      "Iteration:  196\n",
      "Iteration:  197\n",
      "Iteration:  198\n",
      "Iteration:  199\n",
      "Iteration:  200\n",
      "Iteration:  201\n",
      "Iteration:  202\n",
      "Iteration:  203\n",
      "Iteration:  204\n",
      "Iteration:  205\n",
      "Iteration:  206\n",
      "Iteration:  207\n",
      "Iteration:  208\n",
      "Iteration:  209\n",
      "Iteration:  210\n",
      "Iteration:  211\n",
      "Iteration:  212\n",
      "Iteration:  213\n",
      "Iteration:  214\n",
      "Iteration:  215\n",
      "Iteration:  216\n",
      "Iteration:  217\n",
      "Iteration:  218\n",
      "Iteration:  219\n",
      "Iteration:  220\n",
      "Iteration:  221\n",
      "Iteration:  222\n",
      "Iteration:  223\n",
      "Iteration:  224\n",
      "Iteration:  225\n",
      "Iteration:  226\n",
      "Iteration:  227\n",
      "Iteration:  228\n",
      "Iteration:  229\n",
      "Iteration:  230\n",
      "Iteration:  231\n",
      "Iteration:  232\n",
      "Iteration:  233\n",
      "Iteration:  234\n",
      "Iteration:  235\n",
      "Iteration:  236\n",
      "Iteration:  237\n",
      "Iteration:  238\n",
      "Iteration:  239\n",
      "Iteration:  240\n",
      "Iteration:  241\n",
      "Iteration:  242\n",
      "Iteration:  243\n",
      "Iteration:  244\n",
      "Iteration:  245\n",
      "Iteration:  246\n",
      "Iteration:  247\n",
      "Iteration:  248\n",
      "Iteration:  249\n",
      "Iteration:  250\n",
      "Iteration:  251\n",
      "Iteration:  252\n",
      "Iteration:  253\n",
      "Iteration:  254\n",
      "Iteration:  255\n",
      "Iteration:  256\n",
      "Iteration:  257\n",
      "Iteration:  258\n",
      "Iteration:  259\n",
      "Iteration:  260\n",
      "Iteration:  261\n",
      "Iteration:  262\n",
      "Iteration:  263\n",
      "Iteration:  264\n",
      "Iteration:  265\n",
      "Iteration:  266\n",
      "Iteration:  267\n",
      "Iteration:  268\n",
      "Iteration:  269\n",
      "Iteration:  270\n",
      "Iteration:  271\n",
      "Iteration:  272\n",
      "Iteration:  273\n",
      "Iteration:  274\n",
      "Iteration:  275\n",
      "Iteration:  276\n",
      "Iteration:  277\n",
      "Iteration:  278\n",
      "Iteration:  279\n",
      "Iteration:  280\n",
      "Iteration:  281\n",
      "Iteration:  282\n",
      "Iteration:  283\n",
      "Iteration:  284\n",
      "Iteration:  285\n",
      "Iteration:  286\n",
      "Iteration:  287\n",
      "Iteration:  288\n",
      "Iteration:  289\n",
      "Iteration:  290\n",
      "Iteration:  291\n",
      "Iteration:  292\n",
      "Iteration:  293\n",
      "Iteration:  294\n",
      "Iteration:  295\n",
      "Iteration:  296\n",
      "Iteration:  297\n",
      "Iteration:  298\n",
      "Iteration:  299\n",
      "Iteration:  300\n",
      "Iteration:  301\n",
      "Iteration:  302\n",
      "Iteration:  303\n",
      "Iteration:  304\n",
      "Iteration:  305\n",
      "Iteration:  306\n",
      "Iteration:  307\n",
      "Iteration:  308\n",
      "Iteration:  309\n",
      "Iteration:  310\n",
      "Iteration:  311\n",
      "Iteration:  312\n",
      "Iteration:  313\n",
      "Iteration:  314\n",
      "Iteration:  315\n",
      "Iteration:  316\n",
      "Iteration:  317\n",
      "Iteration:  318\n",
      "Iteration:  319\n",
      "Iteration:  320\n",
      "Iteration:  321\n",
      "Iteration:  322\n",
      "Iteration:  323\n",
      "Iteration:  324\n",
      "Iteration:  325\n",
      "Iteration:  326\n",
      "Iteration:  327\n",
      "Iteration:  328\n",
      "Iteration:  329\n",
      "Iteration:  330\n",
      "Iteration:  331\n",
      "Iteration:  332\n",
      "Iteration:  333\n",
      "Iteration:  334\n",
      "Iteration:  335\n",
      "Iteration:  336\n",
      "Iteration:  337\n",
      "Iteration:  338\n",
      "Iteration:  339\n",
      "Iteration:  340\n",
      "Iteration:  341\n",
      "Iteration:  342\n",
      "Iteration:  343\n",
      "Iteration:  344\n",
      "Iteration:  345\n",
      "Iteration:  346\n",
      "Iteration:  347\n",
      "Iteration:  348\n",
      "Iteration:  349\n",
      "Iteration:  350\n",
      "Iteration:  351\n",
      "Iteration:  352\n",
      "Iteration:  353\n",
      "Iteration:  354\n",
      "Iteration:  355\n",
      "Iteration:  356\n",
      "Iteration:  357\n",
      "Iteration:  358\n",
      "Iteration:  359\n",
      "Iteration:  360\n",
      "Iteration:  361\n",
      "Iteration:  362\n",
      "Iteration:  363\n",
      "Iteration:  364\n",
      "Iteration:  365\n",
      "Iteration:  366\n",
      "Iteration:  367\n",
      "Iteration:  368\n",
      "Iteration:  369\n",
      "Iteration:  370\n",
      "Iteration:  371\n",
      "Iteration:  372\n",
      "Iteration:  373\n",
      "Iteration:  374\n",
      "Iteration:  375\n",
      "Iteration:  376\n",
      "Iteration:  377\n",
      "Iteration:  378\n",
      "Iteration:  379\n",
      "Iteration:  380\n",
      "Iteration:  381\n",
      "Iteration:  382\n",
      "Iteration:  383\n",
      "Iteration:  384\n",
      "Iteration:  385\n",
      "Iteration:  386\n",
      "Iteration:  387\n",
      "Iteration:  388\n",
      "Iteration:  389\n",
      "Iteration:  390\n",
      "Iteration:  391\n",
      "Iteration:  392\n",
      "Iteration:  393\n",
      "Iteration:  394\n",
      "Iteration:  395\n",
      "Iteration:  396\n",
      "Iteration:  397\n",
      "Iteration:  398\n",
      "Iteration:  399\n",
      "Iteration:  400\n",
      "Iteration:  401\n",
      "Iteration:  402\n",
      "Iteration:  403\n",
      "Iteration:  404\n",
      "Iteration:  405\n",
      "Iteration:  406\n",
      "Iteration:  407\n",
      "Iteration:  408\n",
      "Iteration:  409\n",
      "Iteration:  410\n",
      "Iteration:  411\n",
      "Iteration:  412\n",
      "Iteration:  413\n",
      "Iteration:  414\n",
      "Iteration:  415\n",
      "Iteration:  416\n",
      "Iteration:  417\n",
      "Iteration:  418\n",
      "Iteration:  419\n",
      "Iteration:  420\n",
      "Iteration:  421\n",
      "Iteration:  422\n",
      "Iteration:  423\n",
      "Iteration:  424\n",
      "Iteration:  425\n",
      "Iteration:  426\n",
      "Iteration:  427\n",
      "Iteration:  428\n",
      "Iteration:  429\n",
      "Iteration:  430\n",
      "Iteration:  431\n",
      "Iteration:  432\n",
      "Iteration:  433\n",
      "Iteration:  434\n",
      "Iteration:  435\n",
      "Iteration:  436\n",
      "Iteration:  437\n",
      "Iteration:  438\n",
      "Iteration:  439\n",
      "Iteration:  440\n",
      "Iteration:  441\n",
      "Iteration:  442\n",
      "Iteration:  443\n",
      "Iteration:  444\n",
      "Iteration:  445\n",
      "Iteration:  446\n",
      "Iteration:  447\n",
      "Iteration:  448\n",
      "Iteration:  449\n",
      "Iteration:  450\n",
      "Iteration:  451\n",
      "Iteration:  452\n",
      "Iteration:  453\n",
      "Iteration:  454\n",
      "Iteration:  455\n",
      "Iteration:  456\n",
      "Iteration:  457\n",
      "Iteration:  458\n",
      "Iteration:  459\n",
      "Iteration:  460\n",
      "Iteration:  461\n",
      "Iteration:  462\n",
      "Iteration:  463\n",
      "Iteration:  464\n",
      "Iteration:  465\n",
      "Iteration:  466\n",
      "Iteration:  467\n",
      "Iteration:  468\n",
      "Iteration:  469\n",
      "Iteration:  470\n",
      "Iteration:  471\n",
      "Iteration:  472\n",
      "Iteration:  473\n",
      "Iteration:  474\n",
      "Iteration:  475\n",
      "Iteration:  476\n",
      "Iteration:  477\n",
      "Iteration:  478\n",
      "Iteration:  479\n",
      "Iteration:  480\n",
      "Iteration:  481\n",
      "Iteration:  482\n",
      "Iteration:  483\n",
      "Iteration:  484\n",
      "Iteration:  485\n",
      "Iteration:  486\n",
      "Iteration:  487\n",
      "Iteration:  488\n",
      "Iteration:  489\n",
      "Iteration:  490\n",
      "Iteration:  491\n",
      "Iteration:  492\n",
      "Iteration:  493\n",
      "Iteration:  494\n",
      "Iteration:  495\n",
      "Iteration:  496\n",
      "Iteration:  497\n",
      "Iteration:  498\n",
      "Iteration:  499\n",
      "Iteration:  500\n",
      "Iteration:  501\n",
      "Iteration:  502\n",
      "Iteration:  503\n",
      "Iteration:  504\n",
      "Iteration:  505\n",
      "Iteration:  506\n",
      "Iteration:  507\n",
      "Iteration:  508\n",
      "Iteration:  509\n",
      "Iteration:  510\n",
      "Iteration:  511\n",
      "Iteration:  512\n",
      "Iteration:  513\n",
      "Iteration:  514\n",
      "Iteration:  515\n",
      "Iteration:  516\n",
      "Iteration:  517\n",
      "Iteration:  518\n",
      "Iteration:  519\n",
      "Iteration:  520\n",
      "Iteration:  521\n",
      "Iteration:  522\n",
      "Iteration:  523\n",
      "Iteration:  524\n",
      "Iteration:  525\n",
      "Iteration:  526\n",
      "Iteration:  527\n",
      "Iteration:  528\n",
      "Iteration:  529\n",
      "Iteration:  530\n",
      "Iteration:  531\n",
      "Iteration:  532\n",
      "Iteration:  533\n",
      "Iteration:  534\n",
      "Iteration:  535\n",
      "Iteration:  536\n",
      "Iteration:  537\n",
      "Iteration:  538\n",
      "Iteration:  539\n",
      "Iteration:  540\n",
      "Iteration:  541\n",
      "Iteration:  542\n",
      "Iteration:  543\n",
      "Iteration:  544\n",
      "Iteration:  545\n",
      "Iteration:  546\n",
      "Iteration:  547\n",
      "Iteration:  548\n",
      "Iteration:  549\n",
      "Iteration:  550\n",
      "Iteration:  551\n",
      "Iteration:  552\n",
      "Iteration:  553\n",
      "Iteration:  554\n",
      "Iteration:  555\n",
      "Iteration:  556\n",
      "Iteration:  557\n",
      "Iteration:  558\n",
      "Iteration:  559\n",
      "Iteration:  560\n",
      "Iteration:  561\n",
      "Iteration:  562\n",
      "Iteration:  563\n",
      "Iteration:  564\n",
      "Iteration:  565\n",
      "Iteration:  566\n",
      "Iteration:  567\n",
      "Iteration:  568\n",
      "Iteration:  569\n",
      "Iteration:  570\n",
      "Iteration:  571\n",
      "Iteration:  572\n",
      "Iteration:  573\n",
      "Iteration:  574\n",
      "Iteration:  575\n",
      "Iteration:  576\n",
      "Iteration:  577\n",
      "Iteration:  578\n",
      "Iteration:  579\n",
      "Iteration:  580\n",
      "Iteration:  581\n",
      "Iteration:  582\n",
      "Iteration:  583\n",
      "Iteration:  584\n",
      "Iteration:  585\n",
      "Iteration:  586\n",
      "Iteration:  587\n",
      "Iteration:  588\n",
      "Iteration:  589\n",
      "Iteration:  590\n",
      "Iteration:  591\n",
      "Iteration:  592\n",
      "Iteration:  593\n",
      "Iteration:  594\n",
      "Iteration:  595\n",
      "Iteration:  596\n",
      "Iteration:  597\n",
      "Iteration:  598\n",
      "Iteration:  599\n",
      "Iteration:  600\n",
      "Iteration:  601\n",
      "Iteration:  602\n",
      "Iteration:  603\n",
      "Iteration:  604\n",
      "Iteration:  605\n",
      "Iteration:  606\n",
      "Iteration:  607\n",
      "Iteration:  608\n",
      "Iteration:  609\n",
      "Iteration:  610\n",
      "Iteration:  611\n",
      "Iteration:  612\n",
      "Iteration:  613\n",
      "Iteration:  614\n",
      "Iteration:  615\n",
      "Iteration:  616\n",
      "Iteration:  617\n",
      "Iteration:  618\n",
      "Iteration:  619\n",
      "Iteration:  620\n",
      "Iteration:  621\n",
      "Iteration:  622\n",
      "Iteration:  623\n",
      "Iteration:  624\n",
      "Iteration:  625\n",
      "Iteration:  626\n",
      "Iteration:  627\n",
      "Iteration:  628\n",
      "Iteration:  629\n",
      "Iteration:  630\n",
      "Iteration:  631\n",
      "Iteration:  632\n",
      "Iteration:  633\n",
      "Iteration:  634\n",
      "Iteration:  635\n",
      "Iteration:  636\n",
      "Iteration:  637\n",
      "Iteration:  638\n",
      "Iteration:  639\n",
      "Iteration:  640\n",
      "Iteration:  641\n",
      "Iteration:  642\n",
      "Iteration:  643\n",
      "Iteration:  644\n",
      "Iteration:  645\n",
      "Iteration:  646\n",
      "Iteration:  647\n",
      "Iteration:  648\n",
      "Iteration:  649\n",
      "Iteration:  650\n",
      "Iteration:  651\n",
      "Iteration:  652\n",
      "Iteration:  653\n",
      "Iteration:  654\n",
      "Iteration:  655\n",
      "Iteration:  656\n",
      "Iteration:  657\n",
      "Iteration:  658\n",
      "Iteration:  659\n",
      "Iteration:  660\n",
      "Iteration:  661\n",
      "Iteration:  662\n",
      "Iteration:  663\n",
      "Iteration:  664\n",
      "Iteration:  665\n",
      "Iteration:  666\n",
      "Iteration:  667\n",
      "Iteration:  668\n",
      "Iteration:  669\n",
      "Iteration:  670\n",
      "Iteration:  671\n",
      "Iteration:  672\n",
      "Iteration:  673\n",
      "Iteration:  674\n",
      "Iteration:  675\n",
      "Iteration:  676\n",
      "Iteration:  677\n",
      "Iteration:  678\n",
      "Iteration:  679\n",
      "Iteration:  680\n",
      "Iteration:  681\n",
      "Iteration:  682\n",
      "Iteration:  683\n",
      "Iteration:  684\n",
      "Iteration:  685\n",
      "Iteration:  686\n",
      "Iteration:  687\n",
      "Iteration:  688\n",
      "Iteration:  689\n",
      "Iteration:  690\n",
      "Iteration:  691\n",
      "Iteration:  692\n",
      "Iteration:  693\n",
      "Iteration:  694\n",
      "Iteration:  695\n",
      "Iteration:  696\n",
      "Iteration:  697\n",
      "Iteration:  698\n",
      "Iteration:  699\n",
      "Iteration:  700\n",
      "Iteration:  701\n",
      "Iteration:  702\n",
      "Iteration:  703\n",
      "Iteration:  704\n",
      "Iteration:  705\n",
      "Iteration:  706\n",
      "Iteration:  707\n",
      "Iteration:  708\n",
      "Iteration:  709\n",
      "Iteration:  710\n",
      "Iteration:  711\n",
      "Iteration:  712\n",
      "Iteration:  713\n",
      "Iteration:  714\n",
      "Iteration:  715\n",
      "Iteration:  716\n",
      "Iteration:  717\n",
      "Iteration:  718\n",
      "Iteration:  719\n",
      "Iteration:  720\n",
      "Iteration:  721\n",
      "Iteration:  722\n",
      "Iteration:  723\n",
      "Iteration:  724\n",
      "Iteration:  725\n",
      "Iteration:  726\n",
      "Iteration:  727\n",
      "Iteration:  728\n",
      "Iteration:  729\n",
      "Iteration:  730\n",
      "Iteration:  731\n",
      "Iteration:  732\n",
      "Iteration:  733\n",
      "Iteration:  734\n",
      "Iteration:  735\n",
      "Iteration:  736\n",
      "Iteration:  737\n",
      "Iteration:  738\n",
      "Iteration:  739\n",
      "Iteration:  740\n",
      "Iteration:  741\n",
      "Iteration:  742\n",
      "Iteration:  743\n",
      "Iteration:  744\n",
      "Iteration:  745\n",
      "Iteration:  746\n",
      "Iteration:  747\n",
      "Iteration:  748\n",
      "Iteration:  749\n",
      "Iteration:  750\n",
      "Iteration:  751\n",
      "Iteration:  752\n",
      "Iteration:  753\n",
      "Iteration:  754\n",
      "Iteration:  755\n",
      "Iteration:  756\n",
      "Iteration:  757\n",
      "Iteration:  758\n",
      "Iteration:  759\n",
      "Iteration:  760\n",
      "Iteration:  761\n",
      "Iteration:  762\n",
      "Iteration:  763\n",
      "Iteration:  764\n",
      "Iteration:  765\n",
      "Iteration:  766\n",
      "Iteration:  767\n",
      "Iteration:  768\n",
      "Iteration:  769\n",
      "Iteration:  770\n",
      "Iteration:  771\n",
      "Iteration:  772\n",
      "Iteration:  773\n",
      "Iteration:  774\n",
      "Iteration:  775\n",
      "Iteration:  776\n",
      "Iteration:  777\n",
      "Iteration:  778\n",
      "Iteration:  779\n",
      "Iteration:  780\n",
      "Iteration:  781\n",
      "Iteration:  782\n",
      "Iteration:  783\n",
      "Iteration:  784\n",
      "Iteration:  785\n",
      "Iteration:  786\n",
      "Iteration:  787\n",
      "Iteration:  788\n",
      "Iteration:  789\n",
      "Iteration:  790\n",
      "Iteration:  791\n",
      "Iteration:  792\n",
      "Iteration:  793\n",
      "Iteration:  794\n",
      "Iteration:  795\n",
      "Iteration:  796\n",
      "Iteration:  797\n",
      "Iteration:  798\n",
      "Iteration:  799\n",
      "Iteration:  800\n",
      "Iteration:  801\n",
      "Iteration:  802\n",
      "Iteration:  803\n",
      "Iteration:  804\n",
      "Iteration:  805\n",
      "Iteration:  806\n",
      "Iteration:  807\n",
      "Iteration:  808\n",
      "Iteration:  809\n",
      "Iteration:  810\n",
      "Iteration:  811\n",
      "Iteration:  812\n",
      "Iteration:  813\n",
      "Iteration:  814\n",
      "Iteration:  815\n",
      "Iteration:  816\n",
      "Iteration:  817\n",
      "Iteration:  818\n",
      "Iteration:  819\n",
      "Iteration:  820\n",
      "Iteration:  821\n",
      "Iteration:  822\n",
      "Iteration:  823\n",
      "Iteration:  824\n",
      "Iteration:  825\n",
      "Iteration:  826\n",
      "Iteration:  827\n",
      "Iteration:  828\n",
      "Iteration:  829\n",
      "Iteration:  830\n",
      "Iteration:  831\n",
      "Iteration:  832\n",
      "Iteration:  833\n",
      "Iteration:  834\n",
      "Iteration:  835\n",
      "Iteration:  836\n",
      "Iteration:  837\n",
      "Iteration:  838\n",
      "Iteration:  839\n",
      "Iteration:  840\n",
      "Iteration:  841\n",
      "Iteration:  842\n",
      "Iteration:  843\n",
      "Iteration:  844\n",
      "Iteration:  845\n",
      "Iteration:  846\n",
      "Iteration:  847\n",
      "Iteration:  848\n",
      "Iteration:  849\n",
      "Iteration:  850\n",
      "Iteration:  851\n",
      "Iteration:  852\n",
      "Iteration:  853\n",
      "Iteration:  854\n",
      "Iteration:  855\n",
      "Iteration:  856\n",
      "Iteration:  857\n",
      "Iteration:  858\n",
      "Iteration:  859\n",
      "Iteration:  860\n",
      "Iteration:  861\n",
      "Iteration:  862\n",
      "Iteration:  863\n",
      "Iteration:  864\n",
      "Iteration:  865\n",
      "Iteration:  866\n",
      "Iteration:  867\n",
      "Iteration:  868\n",
      "Iteration:  869\n",
      "Iteration:  870\n",
      "Iteration:  871\n",
      "Iteration:  872\n",
      "Iteration:  873\n",
      "Iteration:  874\n",
      "Iteration:  875\n",
      "Iteration:  876\n",
      "Iteration:  877\n",
      "Iteration:  878\n",
      "Iteration:  879\n",
      "Iteration:  880\n",
      "Iteration:  881\n",
      "Iteration:  882\n",
      "Iteration:  883\n",
      "Iteration:  884\n",
      "Iteration:  885\n",
      "Iteration:  886\n",
      "Iteration:  887\n",
      "Iteration:  888\n",
      "Iteration:  889\n",
      "Iteration:  890\n",
      "Iteration:  891\n",
      "Iteration:  892\n",
      "Iteration:  893\n",
      "Iteration:  894\n",
      "Iteration:  895\n",
      "Iteration:  896\n",
      "Iteration:  897\n",
      "Iteration:  898\n",
      "Iteration:  899\n",
      "Iteration:  900\n",
      "Iteration:  901\n",
      "Iteration:  902\n",
      "Iteration:  903\n",
      "Iteration:  904\n",
      "Iteration:  905\n",
      "Iteration:  906\n",
      "Iteration:  907\n",
      "Iteration:  908\n",
      "Iteration:  909\n",
      "Iteration:  910\n",
      "Iteration:  911\n",
      "Iteration:  912\n",
      "Iteration:  913\n",
      "Iteration:  914\n",
      "Iteration:  915\n",
      "Iteration:  916\n",
      "Iteration:  917\n",
      "Iteration:  918\n",
      "Iteration:  919\n",
      "Iteration:  920\n",
      "Iteration:  921\n",
      "Iteration:  922\n",
      "Iteration:  923\n",
      "Iteration:  924\n",
      "Iteration:  925\n",
      "Iteration:  926\n",
      "Iteration:  927\n",
      "Iteration:  928\n",
      "Iteration:  929\n",
      "Iteration:  930\n",
      "Iteration:  931\n",
      "Iteration:  932\n",
      "Iteration:  933\n",
      "Iteration:  934\n",
      "Iteration:  935\n",
      "Iteration:  936\n",
      "Iteration:  937\n",
      "Iteration:  938\n",
      "Iteration:  939\n",
      "Iteration:  940\n",
      "Iteration:  941\n",
      "Iteration:  942\n",
      "Iteration:  943\n",
      "Iteration:  944\n",
      "Iteration:  945\n",
      "Iteration:  946\n",
      "Iteration:  947\n",
      "Iteration:  948\n",
      "Iteration:  949\n",
      "Iteration:  950\n",
      "Iteration:  951\n",
      "Iteration:  952\n",
      "Iteration:  953\n",
      "Iteration:  954\n",
      "Iteration:  955\n",
      "Iteration:  956\n",
      "Iteration:  957\n",
      "Iteration:  958\n",
      "Iteration:  959\n",
      "Iteration:  960\n",
      "Iteration:  961\n",
      "Iteration:  962\n",
      "Iteration:  963\n",
      "Iteration:  964\n",
      "Iteration:  965\n",
      "Iteration:  966\n",
      "Iteration:  967\n",
      "Iteration:  968\n",
      "Iteration:  969\n",
      "Iteration:  970\n",
      "Iteration:  971\n",
      "Iteration:  972\n",
      "Iteration:  973\n",
      "Iteration:  974\n",
      "Iteration:  975\n",
      "Iteration:  976\n",
      "Iteration:  977\n",
      "Iteration:  978\n",
      "Iteration:  979\n",
      "Iteration:  980\n",
      "Iteration:  981\n",
      "Iteration:  982\n",
      "Iteration:  983\n",
      "Iteration:  984\n",
      "Iteration:  985\n",
      "Iteration:  986\n",
      "Iteration:  987\n",
      "Iteration:  988\n",
      "Iteration:  989\n",
      "Iteration:  990\n",
      "Iteration:  991\n",
      "Iteration:  992\n",
      "Iteration:  993\n",
      "Iteration:  994\n",
      "Iteration:  995\n",
      "Iteration:  996\n",
      "Iteration:  997\n",
      "Iteration:  998\n",
      "Iteration:  999\n",
      "[[ 0.19140912]\n",
      " [ 0.19374282]\n",
      " [ 0.19615019]\n",
      " [ 0.19863453]\n",
      " [ 0.20119972]]\n"
     ]
    }
   ],
   "source": [
    "zcr_data = [np.loadtxt(\"reps/clarinet_zcr.txt\"), np.loadtxt(\"reps/flute_zcr.txt\"), np.loadtxt(\"reps/guitar_zcr.txt\"), np.loadtxt(\"reps/saxophone_zcr.txt\"), np.loadtxt(\"reps/violin_zcr.txt\")]\n",
    "sc_data = [np.loadtxt(\"reps/clarinet_centroid.txt\"), np.loadtxt(\"reps/flute_centroid.txt\"), np.loadtxt(\"reps/guitar_centroid.txt\"), np.loadtxt(\"reps/saxophone_centroid.txt\"), np.loadtxt(\"reps/violin_centroid.txt\")]\n",
    "# f_data = \n",
    "# g_data = \n",
    "# s_data = \n",
    "# v_data = \n",
    "inp = []\n",
    "inputs = []\n",
    "outputs = []\n",
    "for i in range(100):\n",
    "    for j in range(5):\n",
    "        inputs = []\n",
    "        output = [0]*5\n",
    "        inputs.append(zcr_data[j][i])\n",
    "        inputs.append(sc_data[j][i])\n",
    "        output[j] = 1\n",
    "        outputs.append(output)\n",
    "        inp.append(np.array(inputs))\n",
    "        # for k in range(5):\n",
    "        #    if k != j:\n",
    "         #       outputs[k].append(0)\n",
    "# print(inp)\n",
    "inp = np.array(inp)\n",
    "print(inp.T)\n",
    "output = np.array(outputs)\n",
    "print(output)\n",
    "test_inp = np.array([inp[0]])\n",
    "nn.initialize_network(2, 5, 3, 5)\n",
    "nn.train_network(inp.T, output.T, nn.update_weights, 1000, 0.1)\n",
    "print(nn.predict_network(test_inp.T, nn.logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Thursday, May 26\n",
    "\n",
    "Our neural network code is not predicting correctly, and I am spending my time going back to try and de-bug it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
