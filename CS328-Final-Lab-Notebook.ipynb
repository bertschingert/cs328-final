{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import audio_processing as ap\n",
    "import audio_utils as au\n",
    "import math\n",
    "import numpy.fft as fft\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 328 Final Project\n",
    "## Thomas Bertschinger and Sanders McMillan\n",
    "### Lab Notebook\n",
    "\n",
    "### Tuesday, April 26, 2016 **(joint entry)**\n",
    "\n",
    "Our final project will (likely) be *model* focused rather than experiment focused. We will \"implemenet computational model(s) and compare results to existing human data.\" In this case, the human data will be musical compositions created by humans (*e.g.* Bach fugues) and our model will be a system that \"composes\" music (perhaps learning from human compositions). \n",
    "\n",
    "We set up a bibliography using Latex and BibTex, and a git repository to keep track of our code and materials. \n",
    "\n",
    "https://github.com/bertschingert/cs328-final/tree/master/references"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wednesday, April 27, 2016 (joint entry)\n",
    "\n",
    "We are thinking about doing a signal-processing based model that creates a representation of a signal and can classify the signal into categories such as instrument family, voice, etc. We will need to create a representation of audio sound in a (hopefully) small number of dimensions that includes enough relevant information so that we can classify a sound into a category such as brass versus string instrument, for example. \n",
    "\n",
    "The representation will likely include attributes such as attack and decay time of the waveform; the spectrum at various points in time; how much the spectrum changes over time; irregularities in the spectrum. \n",
    "\n",
    "Our model will ideally be able to classify instruments correctly at different amplitudes and pitches. It would also be important to limit the model to audio information that humans can actually percieve. (For example, it wouldn't make sense for our model to take into account frequences substantially above 20,000Hz because humans cannot hear that high.) \n",
    "\n",
    "Being able to pull out the relevant information from an audio signal is important because it will help us understand how humans can do things such as distinguish different people's voices. We know probably hundreds of different voices that we can identify from only a few words of speech. This is also important for being able to recognize different instruments present in a single audio signal. \n",
    "\n",
    "We think it would be plausible to create a neural network to identify different audio signals. We will first take an audio signal and use tools such as the discrete Fourier transform to create a suitable representation of the signal that omits extraneous information or information that humans cannot perciever. Then, we train a neural network to be able to identify, from the features of the representation, what category the signal belongs to. \n",
    "\n",
    "Theoretically, our model will be grounded in the similarity models learned earlier in this course, in addition to Gibsonian and Gestalt principles of perceiving invariants in stimuli and grouping similar and proximal stimuli as belonging to the same perceptual unit (e.g. being able to classify a signal as being of a certain category regardless of it's amplitude and signal, and classifying/perceiving similar successive waveforms as being from the same instrument). It will also involve the place and time theories for how humans transform air pressure hitting the ear into an auditory representation (which is where our Fourier transform and representation stages come in). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Monday, May 16, 2016 (joint entry)\n",
    "\n",
    "We have written some code to do basic audio processing (FFT). We are also starting to create the NN..\n",
    "\n",
    "In order to keep the neural network consistent, we will have the option to save weights and biases to a text file so that they can be loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'audio_files/violin-a440.raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9097abe804ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_raw_stereo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"audio_files/violin-a440.raw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m44100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mau\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_fft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Accounts/mcmillans/Desktop/Final Project/cs328-final/audio_utils.py\u001b[0m in \u001b[0;36mread_raw_stereo\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_raw_stereo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mraw_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'audio_files/violin-a440.raw'"
     ]
    }
   ],
   "source": [
    "left, right = au.read_raw_stereo(\"audio_files/violin-a440.raw\")\n",
    "chunk = left[:44100]\n",
    "freqs = fft.rfft(chunk)\n",
    "au.graph_fft(freqs[:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also started compiling our audio sample library. The audio samples are 1.5 second long clips downloaded from http://www.philharmonia.co.uk/explore/make_music. So far we have 5 different instruments (guitar, saxophone, flute, violin, and trumpet), each at three different pitches (A, C, and E). The octaves of the pitches are different for each instrument as the library did not contain all octaves for each instrument, and each instrument has different pitch restraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Tuesday, May 17, 2016 (joint entry)\n",
    "\n",
    "We are starting to write code that uses the FFT to get some information on the signal, such as attack time. This will be helpful for our representation. We are also starting to implement the neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saturday, May 21, 2016\n",
    "\n",
    "Created the Hann window function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_signal = []\n",
    "for i in range(100):\n",
    "    test_signal.append(1)\n",
    "w = ap.hann_window(test_signal)\n",
    "au.graph_signal(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created code to compute the spectral centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wave = au.create_sine_wave(440, 1, 1)\n",
    "spectrum = fft.rfft(wave)\n",
    "print(\"before window: spectral centroid is \", ap.get_spectral_centroid(spectrum, 44100))\n",
    "au.graph_fft(spectrum[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_signal = ap.hann_window(wave)\n",
    "w_spectrum = fft.rfft(w_signal)\n",
    "print(\"after window: spectral centroid is \", ap.get_spectral_centroid(w_spectrum, 44100))\n",
    "au.graph_fft(w_spectrum[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Sunday, May 22, 2016\n",
    "\n",
    "Now using the python wave library to read .wav files. Created the function read_wav_mono in audio_utils.py which returns a list of the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = au.read_wav_mono('audio_files/guitar_A4_very-long_forte_normal.wav')\n",
    "au.graph_signal(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step= int(44100 / 4)\n",
    "start = 44100\n",
    "chunk = f[start:start+step]\n",
    "s = fft.rfft(chunk)\n",
    "au.graph_fft(s, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = ap.hann_window(chunk)\n",
    "s = fft.rfft(w)\n",
    "au.graph_fft(s, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = ap.spectral_flux(f)\n",
    "au.graph_signal(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f2 = au.read_wav_mono('audio_files/saxophone_A4_15_forte_normal.wav')\n",
    "au.graph_signal(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s2 = ap.spectral_flux(f2)\n",
    "au.graph_signal(s2)\n",
    "for i in s2:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Thursday, May 26\n",
    "\n",
    "We finished writing code for our neural network, and are now testing and de-bugging it. Our neural network has an initialize_network function that initializes the weights and nodes of the networks based on input length, the output length, the number of hidden units, and the number of layers of the network. It also has a set_hidden_units function that allows you to set the number of hidden units at any particular hidden layer, and changes the weights accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.53583157,  0.37605029,  0.64027522,  0.42711325],\n",
       "        [ 0.99766032,  0.78736334,  0.50272463,  0.68582115],\n",
       "        [ 0.83431295,  0.3429505 ,  0.07815369,  0.36288005]]),\n",
       " array([[ 0.82471077,  0.2414166 ,  0.87454694],\n",
       "        [ 0.02866854,  0.70695496,  0.4506752 ],\n",
       "        [ 0.11439646,  0.72454449,  0.53050173]])]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the initialize network function\n",
    "import neural_net as nn\n",
    "import numpy as np\n",
    "nn.initialize_network(4, 3, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.46905087,  0.57140433,  0.44536742,  0.43432284],\n",
       "        [ 0.59206108,  0.30714227,  0.37903945,  0.96927626],\n",
       "        [ 0.7412559 ,  0.75477984,  0.9881189 ,  0.44569425],\n",
       "        [ 0.26689418,  0.16495126,  0.04135824,  0.62399558],\n",
       "        [ 0.47180655,  0.84089444,  0.41694873,  0.21328739]]),\n",
       " array([[ 0.82075489,  0.59253942,  0.45404062,  0.11430654,  0.85821336],\n",
       "        [ 0.40295664,  0.65716793,  0.82903873,  0.55309532,  0.18747298],\n",
       "        [ 0.77588933,  0.71952014,  0.26807361,  0.81921477,  0.6340558 ]])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing the set hidden units function\n",
    "nn.set_hidden_units(1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99483795]\n",
      " [ 0.9982188 ]\n",
      " [ 0.99962178]\n",
      " [ 0.96174899]\n",
      " [ 0.99369691]]\n",
      "activation at layer:  2 [[ 0.9713287 ]\n",
      " [ 0.93548685]\n",
      " [ 0.96809513]]\n",
      "New weights at layer  1 [[ 0.82076283  0.59254739  0.45404861  0.11431422  0.85822129]\n",
      " [ 0.40239498  0.65660436  0.82847436  0.55255234  0.18691196]\n",
      " [ 0.77559186  0.71922166  0.2677747   0.81892719  0.63375867]]\n",
      "New weights at layer  0 [[ 0.46904855  0.57139968  0.44536045  0.43431354]\n",
      " [ 0.59206005  0.3071402   0.37903635  0.96927213]\n",
      " [ 0.7412557   0.75477943  0.98811829  0.44569343]\n",
      " [ 0.26687373  0.16491035  0.04129689  0.62391377]\n",
      " [ 0.47180474  0.84089083  0.41694332  0.21328017]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.9999976 ]\n",
      " [ 0.99999978]\n",
      " [ 1.        ]\n",
      " [ 0.99950573]\n",
      " [ 0.99999733]]\n",
      "activation at layer:  2 [[ 0.97174988]\n",
      " [ 0.93681787]\n",
      " [ 0.96926408]]\n",
      "New weights at layer  1 [[ 0.82049607  0.59228062  0.45378184  0.11404758  0.85795453]\n",
      " [ 0.40243238  0.65664176  0.82851176  0.55258971  0.18694935]\n",
      " [ 0.77530311  0.71893291  0.26748595  0.81863858  0.63346992]]\n",
      "New weights at layer  0 [[ 0.46904854  0.57139968  0.44536044  0.43431354]\n",
      " [ 0.59206005  0.3071402   0.37903635  0.96927213]\n",
      " [ 0.7412557   0.75477943  0.98811829  0.44569343]\n",
      " [ 0.26687312  0.16490962  0.04129604  0.6239128 ]\n",
      " [ 0.47180474  0.84089082  0.41694331  0.21328016]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999386]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97170756]\n",
      " [ 0.93684721]\n",
      " [ 0.96922446]]\n",
      "New weights at layer  1 [[ 0.82022893  0.59201348  0.4535147   0.11378044  0.85768739]\n",
      " [ 0.4018781   0.65608747  0.82795748  0.55203544  0.18639507]\n",
      " [ 0.77531229  0.71894209  0.26749513  0.81864776  0.6334791 ]]\n",
      "New weights at layer  0 [[ 0.46904854  0.57139968  0.44536044  0.43431354]\n",
      " [ 0.59206005  0.3071402   0.37903635  0.96927213]\n",
      " [ 0.7412557   0.75477943  0.98811829  0.44569343]\n",
      " [ 0.2668731   0.1649096   0.04129601  0.62391278]\n",
      " [ 0.47180474  0.84089082  0.41694331  0.21328016]]\n",
      "[array([[ 0.46904854,  0.57139968,  0.44536044,  0.43431354],\n",
      "       [ 0.59206005,  0.3071402 ,  0.37903635,  0.96927213],\n",
      "       [ 0.7412557 ,  0.75477943,  0.98811829,  0.44569343],\n",
      "       [ 0.2668731 ,  0.1649096 ,  0.04129601,  0.62391278],\n",
      "       [ 0.47180474,  0.84089082,  0.41694331,  0.21328016]]), array([[ 0.82022893,  0.59201348,  0.4535147 ,  0.11378044,  0.85768739],\n",
      "       [ 0.4018781 ,  0.65608747,  0.82795748,  0.55203544,  0.18639507],\n",
      "       [ 0.77531229,  0.71894209,  0.26749513,  0.81864776,  0.6334791 ]])]\n"
     ]
    }
   ],
   "source": [
    "#Testing the update weights function\n",
    "input = np.array([[1,2,3, 4], [5, 6, 7, 8], [9,10,11, 12]])\n",
    "output = np.array([[1, 0, 0], [0,1,0], [0,0,1]])\n",
    "np.array(output)\n",
    "nn.update_weights(input.T, output.T, 0.01)\n",
    "print(nn.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99483758]\n",
      " [ 0.99821874]\n",
      " [ 0.99962178]\n",
      " [ 0.96172533]\n",
      " [ 0.99369656]]\n",
      "activation at layer:  2 [[ 0.97124135]\n",
      " [ 0.93509846]\n",
      " [ 0.96798851]]\n",
      "New weights at layer  1 [[ 0.82030884  0.59209367  0.453595    0.1138577   0.85776721]\n",
      " [ 0.39623234  0.65042253  0.82228458  0.5465776   0.1807558 ]\n",
      " [ 0.77232829  0.71594795  0.26449678  0.81576308  0.63049852]]\n",
      "New weights at layer  0 [[ 0.46902544  0.57135346  0.44529112  0.43422111]\n",
      " [ 0.59204975  0.30711961  0.37900545  0.96923094]\n",
      " [ 0.74125365  0.75477533  0.98811213  0.44568523]\n",
      " [ 0.26666919  0.16450178  0.04068428  0.62309714]\n",
      " [ 0.4717869   0.84085514  0.41688979  0.2132088 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.9999976 ]\n",
      " [ 0.99999978]\n",
      " [ 1.        ]\n",
      " [ 0.99949851]\n",
      " [ 0.99999732]]\n",
      "activation at layer:  2 [[ 0.97167494]\n",
      " [ 0.93460079]\n",
      " [ 0.96867711]]\n",
      "New weights at layer  1 [[ 0.81763453  0.58941935  0.45092068  0.11118472  0.8550929 ]\n",
      " [ 0.39663208  0.65082227  0.82268431  0.54697713  0.18115553]\n",
      " [ 0.76938916  0.71300881  0.26155764  0.81282542  0.62755939]]\n",
      "New weights at layer  0 [[ 0.46902539  0.5713534   0.44529105  0.43422102]\n",
      " [ 0.59204974  0.3071196   0.37900545  0.96923094]\n",
      " [ 0.74125365  0.75477533  0.98811213  0.44568523]\n",
      " [ 0.26666301  0.16449436  0.04067562  0.62308724]\n",
      " [ 0.47178684  0.84085508  0.41688972  0.21320871]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999371]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.9712317 ]\n",
      " [ 0.93476383]\n",
      " [ 0.96815021]]\n",
      "New weights at layer  1 [[ 0.81492084  0.58670566  0.44820699  0.10847105  0.85237921]\n",
      " [ 0.39093185  0.64512204  0.81698408  0.54127694  0.1754553 ]\n",
      " [ 0.76948737  0.71310702  0.26165585  0.81292363  0.6276576 ]]\n",
      "New weights at layer  0 [[ 0.46902539  0.5713534   0.44529105  0.43422102]\n",
      " [ 0.59204974  0.3071196   0.37900545  0.96923094]\n",
      " [ 0.74125365  0.75477533  0.98811213  0.44568523]\n",
      " [ 0.26666282  0.16449415  0.0406754   0.62308699]\n",
      " [ 0.47178684  0.84085508  0.41688972  0.21320871]]\n",
      "Iteration:  1\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99483389]\n",
      " [ 0.99821817]\n",
      " [ 0.99962176]\n",
      " [ 0.96148863]\n",
      " [ 0.99369309]]\n",
      "activation at layer:  2 [[ 0.9703453 ]\n",
      " [ 0.93103206]\n",
      " [ 0.96689292]]\n",
      "New weights at layer  1 [[ 0.81500573  0.58679084  0.44829229  0.10855309  0.852464  ]\n",
      " [ 0.38498445  0.63915441  0.81100806  0.53552889  0.16951472]\n",
      " [ 0.76640824  0.71001741  0.2585619   0.8099477   0.624582  ]]\n",
      "New weights at layer  0 [[ 0.46900172  0.57130607  0.44522006  0.43412637]\n",
      " [ 0.59203913  0.30709837  0.3789736   0.96918847]\n",
      " [ 0.74125153  0.75477108  0.98810577  0.44567674]\n",
      " [ 0.26645179  0.16407209  0.04004231  0.62224287]\n",
      " [ 0.47176883  0.84081906  0.41683569  0.21313667]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.9999976 ]\n",
      " [ 0.99999978]\n",
      " [ 1.        ]\n",
      " [ 0.99949083]\n",
      " [ 0.99999732]]\n",
      "activation at layer:  2 [[ 0.97078598]\n",
      " [ 0.93036112]\n",
      " [ 0.96758323]]\n",
      "New weights at layer  1 [[ 0.81225254  0.58403764  0.44553909  0.10580129  0.84971081]\n",
      " [ 0.38543563  0.63960559  0.81145925  0.53597984  0.16996591]\n",
      " [ 0.76337333  0.7069825   0.25552699  0.80691433  0.6215471 ]]\n",
      "New weights at layer  0 [[ 0.46900167  0.57130601  0.44521999  0.43412629]\n",
      " [ 0.59203912  0.30709837  0.3789736   0.96918847]\n",
      " [ 0.74125153  0.75477108  0.98810577  0.44567674]\n",
      " [ 0.26644544  0.16406447  0.04003341  0.6222327 ]\n",
      " [ 0.47176878  0.84081899  0.41683561  0.21313658]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999356]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.97031556]\n",
      " [ 0.93055379]\n",
      " [ 0.96702034]]\n",
      "New weights at layer  1 [[ 0.80945771  0.58124281  0.44274426  0.10300649  0.84691598]\n",
      " [ 0.37942207  0.63359203  0.80544569  0.52996632  0.16395235]\n",
      " [ 0.76347851  0.70708768  0.25563217  0.80701951  0.62165228]]\n",
      "New weights at layer  0 [[ 0.46900167  0.57130601  0.44521998  0.43412629]\n",
      " [ 0.59203912  0.30709837  0.3789736   0.96918847]\n",
      " [ 0.74125153  0.75477108  0.98810577  0.44567674]\n",
      " [ 0.26644524  0.16406425  0.04003317  0.62223244]\n",
      " [ 0.47176878  0.84081899  0.41683561  0.21313658]]\n",
      "Iteration:  2\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99483012]\n",
      " [ 0.99821759]\n",
      " [ 0.99962173]\n",
      " [ 0.96124221]\n",
      " [ 0.99368958]]\n",
      "activation at layer:  2 [[ 0.96939489]\n",
      " [ 0.92650343]\n",
      " [ 0.96572489]]\n",
      "New weights at layer  1 [[ 0.80954804  0.58133345  0.44283503  0.10309377  0.84700621]\n",
      " [ 0.37314568  0.62729427  0.79913906  0.52390184  0.15768315]\n",
      " [ 0.76029845  0.7038968   0.2524368   0.80394682  0.61847587]]\n",
      "New weights at layer  0 [[ 0.46897744  0.57125755  0.4451473   0.43402937]\n",
      " [ 0.59202817  0.30707646  0.37894074  0.96914466]\n",
      " [ 0.74124933  0.75476669  0.98809918  0.44566796]\n",
      " [ 0.2662267   0.16362718  0.03937756  0.6213583 ]\n",
      " [ 0.47175062  0.84078269  0.41678115  0.21306397]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999759]\n",
      " [ 0.99999978]\n",
      " [ 1.        ]\n",
      " [ 0.99948276]\n",
      " [ 0.99999732]]\n",
      "activation at layer:  2 [[ 0.96984287]\n",
      " [ 0.92562529]\n",
      " [ 0.96641563]]\n",
      "New weights at layer  1 [[ 0.80671148  0.57849689  0.43999846  0.10025867  0.84416965]\n",
      " [ 0.3736577   0.62780629  0.79965108  0.52441359  0.15819517]\n",
      " [ 0.75716182  0.70076015  0.24930015  0.8008118   0.61533923]]\n",
      "New weights at layer  0 [[ 0.46897739  0.57125749  0.44514722  0.43402929]\n",
      " [ 0.59202817  0.30707646  0.37894074  0.96914466]\n",
      " [ 0.74124933  0.75476669  0.98809918  0.44566796]\n",
      " [ 0.26622017  0.16361934  0.03936841  0.62134784]\n",
      " [ 0.47175057  0.84078262  0.41678107  0.21306388]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.       ]\n",
      " [ 1.       ]\n",
      " [ 1.       ]\n",
      " [ 0.9999934]\n",
      " [ 1.       ]]\n",
      "activation at layer:  2 [[ 0.96934283]\n",
      " [ 0.92585496]\n",
      " [ 0.96581315]]\n",
      "New weights at layer  1 [[ 0.80383086  0.57561626  0.43711784  0.09737806  0.84128902]\n",
      " [ 0.36730193  0.62145052  0.79329532  0.51805786  0.1518394 ]\n",
      " [ 0.7572747   0.70087303  0.24941303  0.80092468  0.61545211]]\n",
      "New weights at layer  0 [[ 0.46897739  0.57125749  0.44514722  0.43402928]\n",
      " [ 0.59202817  0.30707646  0.37894074  0.96914466]\n",
      " [ 0.74124933  0.75476669  0.98809918  0.44566796]\n",
      " [ 0.26621996  0.16361911  0.03936816  0.62134757]\n",
      " [ 0.47175057  0.84078262  0.41678107  0.21306388]]\n",
      "Iteration:  3\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99482625]\n",
      " [ 0.99821698]\n",
      " [ 0.99962171]\n",
      " [ 0.96098545]\n",
      " [ 0.99368605]]\n",
      "activation at layer:  2 [[ 0.96838528]\n",
      " [ 0.92144068]\n",
      " [ 0.96447736]]\n",
      "New weights at layer  1 [[ 0.80392715  0.57571288  0.43721459  0.09747107  0.8413852 ]\n",
      " [ 0.36066634  0.61479231  0.78662774  0.51164799  0.14521141]\n",
      " [ 0.75398742  0.69757455  0.24610991  0.79774922  0.6121686 ]]\n",
      "New weights at layer  0 [[ 0.46895258  0.57120788  0.44507281  0.43393007]\n",
      " [ 0.59201687  0.30705385  0.37890683  0.96909945]\n",
      " [ 0.74124705  0.75476214  0.98809235  0.44565885]\n",
      " [ 0.26599353  0.16316625  0.03868887  0.62044185]\n",
      " [ 0.47173231  0.8407461   0.4167263   0.21299085]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999759]\n",
      " [ 0.99999978]\n",
      " [ 1.        ]\n",
      " [ 0.99947425]\n",
      " [ 0.99999731]]\n",
      "activation at layer:  2 [[ 0.96884074]\n",
      " [ 0.92031419]\n",
      " [ 0.96516702]]\n",
      "New weights at layer  1 [[ 0.80100238  0.57278811  0.43428982  0.09454784  0.83846044]\n",
      " [ 0.36125072  0.6153767   0.78721212  0.51223207  0.1457958 ]\n",
      " [ 0.75074257  0.69432969  0.24286505  0.79450607  0.60892375]]\n",
      "New weights at layer  0 [[ 0.46895253  0.57120781  0.44507273  0.43392998]\n",
      " [ 0.59201686  0.30705385  0.37890683  0.96909944]\n",
      " [ 0.74124705  0.75476214  0.98809235  0.44565885]\n",
      " [ 0.26598682  0.16315819  0.03867947  0.6204311 ]\n",
      " [ 0.47173225  0.84074603  0.41672622  0.21299076]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999323]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.96830832]\n",
      " [ 0.92059044]\n",
      " [ 0.96452087]]\n",
      "New weights at layer  1 [[ 0.7980309   0.56981663  0.43131834  0.09157638  0.83548896]\n",
      " [ 0.35452087  0.60864684  0.78048227  0.50550226  0.13906594]\n",
      " [ 0.75086398  0.6944511   0.24298646  0.79462748  0.60904516]]\n",
      "New weights at layer  0 [[ 0.46895253  0.57120781  0.44507273  0.43392998]\n",
      " [ 0.59201686  0.30705385  0.37890683  0.96909944]\n",
      " [ 0.74124705  0.75476214  0.98809235  0.44565885]\n",
      " [ 0.2659866   0.16315795  0.0386792   0.62043081]\n",
      " [ 0.47173225  0.84074603  0.41672622  0.21299076]]\n",
      "Iteration:  4\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99482229]\n",
      " [ 0.99821636]\n",
      " [ 0.99962168]\n",
      " [ 0.96071773]\n",
      " [ 0.99368249]]\n",
      "activation at layer:  2 [[ 0.967311  ]\n",
      " [ 0.91575878]\n",
      " [ 0.96314242]]\n",
      "New weights at layer  1 [[ 0.79813373  0.56991981  0.43142166  0.09167569  0.83559167]\n",
      " [ 0.34749286  0.60159485  0.77342035  0.49871519  0.13204598]\n",
      " [ 0.74746261  0.69103813  0.23956868  0.79134272  0.60564769]]\n",
      "New weights at layer  0 [[ 0.46892714  0.57115704  0.44499658  0.43382844]\n",
      " [ 0.59200519  0.30703051  0.37887182  0.96905277]\n",
      " [ 0.74124469  0.75475742  0.98808528  0.44564942]\n",
      " [ 0.26575189  0.16268852  0.03797506  0.61949195]\n",
      " [ 0.47171394  0.84070941  0.41667128  0.2129175 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999758]\n",
      " [ 0.99999978]\n",
      " [ 1.        ]\n",
      " [ 0.9994653 ]\n",
      " [ 0.99999731]]\n",
      "activation at layer:  2 [[ 0.96777415]\n",
      " [ 0.91433396]\n",
      " [ 0.96382915]]\n",
      "New weights at layer  1 [[ 0.79511551  0.56690158  0.42840343  0.08865907  0.83257345]\n",
      " [ 0.34816385  0.60226585  0.77409135  0.49938583  0.13271698]\n",
      " [ 0.74410247  0.68767798  0.23620853  0.78798437  0.60228755]]\n",
      "New weights at layer  0 [[ 0.46892709  0.57115698  0.4449965   0.43382835]\n",
      " [ 0.59200519  0.30703051  0.37887181  0.96905276]\n",
      " [ 0.74124469  0.75475742  0.98808528  0.44564942]\n",
      " [ 0.26574499  0.16268024  0.03796541  0.61948092]\n",
      " [ 0.47171388  0.84070933  0.41667119  0.21291741]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999305]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.96720628]\n",
      " [ 0.9146694 ]\n",
      " [ 0.9631347 ]]\n",
      "New weights at layer  1 [[ 0.79204769  0.56383376  0.42533562  0.08559128  0.82950564]\n",
      " [ 0.34102492  0.59512692  0.76695242  0.49224695  0.12557805]\n",
      " [ 0.74423337  0.68780888  0.23633943  0.78811526  0.60241845]]\n",
      "New weights at layer  0 [[ 0.46892709  0.57115698  0.4449965   0.43382835]\n",
      " [ 0.59200519  0.30703051  0.37887181  0.96905276]\n",
      " [ 0.74124469  0.75475742  0.98808528  0.44564942]\n",
      " [ 0.26574476  0.16267999  0.03796512  0.61948062]\n",
      " [ 0.47171388  0.84070933  0.41667119  0.21291741]]\n",
      "Iteration:  5\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99481823]\n",
      " [ 0.99821572]\n",
      " [ 0.99962165]\n",
      " [ 0.96043838]\n",
      " [ 0.99367892]]\n",
      "activation at layer:  2 [[ 0.96616594]\n",
      " [ 0.90935728]\n",
      " [ 0.96171107]]\n",
      "New weights at layer  1 [[ 0.79215772  0.56394417  0.42544618  0.0856975   0.82961554]\n",
      " [ 0.33356824  0.58764477  0.75945973  0.48504796  0.11812991]\n",
      " [ 0.74071042  0.6842739   0.23279947  0.78471406  0.59889953]]\n",
      "New weights at layer  0 [[ 0.46890113  0.57110506  0.44491862  0.43372452]\n",
      " [ 0.59199314  0.30700641  0.37883566  0.96900456]\n",
      " [ 0.74124225  0.75475253  0.98807794  0.44563963]\n",
      " [ 0.26550139  0.16219324  0.03723501  0.61850713]\n",
      " [ 0.47169557  0.84067272  0.41661627  0.21284418]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999758]\n",
      " [ 0.99999978]\n",
      " [ 1.        ]\n",
      " [ 0.99945585]\n",
      " [ 0.99999731]]\n",
      "activation at layer:  2 [[ 0.96663699]\n",
      " [ 0.90757344]\n",
      " [ 0.96239267]]\n",
      "New weights at layer  1 [[ 0.78904033  0.56082677  0.42232878  0.0825818   0.82649815]\n",
      " [ 0.33434355  0.58842008  0.76023504  0.48582284  0.11890521]\n",
      " [ 0.73722724  0.68079071  0.22931628  0.78123277  0.59541635]]\n",
      "New weights at layer  0 [[ 0.46890107  0.57110499  0.44491854  0.43372442]\n",
      " [ 0.59199314  0.3070064   0.37883566  0.96900455]\n",
      " [ 0.74124225  0.75475253  0.98807794  0.44563963]\n",
      " [ 0.26549431  0.16218475  0.0372251   0.61849581]\n",
      " [ 0.47169551  0.84067265  0.41661619  0.21284408]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999285]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.96603019]\n",
      " [ 0.90798475]\n",
      " [ 0.96164467]]\n",
      "New weights at layer  1 [[ 0.78587022  0.55765666  0.41915867  0.07941171  0.82332804]\n",
      " [ 0.32675748  0.58083401  0.75264897  0.47823683  0.11131914]\n",
      " [ 0.73736871  0.68093218  0.22945775  0.78137424  0.59555782]]\n",
      "New weights at layer  0 [[ 0.46890107  0.57110499  0.44491854  0.43372442]\n",
      " [ 0.59199314  0.3070064   0.37883566  0.96900455]\n",
      " [ 0.74124225  0.75475253  0.98807794  0.44563963]\n",
      " [ 0.26549407  0.16218448  0.03722481  0.61849548]\n",
      " [ 0.47169551  0.84067265  0.41661619  0.21284408]]\n",
      "Iteration:  6\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99481407]\n",
      " [ 0.99821505]\n",
      " [ 0.99962162]\n",
      " [ 0.96014676]\n",
      " [ 0.99367535]]\n",
      "activation at layer:  2 [[ 0.96494326]\n",
      " [ 0.90211785]\n",
      " [ 0.96017316]]\n",
      "New weights at layer  1 [[ 0.7859882   0.55777504  0.41927722  0.07952558  0.82344588]\n",
      " [ 0.31883297  0.57288242  0.74468617  0.47058848  0.10340371]\n",
      " [ 0.73371598  0.67726697  0.22578737  0.77784881  0.59190928]]\n",
      "New weights at layer  0 [[ 0.46887455  0.57105195  0.44483898  0.43361834]\n",
      " [ 0.59198069  0.30698151  0.37879833  0.96895478]\n",
      " [ 0.74123971  0.75474745  0.98807032  0.44562947]\n",
      " [ 0.2652417   0.16167975  0.0364677   0.61748601]\n",
      " [ 0.47167729  0.8406362   0.41656152  0.21277119]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999757]\n",
      " [ 0.99999978]\n",
      " [ 1.        ]\n",
      " [ 0.99944587]\n",
      " [ 0.9999973 ]]\n",
      "activation at layer:  2 [[ 0.9654224 ]\n",
      " [ 0.89990144]\n",
      " [ 0.96084696]]\n",
      "New weights at layer  1 [[ 0.78276543  0.55455227  0.41605444  0.07630459  0.82022311]\n",
      " [ 0.31973465  0.57378409  0.74558785  0.47148966  0.10430538]\n",
      " [ 0.73010128  0.67365226  0.22217266  0.77423609  0.58829458]]\n",
      "New weights at layer  0 [[ 0.46887449  0.57105188  0.4448389   0.43361825]\n",
      " [ 0.59198069  0.30698151  0.37879832  0.96895477]\n",
      " [ 0.74123971  0.75474745  0.98807032  0.44562947]\n",
      " [ 0.26523445  0.16167104  0.03645755  0.61747441]\n",
      " [ 0.47167722  0.84063613  0.41656143  0.21277109]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999265]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.96477275]\n",
      " [ 0.90041085]\n",
      " [ 0.96003938]]\n",
      "New weights at layer  1 [[ 0.77948653  0.55127336  0.41277554  0.07302571  0.81694421]\n",
      " [ 0.31166056  0.56571001  0.73751376  0.46341563  0.0962313 ]\n",
      " [ 0.73025458  0.67380556  0.22232597  0.7743894   0.58844788]]\n",
      "New weights at layer  0 [[ 0.46887449  0.57105188  0.4448389   0.43361825]\n",
      " [ 0.59198069  0.30698151  0.37879832  0.96895477]\n",
      " [ 0.74123971  0.75474745  0.98807032  0.44562947]\n",
      " [ 0.26523419  0.16167076  0.03645724  0.61747407]\n",
      " [ 0.47167722  0.84063613  0.41656143  0.21277109]]\n",
      "Iteration:  7\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99480983]\n",
      " [ 0.99821436]\n",
      " [ 0.99962159]\n",
      " [ 0.95984223]\n",
      " [ 0.99367179]]\n",
      "activation at layer:  2 [[ 0.96363519]\n",
      " [ 0.89390186]\n",
      " [ 0.95851711]]\n",
      "New weights at layer  1 [[ 0.7796133   0.55140057  0.41290292  0.07314802  0.81707083]\n",
      " [ 0.30322668  0.55724726  0.72903908  0.4552782   0.08780706]\n",
      " [ 0.7264631   0.6700011   0.21851615  0.77073119  0.58466074]]\n",
      "New weights at layer  0 [[ 0.46884743  0.57099777  0.44475773  0.43351002]\n",
      " [ 0.59196784  0.30695581  0.37875978  0.96890338]\n",
      " [ 0.74123708  0.75474218  0.98806242  0.44561894]\n",
      " [ 0.26497255  0.16114748  0.03567231  0.6164275 ]\n",
      " [ 0.47165919  0.84060005  0.41650732  0.21269894]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999757]\n",
      " [ 0.99999978]\n",
      " [ 1.        ]\n",
      " [ 0.99943534]\n",
      " [ 0.9999973 ]]\n",
      "activation at layer:  2 [[ 0.96412264]\n",
      " [ 0.89116416]\n",
      " [ 0.95917991]]\n",
      "New weights at layer  1 [[ 0.77627839  0.54806565  0.409568    0.06981499  0.81373593]\n",
      " [ 0.30428228  0.55830287  0.73009469  0.45633321  0.08886267]\n",
      " [ 0.72270756  0.66624555  0.21476059  0.76697775  0.58090519]]\n",
      "New weights at layer  0 [[ 0.46884737  0.57099769  0.44475764  0.43350993]\n",
      " [ 0.59196784  0.30695581  0.37875977  0.96890337]\n",
      " [ 0.74123708  0.75474218  0.98806242  0.44561894]\n",
      " [ 0.26496513  0.16113857  0.03566192  0.61641562]\n",
      " [ 0.47165912  0.84059997  0.41650723  0.21269884]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999243]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.96342569]\n",
      " [ 0.89180146]\n",
      " [ 0.95830584]]\n",
      "New weights at layer  1 [[ 0.7728836   0.54467086  0.40617322  0.06642023  0.81034114]\n",
      " [ 0.29567715  0.54969773  0.72148955  0.44772813  0.08025753]\n",
      " [ 0.72287415  0.66641214  0.21492718  0.76714434  0.58107178]]\n",
      "New weights at layer  0 [[ 0.46884737  0.57099769  0.44475764  0.43350993]\n",
      " [ 0.59196784  0.30695581  0.37875977  0.96890337]\n",
      " [ 0.74123708  0.75474218  0.98806242  0.44561894]\n",
      " [ 0.26496486  0.16113827  0.03566159  0.61641526]\n",
      " [ 0.47165912  0.84059997  0.41650723  0.21269884]]\n",
      "Iteration:  8\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99480549]\n",
      " [ 0.99821365]\n",
      " [ 0.99962156]\n",
      " [ 0.9595242 ]\n",
      " [ 0.99366827]]\n",
      "activation at layer:  2 [[ 0.96223298]\n",
      " [ 0.88454854]\n",
      " [ 0.95672971]]\n",
      "New weights at layer  1 [[ 0.77302014  0.54480786  0.40631041  0.06655192  0.81047752]\n",
      " [ 0.28669084  0.54068064  0.71245975  0.43906054  0.0712815 ]\n",
      " [ 0.71893406  0.66245855  0.21096801  0.76334399  0.5771362 ]]\n",
      "New weights at layer  0 [[ 0.46881983  0.5709426   0.444675    0.43339973]\n",
      " [ 0.59195458  0.3069293   0.37872001  0.96885036]\n",
      " [ 0.74123435  0.75473672  0.98805423  0.44560802]\n",
      " [ 0.26469376  0.16059607  0.03484829  0.61533086]\n",
      " [ 0.47164139  0.84056451  0.41645403  0.2126279 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999756]\n",
      " [ 0.99999978]\n",
      " [ 1.        ]\n",
      " [ 0.99942422]\n",
      " [ 0.9999973 ]]\n",
      "activation at layer:  2 [[ 0.96272894]\n",
      " [ 0.88118339]\n",
      " [ 0.95737764]]\n",
      "New weights at layer  1 [[ 0.76956569  0.54135341  0.40285596  0.06309945  0.80702307]\n",
      " [ 0.28793484  0.54192464  0.71370375  0.44030382  0.0725255 ]\n",
      " [ 0.71502742  0.6585519   0.20706137  0.75943959  0.57322956]]\n",
      "New weights at layer  0 [[ 0.46881976  0.57094252  0.44467491  0.43339963]\n",
      " [ 0.59195458  0.3069293   0.37872     0.96885035]\n",
      " [ 0.74123435  0.75473672  0.98805423  0.44560802]\n",
      " [ 0.26468617  0.16058696  0.03483767  0.61531872]\n",
      " [ 0.47164132  0.84056443  0.41645393  0.2126278 ]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999219]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.96197966]\n",
      " [ 0.88198863]\n",
      " [ 0.95642915]]\n",
      "New weights at layer  1 [[ 0.76604727  0.53783499  0.39933754  0.05958106  0.80350465]\n",
      " [ 0.27875469  0.53274449  0.7045236   0.43112374  0.06334535]\n",
      " [ 0.71520899  0.65873347  0.20724294  0.75962116  0.57341113]]\n",
      "New weights at layer  0 [[ 0.46881976  0.57094252  0.44467491  0.43339963]\n",
      " [ 0.59195458  0.3069293   0.37872     0.96885035]\n",
      " [ 0.74123435  0.75473672  0.98805423  0.44560802]\n",
      " [ 0.26468589  0.16058665  0.03483732  0.61531835]\n",
      " [ 0.47164132  0.84056443  0.41645393  0.2126278 ]]\n",
      "Iteration:  9\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99480107]\n",
      " [ 0.99821292]\n",
      " [ 0.99962153]\n",
      " [ 0.95919219]\n",
      " [ 0.99366481]]\n",
      "activation at layer:  2 [[ 0.96072671]\n",
      " [ 0.87387439]\n",
      " [ 0.95479584]]\n",
      "New weights at layer  1 [[ 0.76619468  0.5379829   0.39948566  0.05972319  0.80365189]\n",
      " [ 0.2691731   0.52313004  0.69489558  0.42188513  0.0537747 ]\n",
      " [ 0.71110944  0.65461987  0.20312353  0.75566836  0.56931627]]\n",
      "New weights at layer  0 [[ 0.46879179  0.57088657  0.44459098  0.43328772]\n",
      " [ 0.59194092  0.30690198  0.37867903  0.96879572]\n",
      " [ 0.74123152  0.75473107  0.98804575  0.44559672]\n",
      " [ 0.26440529  0.16002545  0.03399552  0.61419594]\n",
      " [ 0.47162404  0.84052987  0.41640209  0.21255868]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999756]\n",
      " [ 0.99999978]\n",
      " [ 1.        ]\n",
      " [ 0.99941248]\n",
      " [ 0.99999729]]\n",
      "activation at layer:  2 [[ 0.96123137]\n",
      " [ 0.86975644]\n",
      " [ 0.95542423]]\n",
      "New weights at layer  1 [[ 0.7626126   0.53440082  0.39590357  0.05614321  0.80006981]\n",
      " [ 0.2706485   0.52460544  0.69637098  0.42335966  0.0552501 ]\n",
      " [ 0.70704042  0.65055084  0.19905449  0.75160171  0.56524724]]\n",
      "New weights at layer  0 [[ 0.46879172  0.57088649  0.44459089  0.43328762]\n",
      " [ 0.59194092  0.30690197  0.37867902  0.96879571]\n",
      " [ 0.74123152  0.75473107  0.98804575  0.44559672]\n",
      " [ 0.26439755  0.16001617  0.03398469  0.61418357]\n",
      " [ 0.47162397  0.84052978  0.416402    0.21255857]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999194]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.96042401]\n",
      " [ 0.87078348]\n",
      " [ 0.95439214]]\n",
      "New weights at layer  1 [[ 0.75896205  0.53075027  0.39225303  0.05249269  0.79641927]\n",
      " [ 0.26085048  0.51480742  0.68657296  0.41356172  0.04545208]\n",
      " [ 0.70723894  0.65074936  0.19925302  0.75180023  0.56544577]]\n",
      "New weights at layer  0 [[ 0.46879172  0.57088649  0.44459089  0.43328762]\n",
      " [ 0.59194092  0.30690197  0.37867902  0.96879571]\n",
      " [ 0.74123152  0.75473107  0.98804575  0.44559672]\n",
      " [ 0.26439726  0.16001584  0.03398433  0.61418317]\n",
      " [ 0.47162397  0.84052978  0.416402    0.21255857]]\n",
      "Iteration:  10\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99479658]\n",
      " [ 0.99821216]\n",
      " [ 0.9996215 ]\n",
      " [ 0.95884586]\n",
      " [ 0.99366143]]\n",
      "activation at layer:  2 [[ 0.95910506]\n",
      " [ 0.8616752 ]\n",
      " [ 0.95269817]]\n",
      "New weights at layer  1 [[ 0.75912162  0.53091038  0.39241337  0.05264649  0.79657865]\n",
      " [ 0.25063352  0.50455538  0.67630645  0.40371399  0.03524678]\n",
      " [ 0.70296801  0.64646376  0.19496137  0.74768365  0.5611797 ]]\n",
      "New weights at layer  0 [[ 0.46876341  0.57082986  0.44450594  0.43317435]\n",
      " [ 0.59192687  0.30687388  0.37863687  0.96873951]\n",
      " [ 0.7412286   0.75472523  0.98803699  0.44558503]\n",
      " [ 0.2641073   0.15943593  0.03311447  0.61302337]\n",
      " [ 0.47160732  0.84049649  0.41635205  0.21249197]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999755]\n",
      " [ 0.99999978]\n",
      " [ 1.        ]\n",
      " [ 0.9994001 ]\n",
      " [ 0.99999729]]\n",
      "activation at layer:  2 [[ 0.95961862]\n",
      " [ 0.85665947]\n",
      " [ 0.95330135]]\n",
      "New weights at layer  1 [[ 0.75540303  0.52719179  0.38869477  0.04893013  0.79286007]\n",
      " [ 0.25239365  0.50631552  0.67806659  0.40547307  0.03700691]\n",
      " [ 0.69872412  0.64221986  0.19071747  0.7434423   0.55693582]]\n",
      "New weights at layer  0 [[ 0.46876334  0.57082978  0.44450585  0.43317425]\n",
      " [ 0.59192686  0.30687387  0.37863687  0.96873951]\n",
      " [ 0.7412286   0.75472523  0.98803699  0.44558503]\n",
      " [ 0.26409944  0.1594265   0.03310346  0.61301079]\n",
      " [ 0.47160725  0.8404964   0.41635195  0.21249185]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999168]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.9587466 ]\n",
      " [ 0.85798047]\n",
      " [ 0.95217502]]\n",
      "New weights at layer  1 [[ 0.75161104  0.5233998   0.38490278  0.04513817  0.78906808]\n",
      " [ 0.24193916  0.49586103  0.66761209  0.39501867  0.02655242]\n",
      " [ 0.6989419   0.64243765  0.19093525  0.74366008  0.5571536 ]]\n",
      "New weights at layer  0 [[ 0.46876334  0.57082978  0.44450585  0.43317425]\n",
      " [ 0.59192686  0.30687387  0.37863687  0.96873951]\n",
      " [ 0.7412286   0.75472523  0.98803699  0.44558503]\n",
      " [ 0.26409913  0.15942615  0.03310309  0.61301037]\n",
      " [ 0.47160725  0.8404964   0.41635195  0.21249185]]\n",
      "Iteration:  11\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99479203]\n",
      " [ 0.99821139]\n",
      " [ 0.99962146]\n",
      " [ 0.95848511]\n",
      " [ 0.99365817]]\n",
      "activation at layer:  2 [[ 0.95735513]\n",
      " [ 0.84773247]\n",
      " [ 0.95041667]]\n",
      "New weights at layer  1 [[ 0.75178424  0.52357359  0.38507682  0.04530505  0.78924108]\n",
      " [ 0.23105344  0.48493789  0.65667353  0.38453024  0.01567911]\n",
      " [ 0.69448641  0.63796684  0.18645813  0.73936719  0.55270319]]\n",
      "New weights at layer  0 [[ 0.4687348   0.57077271  0.44442024  0.43306011]\n",
      " [ 0.59191245  0.30684504  0.37859363  0.96868185]\n",
      " [ 0.74122559  0.75471921  0.98802796  0.445573  ]\n",
      " [ 0.26380024  0.15882837  0.03220642  0.61181481]\n",
      " [ 0.47159144  0.84046477  0.4163045   0.2124286 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999755]\n",
      " [ 0.99999978]\n",
      " [ 1.        ]\n",
      " [ 0.99938706]\n",
      " [ 0.99999729]]\n",
      "activation at layer:  2 [[ 0.95787778]\n",
      " [ 0.84165646]\n",
      " [ 0.95098778]]\n",
      "New weights at layer  1 [[ 0.74791941  0.51970875  0.38121198  0.04144258  0.78537625]\n",
      " [ 0.23316369  0.48704815  0.65878378  0.3866392   0.01778936]\n",
      " [ 0.69005386  0.63353428  0.18202557  0.73493735  0.54827064]]\n",
      "New weights at layer  0 [[ 0.46873474  0.57077263  0.44442015  0.43306   ]\n",
      " [ 0.59191244  0.30684504  0.37859362  0.96868185]\n",
      " [ 0.74122559  0.75471921  0.98802796  0.445573  ]\n",
      " [ 0.26379227  0.15881881  0.03219526  0.61180206]\n",
      " [ 0.47159136  0.84046468  0.4163044   0.21242848]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999139]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.95693355]\n",
      " [ 0.8433672 ]\n",
      " [ 0.94975481]]\n",
      "New weights at layer  1 [[ 0.74397572  0.51576507  0.37726829  0.03749892  0.78143256]\n",
      " [ 0.2220229   0.47590735  0.64764299  0.37549851  0.00664857]\n",
      " [ 0.69029364  0.63377405  0.18226534  0.73517713  0.54851041]]\n",
      "New weights at layer  0 [[ 0.46873474  0.57077263  0.44442015  0.43306   ]\n",
      " [ 0.59191244  0.30684504  0.37859362  0.96868185]\n",
      " [ 0.74122559  0.75471921  0.98802796  0.445573  ]\n",
      " [ 0.26379195  0.15881845  0.03219487  0.61180163]\n",
      " [ 0.47159136  0.84046468  0.4163044   0.21242848]]\n",
      "Iteration:  12\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99478744]\n",
      " [ 0.99821059]\n",
      " [ 0.99962143]\n",
      " [ 0.95811015]\n",
      " [ 0.99365507]]\n",
      "activation at layer:  2 [[ 0.95546213]\n",
      " [ 0.83182697]\n",
      " [ 0.94792824]]\n",
      "New weights at layer  1 [[ 0.74416426  0.51595425  0.37745775  0.03768051  0.78162088]\n",
      " [ 0.21044706  0.46429168  0.6360109   0.36434946 -0.0049141 ]\n",
      " [ 0.68563902  0.62910342  0.17758811  0.73069413  0.5438611 ]]\n",
      "New weights at layer  0 [[ 0.46870614  0.57071542  0.44433434  0.43294559]\n",
      " [ 0.59189771  0.30681557  0.37854942  0.96862291]\n",
      " [ 0.7412225   0.75471303  0.9880187   0.44556064]\n",
      " [ 0.26348486  0.15820426  0.03127358  0.61057325]\n",
      " [ 0.47157661  0.84043518  0.41626015  0.21236948]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999754]\n",
      " [ 0.99999977]\n",
      " [ 1.        ]\n",
      " [ 0.99937338]\n",
      " [ 0.99999728]]\n",
      "activation at layer:  2 [[ 0.95599406]\n",
      " [ 0.82451709]\n",
      " [ 0.94845891]]\n",
      "New weights at layer  1 [[ 0.74014246  0.51193244  0.37343594  0.03366122  0.77759908]\n",
      " [ 0.21298609  0.46683072  0.63854993  0.36688691 -0.00237507]\n",
      " [ 0.68100253  0.62446692  0.17295161  0.72606053  0.53922461]]\n",
      "New weights at layer  0 [[ 0.46870607  0.57071534  0.44433424  0.43294548]\n",
      " [ 0.59189771  0.30681557  0.37854942  0.96862291]\n",
      " [ 0.7412225   0.75471303  0.9880187   0.44556064]\n",
      " [ 0.26347681  0.15819461  0.03126232  0.61056038]\n",
      " [ 0.47157654  0.84043509  0.41626005  0.21236936]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999109]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.95496893]\n",
      " [ 0.82674252]\n",
      " [ 0.94710483]]\n",
      "New weights at layer  1 [[ 0.73603578  0.50782576  0.36932926  0.02955458  0.7734924 ]\n",
      " [ 0.20114389  0.45498851  0.62670773  0.35504481 -0.01421727]\n",
      " [ 0.68126752  0.62473191  0.1732166   0.72632552  0.5394896 ]]\n",
      "New weights at layer  0 [[ 0.46870607  0.57071534  0.44433424  0.43294548]\n",
      " [ 0.59189771  0.30681557  0.37854942  0.96862291]\n",
      " [ 0.7412225   0.75471303  0.9880187   0.44556064]\n",
      " [ 0.26347648  0.15819424  0.03126191  0.61055994]\n",
      " [ 0.47157654  0.84043509  0.41626005  0.21236936]]\n",
      "Iteration:  13\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99478283]\n",
      " [ 0.99820977]\n",
      " [ 0.99962139]\n",
      " [ 0.9577216 ]\n",
      " [ 0.99365218]]\n",
      "activation at layer:  2 [[ 0.95340905]\n",
      " [ 0.81376206]\n",
      " [ 0.94520606]]\n",
      "New weights at layer  1 [[ 0.73624166  0.50803235  0.36953614  0.02975278  0.77369805]\n",
      " [ 0.18887539  0.44267775  0.61437956  0.34323338 -0.02647182]\n",
      " [ 0.67639769  0.61984531  0.16832308  0.72163711  0.5346253 ]]\n",
      "New weights at layer  0 [[ 0.46867758  0.57065837  0.44424879  0.43283155]\n",
      " [ 0.59188272  0.30678559  0.37850444  0.96856294]\n",
      " [ 0.74121935  0.75470673  0.98800925  0.44554804]\n",
      " [ 0.26316228  0.15756585  0.03031933  0.60930317]\n",
      " [ 0.4715631   0.84040821  0.41621973  0.2123156 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999754]\n",
      " [ 0.99999977]\n",
      " [ 1.        ]\n",
      " [ 0.99935907]\n",
      " [ 0.99999728]]\n",
      "activation at layer:  2 [[ 0.95395043]\n",
      " [ 0.80504633]\n",
      " [ 0.94568609]]\n",
      "New weights at layer  1 [[ 0.73205106  0.50384174  0.36534553  0.02556486  0.76950745]\n",
      " [ 0.19193512  0.44573749  0.6174393   0.34629116 -0.0234121 ]\n",
      " [ 0.67154029  0.61498789  0.16346567  0.71678281  0.5297679 ]]\n",
      "New weights at layer  0 [[ 0.46867751  0.57065829  0.4442487   0.43283143]\n",
      " [ 0.59188271  0.30678558  0.37850444  0.96856294]\n",
      " [ 0.74121935  0.75470673  0.98800925  0.44554804]\n",
      " [ 0.26315418  0.15755613  0.03030799  0.60929021]\n",
      " [ 0.47156302  0.84040812  0.41621962  0.21231548]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999077]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.95283435]\n",
      " [ 0.80794491]\n",
      " [ 0.94419389]]\n",
      "New weights at layer  1 [[ 0.72776892  0.49955961  0.36106339  0.02128276  0.76522531]\n",
      " [ 0.17939824  0.43320061  0.60490242  0.3337544  -0.03594897]\n",
      " [ 0.67183434  0.61528195  0.16375972  0.71707686  0.53006196]]\n",
      "New weights at layer  0 [[ 0.46867751  0.57065829  0.4442487   0.43283143]\n",
      " [ 0.59188271  0.30678558  0.37850444  0.96856294]\n",
      " [ 0.74121935  0.75470673  0.98800925  0.44554804]\n",
      " [ 0.26315385  0.15755576  0.03030758  0.60928976]\n",
      " [ 0.47156302  0.84040812  0.41621962  0.21231548]]\n",
      "Iteration:  14\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99477824]\n",
      " [ 0.99820894]\n",
      " [ 0.99962135]\n",
      " [ 0.95732059]\n",
      " [ 0.99364955]]\n",
      "activation at layer:  2 [[ 0.95117628]\n",
      " [ 0.79339883]\n",
      " [ 0.94221892]]\n",
      "New weights at layer  1 [[ 0.72799447  0.49978594  0.36129004  0.02149982  0.76545061]\n",
      " [ 0.16646098  0.42021874  0.59190218  0.32130428 -0.04887155]\n",
      " [ 0.66673146  0.61016147  0.158632    0.71216613  0.52496486]]\n",
      "New weights at layer  0 [[ 0.46864936  0.57060198  0.44416423  0.43271882]\n",
      " [ 0.59186755  0.30675525  0.37845895  0.96850228]\n",
      " [ 0.74121616  0.75470035  0.98799967  0.44553528]\n",
      " [ 0.26283405  0.15691618  0.02934821  0.60801059]\n",
      " [ 0.47155113  0.84038434  0.41618396  0.21226793]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999754]\n",
      " [ 0.99999977]\n",
      " [ 1.        ]\n",
      " [ 0.99934416]\n",
      " [ 0.99999728]]\n",
      "activation at layer:  2 [[ 0.95172729]\n",
      " [ 0.78312741]\n",
      " [ 0.94263583]]\n",
      "New weights at layer  1 [[ 0.72362202  0.49541347  0.35691757  0.01713022  0.76107815]\n",
      " [ 0.17014432  0.42390208  0.59558552  0.32498521 -0.04518822]\n",
      " [ 0.66163431  0.6050643   0.15353483  0.70707231  0.51986771]]\n",
      "New weights at layer  0 [[ 0.46864929  0.57060189  0.44416413  0.4327187 ]\n",
      " [ 0.59186754  0.30675525  0.37845894  0.96850227]\n",
      " [ 0.74121616  0.75470035  0.98799967  0.44553528]\n",
      " [ 0.26282592  0.15690642  0.02933682  0.60799758]\n",
      " [ 0.47155105  0.84038424  0.41618384  0.2122678 ]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999043]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.9505086 ]\n",
      " [ 0.78689183]\n",
      " [ 0.94098547]]\n",
      "New weights at layer  1 [[ 0.71915063  0.49094209  0.35244619  0.01265888  0.75660677]\n",
      " [ 0.15694868  0.41070644  0.58238989  0.3117897  -0.05838385]\n",
      " [ 0.66196203  0.60539202  0.15386255  0.70740002  0.52019543]]\n",
      "New weights at layer  0 [[ 0.46864929  0.57060189  0.44416413  0.4327187 ]\n",
      " [ 0.59186754  0.30675525  0.37845894  0.96850227]\n",
      " [ 0.74121616  0.75470035  0.98799967  0.44553528]\n",
      " [ 0.26282558  0.15690604  0.02933641  0.60799713]\n",
      " [ 0.47155105  0.84038424  0.41618384  0.2122678 ]]\n",
      "Iteration:  15\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.9947737 ]\n",
      " [ 0.9982081 ]\n",
      " [ 0.99962132]\n",
      " [ 0.95690876]\n",
      " [ 0.99364721]]\n",
      "activation at layer:  2 [[ 0.94874109]\n",
      " [ 0.77070197]\n",
      " [ 0.93893039]]\n",
      "New weights at layer  1 [[ 0.71939861  0.49119092  0.35269537  0.01289742  0.75685447]\n",
      " [ 0.14339999  0.39711097  0.56877517  0.29875672 -0.07191721]\n",
      " [ 0.65660633  0.60001783  0.14848075  0.70224818  0.5148458 ]]\n",
      "New weights at layer  0 [[ 0.46862169  0.57054669  0.44408133  0.4326083 ]\n",
      " [ 0.59185231  0.30672478  0.37841324  0.96844134]\n",
      " [ 0.74121296  0.75469395  0.98799007  0.44552247]\n",
      " [ 0.26250203  0.15625894  0.02836576  0.60670293]\n",
      " [ 0.47154093  0.840364    0.41615347  0.2122273 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999753]\n",
      " [ 0.99999977]\n",
      " [ 1.        ]\n",
      " [ 0.99932874]\n",
      " [ 0.99999728]]\n",
      "activation at layer:  2 [[ 0.94930193]\n",
      " [ 0.75877487]\n",
      " [ 0.93926888]]\n",
      "New weights at layer  1 [[ 0.71482984  0.48662214  0.3481266   0.00833171  0.7522857 ]\n",
      " [ 0.14781525  0.40152625  0.57319044  0.30316903 -0.06750194]\n",
      " [ 0.65124848  0.59465998  0.1431229   0.69689392  0.50948796]]\n",
      "New weights at layer  0 [[ 0.46862161  0.5705466   0.44408123  0.43260818]\n",
      " [ 0.59185231  0.30672478  0.37841323  0.96844133]\n",
      " [ 0.74121296  0.75469395  0.98799007  0.44552247]\n",
      " [ 0.26249387  0.15624915  0.02835433  0.60668987]\n",
      " [ 0.47154084  0.84036389  0.41615335  0.21222716]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99999008]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.94796702]\n",
      " [ 0.76362566]\n",
      " [ 0.93743666]]\n",
      "New weights at layer  1 [[ 0.71015394  0.48194624  0.3434507   0.00365585  0.7476098 ]\n",
      " [ 0.13403169  0.38774269  0.55940688  0.28938561 -0.0812855 ]\n",
      " [ 0.65161541  0.5950269   0.14348982  0.69726084  0.50985488]]\n",
      "New weights at layer  0 [[ 0.46862161  0.5705466   0.44408123  0.43260818]\n",
      " [ 0.59185231  0.30672478  0.37841323  0.96844133]\n",
      " [ 0.74121296  0.75469395  0.98799007  0.44552247]\n",
      " [ 0.26249354  0.15624877  0.02835392  0.60668942]\n",
      " [ 0.47154084  0.84036389  0.41615335  0.21222716]]\n",
      "Iteration:  16\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99476925]\n",
      " [ 0.99820726]\n",
      " [ 0.99962128]\n",
      " [ 0.95648829]\n",
      " [ 0.99364522]]\n",
      "activation at layer:  2 [[ 0.94607706]\n",
      " [ 0.74578928]\n",
      " [ 0.93529779]]\n",
      "New weights at layer  1 [[ 0.71042759  0.48222084  0.34372568  0.00391897  0.74788314]\n",
      " [ 0.11996641  0.37362879  0.545273    0.27586159 -0.09533489]\n",
      " [ 0.64598499  0.58937702  0.13783194  0.69184709  0.50423082]]\n",
      "New weights at layer  0 [[ 0.46859478  0.57049293  0.44400072  0.43250084]\n",
      " [ 0.59183712  0.3066944   0.37836768  0.96838059]\n",
      " [ 0.74120978  0.75468759  0.98798053  0.44550976]\n",
      " [ 0.26216828  0.15559825  0.02737814  0.60538838]\n",
      " [ 0.47153263  0.84034747  0.41612872  0.21219432]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999753]\n",
      " [ 0.99999977]\n",
      " [ 1.        ]\n",
      " [ 0.99931286]\n",
      " [ 0.99999727]]\n",
      "activation at layer:  2 [[ 0.94664795]\n",
      " [ 0.73218707]\n",
      " [ 0.93553907]]\n",
      "New weights at layer  1 [[ 0.7056465   0.47743974  0.33894458 -0.00085884  0.74310205]\n",
      " [ 0.12521792  0.37888031  0.55052452  0.28110951 -0.09008338]\n",
      " [ 0.64034316  0.58373518  0.1321901   0.68620913  0.498589  ]]\n",
      "New weights at layer  0 [[ 0.4685947   0.57049284  0.44400061  0.43250072]\n",
      " [ 0.59183712  0.3066944   0.37836767  0.96838058]\n",
      " [ 0.74120978  0.75468759  0.98798053  0.44550976]\n",
      " [ 0.26216007  0.1555884   0.02736665  0.60537525]\n",
      " [ 0.47153253  0.84034736  0.41612859  0.21219417]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99998971]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.94518093]\n",
      " [ 0.73835568]\n",
      " [ 0.93349683]]\n",
      "New weights at layer  1 [[ 0.70074915  0.47254238  0.33404722 -0.00575615  0.7382047 ]\n",
      " [ 0.11095388  0.36461627  0.53626048  0.26684561 -0.10434742]\n",
      " [ 0.64075602  0.58414804  0.13260296  0.68662198  0.49900186]]\n",
      "New weights at layer  0 [[ 0.4685947   0.57049284  0.44400061  0.43250072]\n",
      " [ 0.59183712  0.3066944   0.37836767  0.96838058]\n",
      " [ 0.74120978  0.75468759  0.98798053  0.44550976]\n",
      " [ 0.26215974  0.15558804  0.02736625  0.60537482]\n",
      " [ 0.47153253  0.84034736  0.41612859  0.21219417]]\n",
      "Iteration:  17\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99476491]\n",
      " [ 0.99820641]\n",
      " [ 0.99962124]\n",
      " [ 0.95606164]\n",
      " [ 0.99364361]]\n",
      "activation at layer:  2 [[ 0.94315338]\n",
      " [ 0.71897016]\n",
      " [ 0.93127099]]\n",
      "New weights at layer  1 [[ 0.70105234  0.47284662  0.33435189 -0.00546476  0.73850754]\n",
      " [ 0.09650299  0.35011539  0.52173904  0.25295696 -0.11878203]\n",
      " [ 0.63482659  0.5781981   0.12664458  0.68092325  0.49307911]]\n",
      "New weights at layer  0 [[ 0.4685688   0.57044105  0.44392293  0.43239714]\n",
      " [ 0.5918221   0.30666436  0.37832262  0.96832051]\n",
      " [ 0.74120666  0.75468136  0.98797118  0.44549729]\n",
      " [ 0.26183481  0.15493818  0.02639145  0.60407508]\n",
      " [ 0.47152629  0.84033487  0.41610986  0.2121692 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999752]\n",
      " [ 0.99999977]\n",
      " [ 1.        ]\n",
      " [ 0.99929663]\n",
      " [ 0.99999727]]\n",
      "activation at layer:  2 [[ 0.9437346 ]\n",
      " [ 0.70377869]\n",
      " [ 0.93139195]]\n",
      "New weights at layer  1 [[ 0.69604115  0.46783543  0.3293407  -0.01047243  0.73349636]\n",
      " [ 0.10267842  0.35629084  0.52791449  0.25912807 -0.11260659]\n",
      " [ 0.62887492  0.57224641  0.1206929   0.67497575  0.48712744]]\n",
      "New weights at layer  0 [[ 0.46856872  0.57044095  0.44392282  0.43239701]\n",
      " [ 0.59182209  0.30666436  0.37832261  0.9683205 ]\n",
      " [ 0.74120666  0.75468136  0.98797118  0.44549729]\n",
      " [ 0.2618265   0.1549282   0.02637982  0.60406179]\n",
      " [ 0.47152619  0.84033475  0.41610972  0.21216904]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99998932]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.94211678]\n",
      " [ 0.71147952]\n",
      " [ 0.92910615]]\n",
      "New weights at layer  1 [[ 0.69090353  0.4626978   0.32420308 -0.01560999  0.72835874]\n",
      " [ 0.08807343  0.34168584  0.5133095   0.24452323 -0.12721159]\n",
      " [ 0.62934188  0.57271338  0.12115986  0.67544271  0.48759441]]\n",
      "New weights at layer  0 [[ 0.46856872  0.57044095  0.44392282  0.43239701]\n",
      " [ 0.59182209  0.30666436  0.37832261  0.9683205 ]\n",
      " [ 0.74120666  0.75468136  0.98797118  0.44549729]\n",
      " [ 0.26182619  0.15492786  0.02637944  0.60406138]\n",
      " [ 0.47152619  0.84033475  0.41610972  0.21216904]]\n",
      "Iteration:  18\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99476072]\n",
      " [ 0.99820558]\n",
      " [ 0.99962121]\n",
      " [ 0.95563139]\n",
      " [ 0.99364238]]\n",
      "activation at layer:  2 [[ 0.93993396]\n",
      " [ 0.69075345]\n",
      " [ 0.92679095]]\n",
      "New weights at layer  1 [[ 0.69124088  0.46303632  0.32454207 -0.01528592  0.72869571]\n",
      " [ 0.07339534  0.32695692  0.49855968  0.23042251 -0.14187318]\n",
      " [ 0.6230866   0.56643643  0.11487401  0.66943348  0.48134615]]\n",
      "New weights at layer  0 [[ 0.46854388  0.57039127  0.44384829  0.43229764]\n",
      " [ 0.59180735  0.30663488  0.37827839  0.96826154]\n",
      " [ 0.74120365  0.75467532  0.98796213  0.44548522]\n",
      " [ 0.26150333  0.15428214  0.02541085  0.60276992]\n",
      " [ 0.47152186  0.84032608  0.41609671  0.2121517 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999752]\n",
      " [ 0.99999977]\n",
      " [ 1.        ]\n",
      " [ 0.99928011]\n",
      " [ 0.99999727]]\n",
      "activation at layer:  2 [[ 0.94052589]\n",
      " [ 0.67417147]\n",
      " [ 0.92676316]]\n",
      "New weights at layer  1 [[ 0.68597988  0.4577753   0.31928106 -0.02054314  0.72343471]\n",
      " [ 0.08055261  0.33411421  0.50571697  0.23757464 -0.13471591]\n",
      " [ 0.61679638  0.5601462   0.10858377  0.66314777  0.47505593]]\n",
      "New weights at layer  0 [[ 0.46854379  0.57039116  0.44384817  0.4322975 ]\n",
      " [ 0.59180735  0.30663487  0.37827838  0.96826154]\n",
      " [ 0.74120365  0.75467532  0.98796213  0.44548522]\n",
      " [ 0.26149483  0.15427194  0.02539895  0.60275632]\n",
      " [ 0.47152175  0.84032596  0.41609656  0.21215153]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99998893]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.93873525]\n",
      " [ 0.68356738]\n",
      " [ 0.9241937 ]]\n",
      "New weights at layer  1 [[ 0.68058108  0.45237651  0.31388226 -0.02594188  0.71803591]\n",
      " [ 0.06576684  0.31932844  0.49093121  0.22278904 -0.14950168]\n",
      " [ 0.61732748  0.56067729  0.10911487  0.66367886  0.47558703]]\n",
      "New weights at layer  0 [[ 0.46854379  0.57039116  0.44384817  0.4322975 ]\n",
      " [ 0.59180735  0.30663487  0.37827838  0.96826154]\n",
      " [ 0.74120365  0.75467532  0.98796213  0.44548522]\n",
      " [ 0.26149455  0.15427163  0.02539861  0.60275595]\n",
      " [ 0.47152175  0.84032596  0.41609656  0.21215153]]\n",
      "Iteration:  19\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.9947567 ]\n",
      " [ 0.99820476]\n",
      " [ 0.99962117]\n",
      " [ 0.95519977]\n",
      " [ 0.99364152]]\n",
      "activation at layer:  2 [[ 0.9363764 ]\n",
      " [ 0.66180954]\n",
      " [ 0.92178802]]\n",
      "New weights at layer  1 [[ 0.68095814  0.45275487  0.31426116 -0.02557982  0.71841254]\n",
      " [ 0.05103204  0.30454256  0.47612435  0.20864017 -0.16421996]\n",
      " [ 0.6107167   0.5540436   0.10247177  0.65733097  0.46898367]]\n",
      "New weights at layer  0 [[ 0.46852003  0.57034363  0.44377687  0.43220244]\n",
      " [ 0.59179298  0.30660612  0.37823526  0.96820404]\n",
      " [ 0.74120076  0.75466956  0.98795348  0.44547369]\n",
      " [ 0.26117495  0.15363242  0.0244398   0.60147754]\n",
      " [ 0.47151915  0.84032075  0.41608876  0.21214112]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999751]\n",
      " [ 0.99999977]\n",
      " [ 1.        ]\n",
      " [ 0.99926338]\n",
      " [ 0.99999727]]\n",
      "activation at layer:  2 [[ 0.93697961]\n",
      " [ 0.6441317 ]\n",
      " [ 0.92157653]]\n",
      "New weights at layer  1 [[ 0.67542539  0.44722212  0.3087284  -0.0311085   0.7128798 ]\n",
      " [ 0.05918945  0.31269999  0.48428178  0.21679159 -0.15606255]\n",
      " [ 0.60405619  0.54738307  0.09581124  0.65067534  0.46232315]]\n",
      "New weights at layer  0 [[ 0.46851994  0.57034352  0.44377675  0.4322023 ]\n",
      " [ 0.59179297  0.30660612  0.37823525  0.96820403]\n",
      " [ 0.74120076  0.75466956  0.98795348  0.44547369]\n",
      " [ 0.26116614  0.15362185  0.02442747  0.60146345]\n",
      " [ 0.47151903  0.84032062  0.4160886   0.21214094]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99998852]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.93499007]\n",
      " [ 0.655303  ]\n",
      " [ 0.91867538]]\n",
      "New weights at layer  1 [[ 0.66974219  0.44153891  0.30304519 -0.03679164  0.70719659]\n",
      " [ 0.0443874   0.29789794  0.46947973  0.20198971 -0.1708646 ]\n",
      " [ 0.60466377  0.54799066  0.09641882  0.65128292  0.46293074]]\n",
      "New weights at layer  0 [[ 0.46851994  0.57034352  0.44377675  0.4322023 ]\n",
      " [ 0.59179297  0.30660612  0.37823525  0.96820403]\n",
      " [ 0.74120076  0.75466956  0.98795348  0.44547369]\n",
      " [ 0.26116589  0.15362158  0.02442717  0.60146312]\n",
      " [ 0.47151903  0.84032062  0.4160886   0.21214094]]\n",
      "Iteration:  20\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99475285]\n",
      " [ 0.99820396]\n",
      " [ 0.99962114]\n",
      " [ 0.95476833]\n",
      " [ 0.993641  ]]\n",
      "activation at layer:  2 [[ 0.93243072]\n",
      " [ 0.63288805]\n",
      " [ 0.91617999]]\n",
      "New weights at layer  1 [[ 0.67016566  0.44196385  0.30347074 -0.03638519  0.7076196 ]\n",
      " [ 0.02975999  0.28321978  0.45478073  0.18795026 -0.18547567]\n",
      " [ 0.59766496  0.54096756  0.08938576  0.64456543  0.45593975]]\n",
      "New weights at layer  0 [[ 0.46849719  0.57029804  0.44370851  0.43211132]\n",
      " [ 0.59177902  0.30657821  0.3781934   0.96814822]\n",
      " [ 0.74119804  0.75466411  0.98794531  0.4454628 ]\n",
      " [ 0.26085003  0.15298984  0.02347956  0.60019964]\n",
      " [ 0.4715179   0.84031835  0.4160852   0.21213641]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999751]\n",
      " [ 0.99999977]\n",
      " [ 1.        ]\n",
      " [ 0.99924645]\n",
      " [ 0.99999727]]\n",
      "activation at layer:  2 [[ 0.93304604]\n",
      " [ 0.61446534]\n",
      " [ 0.91574185]]\n",
      "New weights at layer  1 [[ 0.66433683  0.43613501  0.2976419  -0.04220964  0.70179077]\n",
      " [ 0.03889319  0.29235301  0.46391396  0.1970766  -0.17634246]\n",
      " [ 0.59059923  0.53390182  0.08232001  0.637505    0.44887402]]\n",
      "New weights at layer  0 [[ 0.4684971   0.57029792  0.44370838  0.43211117]\n",
      " [ 0.59177901  0.30657821  0.37819339  0.96814821]\n",
      " [ 0.74119804  0.75466411  0.98794531  0.4454628 ]\n",
      " [ 0.26084077  0.15297873  0.0234666   0.60018483]\n",
      " [ 0.47151778  0.84031821  0.41608503  0.21213621]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99998811]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.93082665]\n",
      " [ 0.6273939 ]\n",
      " [ 0.91245142]]\n",
      "New weights at layer  1 [[ 0.65834339  0.43014157  0.29164846 -0.04820301  0.69579733]\n",
      " [ 0.02422655  0.27768637  0.44924732  0.18241014 -0.1910091 ]\n",
      " [ 0.5912986   0.53460119  0.08301938  0.63820437  0.44957339]]\n",
      "New weights at layer  0 [[ 0.4684971   0.57029792  0.44370838  0.43211117]\n",
      " [ 0.59177901  0.30657821  0.37819339  0.96814821]\n",
      " [ 0.74119804  0.75466411  0.98794531  0.4454628 ]\n",
      " [ 0.26084056  0.1529785   0.02346635  0.60018456]\n",
      " [ 0.47151778  0.84031821  0.41608503  0.21213621]]\n",
      "Iteration:  21\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99474916]\n",
      " [ 0.99820319]\n",
      " [ 0.9996211 ]\n",
      " [ 0.95433773]\n",
      " [ 0.99364076]]\n",
      "activation at layer:  2 [[ 0.92803786]\n",
      " [ 0.60471362]\n",
      " [ 0.90986988]]\n",
      "New weights at layer  1 [[ 0.65882146  0.43062129  0.29212886 -0.04774437  0.69627486]\n",
      " [ 0.00984768  0.26325757  0.43479802  0.1686154  -0.20537195]\n",
      " [ 0.58387624  0.52715306  0.07556067  0.63108354  0.4421593 ]]\n",
      "New weights at layer  0 [[ 0.46847525  0.57025423  0.44364284  0.43202379]\n",
      " [ 0.59176551  0.30655119  0.37815286  0.96809418]\n",
      " [ 0.7411955   0.75465903  0.98793769  0.44545264]\n",
      " [ 0.26052815  0.15235368  0.02252912  0.59893492]\n",
      " [ 0.47151781  0.84031826  0.41608511  0.21213632]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.9999975 ]\n",
      " [ 0.99999977]\n",
      " [ 1.        ]\n",
      " [ 0.9992293 ]\n",
      " [ 0.99999727]]\n",
      "activation at layer:  2 [[ 0.92866655]\n",
      " [ 0.58590364]\n",
      " [ 0.90915246]]\n",
      "New weights at layer  1 [[ 0.65266952  0.42446934  0.28597691 -0.05389158  0.69012293]\n",
      " [ 0.01989448  0.27330439  0.44484485  0.17865449 -0.19532515]\n",
      " [ 0.57636718  0.51964398  0.06805159  0.62358025  0.43465024]]\n",
      "New weights at layer  0 [[ 0.46847515  0.57025411  0.4436427   0.43202362]\n",
      " [ 0.5917655   0.30655118  0.37815286  0.96809417]\n",
      " [ 0.7411955   0.75465903  0.98793769  0.44545264]\n",
      " [ 0.26051831  0.15234187  0.02251534  0.59891917]\n",
      " [ 0.47151768  0.8403181   0.41608493  0.21213611]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99998768]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.92618042]\n",
      " [ 0.60047798]\n",
      " [ 0.90540373]]\n",
      "New weights at layer  1 [[ 0.6463372   0.41813703  0.27964459 -0.06022382  0.68379061]\n",
      " [ 0.00548876  0.25889868  0.43043913  0.16424895 -0.20973087]\n",
      " [ 0.57717737  0.52045418  0.06886179  0.62439043  0.43546044]]\n",
      "New weights at layer  0 [[ 0.46847515  0.57025411  0.4436427   0.43202362]\n",
      " [ 0.5917655   0.30655118  0.37815286  0.96809417]\n",
      " [ 0.7411955   0.75465903  0.98793769  0.44545264]\n",
      " [ 0.26051815  0.15234169  0.02251514  0.59891895]\n",
      " [ 0.47151768  0.8403181   0.41608493  0.21213611]]\n",
      "Iteration:  22\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99474561]\n",
      " [ 0.99820244]\n",
      " [ 0.99962107]\n",
      " [ 0.95390761]\n",
      " [ 0.99364076]]\n",
      "activation at layer:  2 [[ 0.92312787]\n",
      " [ 0.57789325]\n",
      " [ 0.90274366]]\n",
      "New weights at layer  1 [[ 0.64687984  0.41868155  0.28018989 -0.05970345  0.68433264]\n",
      " [-0.00853387  0.24482731  0.41634777  0.150802   -0.22373793]\n",
      " [ 0.56929315  0.51254256  0.06093892  0.61682989  0.42758497]]\n",
      "New weights at layer  0 [[ 0.46845404  0.57021189  0.44357937  0.43193918]\n",
      " [ 0.59175243  0.30652504  0.37811364  0.96804188]\n",
      " [ 0.74119315  0.75465433  0.98793065  0.44544324]\n",
      " [ 0.26020829  0.15172198  0.02158558  0.59767953]\n",
      " [ 0.47151855  0.84031985  0.41608755  0.2121396 ]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.9999975 ]\n",
      " [ 0.99999977]\n",
      " [ 1.        ]\n",
      " [ 0.9992119 ]\n",
      " [ 0.99999727]]\n",
      "activation at layer:  2 [[ 0.92377185]\n",
      " [ 0.55901632]\n",
      " [ 0.90168267]]\n",
      "New weights at layer  1 [[ 0.6403749   0.41217659  0.27368493 -0.06620329  0.6778277 ]\n",
      " [ 0.0023371   0.25569831  0.42721877  0.16166443 -0.21286696]\n",
      " [ 0.56129966  0.50454905  0.05294541  0.60884268  0.41959149]]\n",
      "New weights at layer  0 [[ 0.46845393  0.57021176  0.44357922  0.43193901]\n",
      " [ 0.59175242  0.30652504  0.37811364  0.96804188]\n",
      " [ 0.74119315  0.75465433  0.98793065  0.44544324]\n",
      " [ 0.26019774  0.15170932  0.02157081  0.59766266]\n",
      " [ 0.47151841  0.84031968  0.41608736  0.21213938]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99998724]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.92097489]\n",
      " [ 0.57505462]\n",
      " [ 0.89739308]]\n",
      "New weights at layer  1 [[ 0.63367203  0.40547372  0.26698206 -0.07290607  0.67112483]\n",
      " [-0.01171532  0.24164589  0.41316635  0.14761218 -0.22691938]\n",
      " [ 0.56224445  0.50549384  0.0538902   0.60978746  0.42053628]]\n",
      "New weights at layer  0 [[ 0.46845393  0.57021176  0.44357922  0.43193901]\n",
      " [ 0.59175242  0.30652504  0.37811364  0.96804188]\n",
      " [ 0.74119315  0.75465433  0.98793065  0.44544324]\n",
      " [ 0.26019763  0.1517092   0.02157067  0.5976625 ]\n",
      " [ 0.47151841  0.84031968  0.41608736  0.21213938]]\n",
      "Iteration:  23\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99474218]\n",
      " [ 0.99820171]\n",
      " [ 0.99962105]\n",
      " [ 0.95347672]\n",
      " [ 0.99364092]]\n",
      "activation at layer:  2 [[ 0.91761777]\n",
      " [ 0.55286126]\n",
      " [ 0.89466805]]\n",
      "New weights at layer  1 [[ 0.63429152  0.40609537  0.2676046  -0.07231227  0.67174364]\n",
      " [-0.02531051  0.22800342  0.39950448  0.13458097 -0.24049952]\n",
      " [ 0.55385769  0.4970779   0.0454623   0.6017486   0.4121588 ]]\n",
      "New weights at layer  0 [[ 0.46843338  0.57017066  0.44351758  0.43185682]\n",
      " [ 0.59173976  0.30649971  0.37807565  0.96799123]\n",
      " [ 0.741191    0.75465003  0.9879242   0.44543464]\n",
      " [ 0.25988899  0.15109192  0.02064476  0.59642795]\n",
      " [ 0.47151987  0.84032259  0.41609172  0.21214521]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.9999975 ]\n",
      " [ 0.99999977]\n",
      " [ 1.        ]\n",
      " [ 0.99919415]\n",
      " [ 0.99999727]]\n",
      "activation at layer:  2 [[ 0.91827996]\n",
      " [ 0.53417324]\n",
      " [ 0.8931854 ]]\n",
      "New weights at layer  1 [[ 0.6274006   0.39920443  0.26071365 -0.07919767  0.66485272]\n",
      " [-0.01371927  0.23959469  0.41109575  0.1461629  -0.22890828]\n",
      " [ 0.54533625  0.48855645  0.03694085  0.59323401  0.40363736]]\n",
      "New weights at layer  0 [[ 0.46843327  0.57017053  0.44351742  0.43185663]\n",
      " [ 0.59173976  0.3064997   0.37807564  0.96799122]\n",
      " [ 0.741191    0.75465003  0.9879242   0.44543464]\n",
      " [ 0.25987766  0.15107832  0.02062889  0.59640982]\n",
      " [ 0.47151972  0.84032242  0.41609152  0.21214497]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99998679]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.91511942]\n",
      " [ 0.55145481]\n",
      " [ 0.8882568 ]]\n",
      "New weights at layer  1 [[ 0.62029233  0.39209616  0.25360538 -0.08630584  0.65774445]\n",
      " [-0.02735963  0.22595432  0.39745538  0.13252271 -0.24254865]\n",
      " [ 0.54644538  0.48966558  0.03804997  0.59434312  0.40474649]]\n",
      "New weights at layer  0 [[ 0.46843327  0.57017053  0.44351742  0.43185663]\n",
      " [ 0.59173976  0.3064997   0.37807564  0.96799122]\n",
      " [ 0.741191    0.75465003  0.9879242   0.44543464]\n",
      " [ 0.25987759  0.15107825  0.02062881  0.59640973]\n",
      " [ 0.47151972  0.84032242  0.41609152  0.21214497]]\n",
      "Iteration:  24\n",
      "Input:  [[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "activation at layer:  1 [[ 0.99473884]\n",
      " [ 0.998201  ]\n",
      " [ 0.99962102]\n",
      " [ 0.95304322]\n",
      " [ 0.99364119]]\n",
      "activation at layer:  2 [[ 0.91140919]\n",
      " [ 0.52986856]\n",
      " [ 0.88548885]]\n",
      "New weights at layer  1 [[ 0.62100387  0.39281018  0.25432042 -0.08562413  0.6584552 ]\n",
      " [-0.04048963  0.21277862  0.38426094  0.11994307 -0.25566416]\n",
      " [ 0.53751391  0.48070302  0.02907466  0.58578603  0.39582487]]\n",
      "New weights at layer  0 [[ 0.46841313  0.57013025  0.44345701  0.43177609]\n",
      " [ 0.59172747  0.30647513  0.37803877  0.96794206]\n",
      " [ 0.74118905  0.75464613  0.98791834  0.44542684]\n",
      " [ 0.25956862  0.15046031  0.01970191  0.59517386]\n",
      " [ 0.47152157  0.84032611  0.41609705  0.21215234]]\n",
      "Input:  [[5]\n",
      " [6]\n",
      " [7]\n",
      " [8]]\n",
      "activation at layer:  1 [[ 0.99999749]\n",
      " [ 0.99999977]\n",
      " [ 1.        ]\n",
      " [ 0.99917597]\n",
      " [ 0.99999727]]\n",
      "activation at layer:  2 [[ 0.91209393]\n",
      " [ 0.51155233]\n",
      " [ 0.88349053]]\n",
      "New weights at layer  1 [[ 0.61369084  0.38549714  0.24700738 -0.09293114  0.65114218]\n",
      " [-0.02828499  0.22498329  0.39646562  0.13213769 -0.24345952]\n",
      " [ 0.52841972  0.47160881  0.01998045  0.57669931  0.38673069]]\n",
      "New weights at layer  0 [[ 0.46841301  0.57013011  0.44345684  0.4317759 ]\n",
      " [ 0.59172746  0.30647512  0.37803877  0.96794205]\n",
      " [ 0.74118905  0.75464613  0.98791834  0.44542684]\n",
      " [ 0.25955647  0.15044573  0.01968489  0.59515441]\n",
      " [ 0.47152141  0.84032592  0.41609683  0.2121521 ]]\n",
      "Input:  [[ 9]\n",
      " [10]\n",
      " [11]\n",
      " [12]]\n",
      "activation at layer:  1 [[ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 1.        ]\n",
      " [ 0.99998632]\n",
      " [ 1.        ]]\n",
      "activation at layer:  2 [[ 0.90850666]\n",
      " [ 0.52984741]\n",
      " [ 0.87780729]]\n",
      "New weights at layer  1 [[ 0.60613913  0.37794542  0.23945566 -0.10048275  0.64359046]\n",
      " [-0.04148397  0.21178431  0.38326663  0.11893889 -0.2566585 ]\n",
      " [ 0.52973038  0.47291947  0.02129111  0.57800995  0.38804134]]\n",
      "New weights at layer  0 [[ 0.46841301  0.57013011  0.44345684  0.4317759 ]\n",
      " [ 0.59172746  0.30647512  0.37803877  0.96794205]\n",
      " [ 0.74118905  0.75464613  0.98791834  0.44542684]\n",
      " [ 0.25955646  0.15044572  0.01968488  0.5951544 ]\n",
      " [ 0.47152141  0.84032592  0.41609683  0.2121521 ]]\n",
      "[array([[ 0.46841301,  0.57013011,  0.44345684,  0.4317759 ],\n",
      "       [ 0.59172746,  0.30647512,  0.37803877,  0.96794205],\n",
      "       [ 0.74118905,  0.75464613,  0.98791834,  0.44542684],\n",
      "       [ 0.25955646,  0.15044572,  0.01968488,  0.5951544 ],\n",
      "       [ 0.47152141,  0.84032592,  0.41609683,  0.2121521 ]]), array([[ 0.60613913,  0.37794542,  0.23945566, -0.10048275,  0.64359046],\n",
      "       [-0.04148397,  0.21178431,  0.38326663,  0.11893889, -0.2566585 ],\n",
      "       [ 0.52973038,  0.47291947,  0.02129111,  0.57800995,  0.38804134]])]\n"
     ]
    }
   ],
   "source": [
    "(weights, bias) = nn.train_network(input.T, output.T, nn.update_weights, 25, 0.1)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation at layer:  1 [[ 0.99473557]\n",
      " [ 0.99820032]\n",
      " [ 0.999621  ]\n",
      " [ 0.95260493]\n",
      " [ 0.99364154]]\n",
      "activation at layer:  2 [[ 0.90438564]\n",
      " [ 0.50900569]\n",
      " [ 0.87503054]]\n",
      "[[ 0.90438564]\n",
      " [ 0.50900569]\n",
      " [ 0.87503054]]\n"
     ]
    }
   ],
   "source": [
    "#Testing predict network function\n",
    "inp = np.array([[1,2,3, 4]])\n",
    "print(nn.predict_network(inp.T, nn.logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.15419501e-02   2.72108844e-02   5.07936508e-03   2.11337868e-02\n",
      "    2.10430839e-02   5.49659864e-02   1.86848073e-02   1.17913832e-02\n",
      "    2.79365079e-02   2.43990930e-02   2.39455782e-02   1.32426304e-02\n",
      "    1.24263039e-02   6.14058957e-02   2.14058957e-02   2.64852608e-02\n",
      "    1.61451247e-02   2.53968254e-03   2.37641723e-02   2.29478458e-02\n",
      "    3.83673469e-02   4.11791383e-02   1.41496599e-02   2.32199546e-02\n",
      "    1.83219955e-02   5.99546485e-02   2.08616780e-02   6.98412698e-03\n",
      "    2.44897959e-02   1.90476190e-02   2.57596372e-02   2.32199546e-02\n",
      "    9.88662132e-03   2.88435374e-02   2.14965986e-02   2.43990930e-02\n",
      "    1.80498866e-02   4.89795918e-03   6.65759637e-02   2.59410431e-02\n",
      "    3.93650794e-02   1.50566893e-02   2.04081633e-02   2.76643991e-02\n",
      "    2.19501134e-02   4.25396825e-02   3.16553288e-02   6.80272109e-03\n",
      "    2.73922902e-02   1.92290249e-02   2.17687075e-02   2.28571429e-02\n",
      "    1.98639456e-02   5.50566893e-02   2.49433107e-02   3.02947846e-02\n",
      "    2.35827664e-02   1.17006803e-02   2.09523810e-02   2.86621315e-02\n",
      "    3.74603175e-02   1.58730159e-02   1.06122449e-02   2.16780045e-02\n",
      "    2.22222222e-02   4.01814059e-02   1.42403628e-02   1.72335601e-03\n",
      "    6.54875283e-02   3.49206349e-02   2.33106576e-02   4.38095238e-02\n",
      "    1.04308390e-02   1.99546485e-02   2.48526077e-02   2.50340136e-02\n",
      "    3.23809524e-02   1.17913832e-03   2.42176871e-02   2.13151927e-02\n",
      "    8.04535147e-02   1.98639456e-02   3.43764172e-02   3.29251701e-02\n",
      "    2.52154195e-02   4.35374150e-02   1.73242630e-02   1.78684807e-02\n",
      "    4.06349206e-02   2.42176871e-02   3.50113379e-02   1.35147392e-02\n",
      "    1.11564626e-02   4.35374150e-02   2.93877551e-02   4.93424036e-02\n",
      "    5.17006803e-03   1.11564626e-02   3.28344671e-02   2.90249433e-02\n",
      "    3.65532880e-02   1.45124717e-02   1.08843537e-02   7.39229025e-02\n",
      "    3.41950113e-02   3.97278912e-02   3.31972789e-02   1.08843537e-02\n",
      "    8.00000000e-02   2.39455782e-02   5.06122449e-02   2.72108844e-03\n",
      "    2.47619048e-02   2.51247166e-02   3.26530612e-02   1.98639456e-02\n",
      "    4.03628118e-02   2.63945578e-02   2.30385488e-02   3.88208617e-02\n",
      "    2.12244898e-02   2.48526077e-02   2.81179138e-03   4.28117914e-02\n",
      "    2.77551020e-02   3.32879819e-02   2.06802721e-02   2.22222222e-02\n",
      "    6.35827664e-02   2.48526077e-02   6.06802721e-02   2.84807256e-02\n",
      "    2.81179138e-03   2.00453515e-02   3.48299320e-02   2.02267574e-02\n",
      "    5.16099773e-02   1.44217687e-02   4.25396825e-02   2.42176871e-02\n",
      "    2.07709751e-02   2.78458050e-02   1.08843537e-03   7.96371882e-02\n",
      "    2.13151927e-02   3.76417234e-02   4.20861678e-02   1.28798186e-02\n",
      "    2.35827664e-02   2.21315193e-02   5.77777778e-02   3.71882086e-02\n",
      "    2.11337868e-02   3.35600907e-02   1.85941043e-02   2.00453515e-02\n",
      "    3.31972789e-02   7.80045351e-03   2.31292517e-02   3.48299320e-02\n",
      "    2.05895692e-02   4.84353741e-02   1.09750567e-02   3.31972789e-02\n",
      "    4.29024943e-02   2.66666667e-02   2.97505669e-02   4.73469388e-02\n",
      "    4.63492063e-02   2.78458050e-02   5.65986395e-02   3.50113379e-02\n",
      "    2.35827664e-03   3.93650794e-02   2.38548753e-02   2.05895692e-02\n",
      "    3.83673469e-02   1.25170068e-02   2.60317460e-02   2.59410431e-02\n",
      "    2.01360544e-02   5.37868481e-02   1.17913832e-03   8.61678005e-02\n",
      "    2.68480726e-02   7.30158730e-02   4.62585034e-02   2.81179138e-02\n",
      "    8.98866213e-02   2.84807256e-02   2.58503401e-02   4.52607710e-02\n",
      "    1.27891156e-02   3.67346939e-02   2.62131519e-02   3.96371882e-02\n",
      "    3.83673469e-02   1.34240363e-02   9.98639456e-02   3.27437642e-02\n",
      "    3.82766440e-02   3.65532880e-02   3.17460317e-03   1.11746032e-01\n",
      "    3.02040816e-02   2.96598639e-02   4.38095238e-02   9.79591837e-03\n",
      "    4.09070295e-02   3.47392290e-02   6.51247166e-02   2.44897959e-03\n",
      "    1.28798186e-02   3.37414966e-02   2.54875283e-02   6.86621315e-02\n",
      "    4.89795918e-03   2.30385488e-02   6.02267574e-02   4.31746032e-02\n",
      "    4.06349206e-02   5.35147392e-03   1.90476190e-03   1.13832200e-01\n",
      "    3.91836735e-02   7.07482993e-02   2.58503401e-02   9.70521542e-03\n",
      "    3.72789116e-02   2.80272109e-02   7.94557823e-02   4.18140590e-02\n",
      "    2.60317460e-02   3.60997732e-02   3.48299320e-02   5.63265306e-02\n",
      "    4.69841270e-02   3.17460317e-03   1.00770975e-01   2.30385488e-02\n",
      "    3.99092971e-02   2.81179138e-02   1.26984127e-02   1.10929705e-01\n",
      "    3.50113379e-02   7.60090703e-02   2.15873016e-02   1.34240363e-02\n",
      "    3.80952381e-02   4.02721088e-02   6.05895692e-02   2.52154195e-02\n",
      "    1.09750567e-02   3.80952381e-02   1.99546485e-02   7.50113379e-02\n",
      "    3.13832200e-02   1.40589569e-02   4.08163265e-02   4.01814059e-02\n",
      "    4.02721088e-02   3.71882086e-03   2.77551020e-02   3.42857143e-02\n",
      "    5.44217687e-03   4.67120181e-02   1.85034014e-02   3.01133787e-02\n",
      "    6.26757370e-02   1.04217687e-01   4.56235828e-02   4.48072562e-02\n",
      "    1.25170068e-02   2.74829932e-02   6.65759637e-02   7.63718821e-02\n",
      "    4.33560091e-02   1.40589569e-02   2.28571429e-02   6.09523810e-02\n",
      "    4.02721088e-02   6.32199546e-02   6.34920635e-03   3.90022676e-02\n",
      "    4.29931973e-02   5.46938776e-02   8.42630385e-02   1.63265306e-02\n",
      "    6.07709751e-02   6.38548753e-02   1.41133787e-01   5.27891156e-02\n",
      "    4.80725624e-03   2.21315193e-02   7.55555556e-02   1.30975057e-01\n",
      "    4.82539683e-02   1.07029478e-02   3.47392290e-02   8.98866213e-02\n",
      "    8.03628118e-02   7.02947846e-02   1.54195011e-03   6.27664399e-02\n",
      "    9.56916100e-02   7.85487528e-02   9.36054422e-02   1.99546485e-02\n",
      "    2.35827664e-02   7.31065760e-02   1.45668934e-01   6.67573696e-02\n",
      "    5.53287982e-03   2.36734694e-02   6.80272109e-02   1.40136054e-01\n",
      "    5.76870748e-02   1.46031746e-02   5.01587302e-02   9.41496599e-02\n",
      "    7.94557823e-02   7.26530612e-02   1.81405896e-03   2.16780045e-02\n",
      "    8.35374150e-02   8.01814059e-02   7.51020408e-02   4.75283447e-02\n",
      "    2.62131519e-02   8.68027211e-02   1.06122449e-01   5.68707483e-02\n",
      "    1.90476190e-03   3.66439909e-02   9.07936508e-02   1.42947846e-01\n",
      "    6.44897959e-02   4.41723356e-02   4.23582766e-02   7.47392290e-02\n",
      "    8.10884354e-02   4.70748299e-02   4.29931973e-02   5.65986395e-02\n",
      "    9.08843537e-02   7.80045351e-02   6.81179138e-02   5.98639456e-03\n",
      "    4.58956916e-02   8.78004535e-02   1.08662132e-01   7.31972789e-02\n",
      "    2.24943311e-02   8.25396825e-02   3.67346939e-02   1.38775510e-01\n",
      "    8.25396825e-02   8.61678005e-03   6.46712018e-02   5.95011338e-02\n",
      "    8.01814059e-02   3.58276644e-02   1.28798186e-02   2.29478458e-02\n",
      "    1.14195011e-01   7.84580499e-02   8.93424036e-02   1.90476190e-03\n",
      "    3.70068027e-02   7.91836735e-02   4.60770975e-02   3.08390023e-03\n",
      "    1.28798186e-02   4.69841270e-02   4.99773243e-02   5.16099773e-02\n",
      "    2.78458050e-02   1.50566893e-02   8.19954649e-02   6.28571429e-02\n",
      "    1.43310658e-02   1.60544218e-02   1.64172336e-02   2.91156463e-02\n",
      "    4.31746032e-02   1.71428571e-02   1.75056689e-02   1.57823129e-02\n",
      "    5.73242630e-02   8.12698413e-02   5.72335601e-02   1.18820862e-02\n",
      "    1.43310658e-02   6.96598639e-02   7.61904762e-02   4.75283447e-02\n",
      "    1.63265306e-02   7.16553288e-03   2.29478458e-02   5.65079365e-02\n",
      "    1.47845805e-02   4.25396825e-02   1.82312925e-02   3.41043084e-02\n",
      "    8.20861678e-02   1.68707483e-02   1.35147392e-02   1.44217687e-02\n",
      "    7.01133787e-02   4.97959184e-02   3.61904762e-02   1.96825397e-02\n",
      "    1.58730159e-02   3.37414966e-02   1.07664399e-01   4.53514739e-02\n",
      "    1.28798186e-02   1.19727891e-02   2.24036281e-02   1.43945578e-01\n",
      "    1.15192744e-02   1.68707483e-02   1.65986395e-02   4.00907029e-02\n",
      "    1.00498866e-01   1.57823129e-02   4.02721088e-02   6.80272109e-03\n",
      "    3.92743764e-02   3.75510204e-02   4.61678005e-02   1.47845805e-02\n",
      "    7.98185941e-03   7.52834467e-02   1.40589569e-01   4.64399093e-02\n",
      "    1.26984127e-02   5.35147392e-03   4.74376417e-02   1.05034014e-01\n",
      "    1.08843537e-02   1.67800454e-02   1.74149660e-02   6.82086168e-02\n",
      "    8.22675737e-02   1.67800454e-02   2.08616780e-02   6.80272109e-03\n",
      "    1.11473923e-01   1.47029478e-01   5.10657596e-02   1.38775510e-02\n",
      "    1.12471655e-02   8.36281179e-02   1.23809524e-01   5.82312925e-02\n",
      "    4.33560091e-02   1.25170068e-02   1.13741497e-01   1.38866213e-01\n",
      "    2.20408163e-02   1.52380952e-02   2.54875283e-02   4.16326531e-02\n",
      "    1.68798186e-01   2.57596372e-02   1.98639456e-02   4.62585034e-03\n",
      "    1.22358277e-01   1.08934240e-01   5.01587302e-02   1.82312925e-02\n",
      "    1.54195011e-03   1.20362812e-01   1.08934240e-01   6.34920635e-02\n",
      "    1.77777778e-02   1.82312925e-02   4.14512472e-02   1.29342404e-01\n",
      "    2.12244898e-02   5.44217687e-03   1.39682540e-02   7.51020408e-02\n",
      "    8.64399093e-02   2.21315193e-02   2.95691610e-02   1.54195011e-03\n",
      "    1.17732426e-01   2.00544218e-01   3.90929705e-02   2.88435374e-02\n",
      "    5.26077098e-03   1.03945578e-01   1.30249433e-01   6.03174603e-02\n",
      "    3.12018141e-02   3.35600907e-03   7.36507937e-02   1.51473923e-01]\n",
      " [  2.16159063e+03   1.99864258e+03   1.39296663e+03   1.36467265e+03\n",
      "    1.54176813e+03   2.07207058e+03   1.65788278e+03   1.06003554e+03\n",
      "    1.74711298e+03   2.09564284e+03   1.53437355e+03   1.86355988e+03\n",
      "    7.17722798e+02   2.63633438e+03   1.77236161e+03   1.45982307e+03\n",
      "    1.63752842e+03   1.90498822e+03   1.47264980e+03   1.76576693e+03\n",
      "    2.19141242e+03   2.26395600e+03   1.04275649e+03   1.40186706e+03\n",
      "    2.20677769e+03   1.94017113e+03   2.28866415e+03   1.34149568e+03\n",
      "    1.40845045e+03   1.38252886e+03   1.67655105e+03   1.85883005e+03\n",
      "    7.80202327e+02   1.58139931e+03   2.32053456e+03   1.70901206e+03\n",
      "    1.80872559e+03   1.43262015e+03   2.55256448e+03   2.03625121e+03\n",
      "    2.02133526e+03   2.32910383e+03   2.14464868e+03   1.38391746e+03\n",
      "    2.27689426e+03   2.02667265e+03   2.47252212e+03   1.47545631e+03\n",
      "    1.64150503e+03   1.99084851e+03   1.35493140e+03   2.01619493e+03\n",
      "    1.09768154e+03   2.63022151e+03   2.65258129e+03   1.86160606e+03\n",
      "    1.86906039e+03   1.05523957e+03   1.92489355e+03   2.24366274e+03\n",
      "    2.04252865e+03   1.98244266e+03   9.51847126e+02   1.41808011e+03\n",
      "    2.45282960e+03   2.16922621e+03   1.29712400e+03   2.25775003e+03\n",
      "    2.69825042e+03   2.48114828e+03   1.51344762e+03   2.46178479e+03\n",
      "    7.53172413e+02   1.58505744e+03   1.56763011e+03   1.56099642e+03\n",
      "    1.79987985e+03   1.60023376e+03   1.56229463e+03   2.01136982e+03\n",
      "    2.43140094e+03   1.80364985e+03   1.49792584e+03   2.08036623e+03\n",
      "    1.99315584e+03   1.66402430e+03   1.85180931e+03   1.43548269e+03\n",
      "    1.88894395e+03   1.85725391e+03   1.69822314e+03   1.45915793e+03\n",
      "    9.40841552e+02   2.48542624e+03   2.48014271e+03   3.15438667e+03\n",
      "    3.26874903e+03   9.90116908e+02   1.12357954e+03   2.34745535e+03\n",
      "    1.48090091e+03   1.53697471e+03   1.01841911e+03   2.23872284e+03\n",
      "    2.01399138e+03   2.14462410e+03   2.34112773e+03   1.60647874e+03\n",
      "    2.68586456e+03   1.76422177e+03   1.86278816e+03   3.01931567e+03\n",
      "    1.81256807e+03   1.52756455e+03   2.14404361e+03   1.85449592e+03\n",
      "    2.28423779e+03   2.67966190e+03   1.86307258e+03   2.25569094e+03\n",
      "    2.15078943e+03   2.27216090e+03   1.31790772e+03   1.72188165e+03\n",
      "    1.62636483e+03   1.57477987e+03   2.19165797e+03   1.37474060e+03\n",
      "    2.46808143e+03   2.10914643e+03   1.91717273e+03   2.66687452e+03\n",
      "    2.16536135e+03   1.20610817e+03   2.27412635e+03   2.37123670e+03\n",
      "    2.47183072e+03   1.71847060e+03   1.94601926e+03   1.94637521e+03\n",
      "    1.75826742e+03   2.07368467e+03   1.79203222e+03   2.72243094e+03\n",
      "    2.47442970e+03   2.27261697e+03   2.50900427e+03   1.26741952e+03\n",
      "    1.48859719e+03   1.94495934e+03   2.22428134e+03   2.90261062e+03\n",
      "    1.18113818e+03   1.76325470e+03   1.71040582e+03   2.41407536e+03\n",
      "    2.17735255e+03   1.57987787e+03   1.77123004e+03   2.35129408e+03\n",
      "    2.30672091e+03   2.86405710e+03   1.87010174e+03   1.47775829e+03\n",
      "    2.75649806e+03   1.57975217e+03   2.64271095e+03   1.84084199e+03\n",
      "    1.89721378e+03   2.51017651e+03   1.80956456e+03   2.50264011e+03\n",
      "    2.48688594e+03   3.47424654e+03   2.01339013e+03   2.04770954e+03\n",
      "    2.35979591e+03   9.79995006e+02   2.64252848e+03   1.80514093e+03\n",
      "    1.90352371e+03   2.10911256e+03   2.04107301e+03   2.82284656e+03\n",
      "    1.97388856e+03   2.04383666e+03   2.19121255e+03   1.17714228e+03\n",
      "    3.24870835e+03   1.90513264e+03   1.85147436e+03   2.24199282e+03\n",
      "    8.30400721e+02   1.61646995e+03   2.59611667e+03   1.85140443e+03\n",
      "    2.42803603e+03   7.21103488e+02   3.01137892e+03   2.33530900e+03\n",
      "    1.36057722e+03   2.39466555e+03   2.01836746e+03   3.44322851e+03\n",
      "    2.11240234e+03   2.42311939e+03   2.56062575e+03   1.55350059e+03\n",
      "    2.62927683e+03   2.42595871e+03   2.29650025e+03   3.02621470e+03\n",
      "    1.11218868e+03   2.35455064e+03   2.43241952e+03   2.31784236e+03\n",
      "    2.95468891e+03   1.58169610e+03   3.27524610e+03   2.95795520e+03\n",
      "    2.46286897e+03   3.65134026e+03   1.90000196e+03   3.57050464e+03\n",
      "    2.58234930e+03   2.18197382e+03   2.02863805e+03   1.25450060e+03\n",
      "    2.16427476e+03   1.91087573e+03   2.93603682e+03   2.64729420e+03\n",
      "    1.22893319e+03   2.50950313e+03   2.15482493e+03   2.44416497e+03\n",
      "    3.13639577e+03   2.09935273e+03   3.11172005e+03   2.16966084e+03\n",
      "    2.68313415e+03   1.86872652e+03   8.18962402e+02   3.56502233e+03\n",
      "    2.39693789e+03   2.92780692e+03   2.27320401e+03   1.35846116e+03\n",
      "    2.13470386e+03   1.94050296e+03   2.13221258e+03   2.23144768e+03\n",
      "    2.15813140e+03   1.63429098e+03   1.68399450e+03   2.94865138e+03\n",
      "    1.82389753e+03   1.74607451e+03   2.12654039e+03   2.14927342e+03\n",
      "    1.58803166e+03   3.16198519e+03   1.47464214e+03   1.97257411e+03\n",
      "    2.60192498e+03   2.20376385e+03   3.50800513e+03   1.70837397e+03\n",
      "    2.69667186e+03   4.54613887e+03   2.38590206e+03   4.01122047e+03\n",
      "    1.26993146e+03   1.67579389e+03   3.81405694e+03   2.48339544e+03\n",
      "    3.23588559e+03   8.14350182e+02   1.21579247e+03   3.55775685e+03\n",
      "    2.24610035e+03   3.59908658e+03   8.27655767e+02   2.06287677e+03\n",
      "    3.75025104e+03   2.41348461e+03   4.18189470e+03   1.54121743e+03\n",
      "    2.30066149e+03   2.90606952e+03   3.59223254e+03   4.38645694e+03\n",
      "    2.08002979e+03   1.72365140e+03   4.57516772e+03   3.44969586e+03\n",
      "    3.17595017e+03   9.36700488e+02   2.05018062e+03   5.00703664e+03\n",
      "    2.95102898e+03   2.95346277e+03   2.05091469e+03   2.33359197e+03\n",
      "    4.51481399e+03   3.22403402e+03   3.71139118e+03   1.39858324e+03\n",
      "    1.14238423e+03   4.17499318e+03   3.93968308e+03   3.42845192e+03\n",
      "    1.03664332e+03   1.89444318e+03   4.13773007e+03   3.60617990e+03\n",
      "    2.81097831e+03   1.18739112e+03   2.96659358e+03   4.34225002e+03\n",
      "    2.64350650e+03   2.54769620e+03   2.40274903e+03   1.53537417e+03\n",
      "    4.74044086e+03   3.33699293e+03   3.89640289e+03   2.34664268e+03\n",
      "    1.66969727e+03   4.69927735e+03   3.43531128e+03   3.91056489e+03\n",
      "    2.31965062e+03   1.86546513e+03   3.69386854e+03   3.96811634e+03\n",
      "    4.06910288e+03   1.63995915e+03   1.83763700e+03   4.78666501e+03\n",
      "    3.21424154e+03   3.58475440e+03   1.63880453e+03   1.79066674e+03\n",
      "    4.52878639e+03   3.06907865e+03   3.20715466e+03   2.65900072e+03\n",
      "    2.33026780e+03   4.72029435e+03   3.55680930e+03   3.65640834e+03\n",
      "    3.18911218e+03   1.84928105e+03   3.36529658e+03   4.07515807e+03\n",
      "    3.57606205e+03   1.51483563e+03   2.54487024e+03   3.46847221e+03\n",
      "    3.63450254e+03   3.70811247e+03   8.59727393e+02   2.01935504e+03\n",
      "    4.06418364e+03   3.37372765e+03   3.98741000e+03   2.08155000e+03\n",
      "    1.99814258e+03   4.24239210e+03   2.48702626e+03   3.76083496e+03\n",
      "    9.77070084e+02   1.96522905e+03   4.11512306e+03   2.21535551e+03\n",
      "    1.94232417e+03   1.70465874e+03   2.18312328e+03   4.57311006e+03\n",
      "    1.30786135e+03   1.67767331e+03   1.70076305e+03   1.13497062e+03\n",
      "    2.95111854e+03   1.30549995e+03   1.63625245e+03   1.34387643e+03\n",
      "    2.25598670e+03   4.74606319e+03   1.61023013e+03   2.39270655e+03\n",
      "    1.67797311e+03   2.35328692e+03   3.32254812e+03   1.96594590e+03\n",
      "    1.56377630e+03   1.57242494e+03   2.16136462e+03   3.57721180e+03\n",
      "    8.64436443e+02   2.54383898e+03   1.71313773e+03   1.98697623e+03\n",
      "    4.13416049e+03   1.00745307e+03   1.75264588e+03   1.40426475e+03\n",
      "    1.71523832e+03   3.25380600e+03   1.56380227e+03   1.63647285e+03\n",
      "    1.62282442e+03   1.34547801e+03   4.78560712e+03   2.45607603e+03\n",
      "    2.00491023e+03   1.88679682e+03   1.38228911e+03   5.56149807e+03\n",
      "    2.41807872e+03   1.17802152e+03   1.44911163e+03   1.43361583e+03\n",
      "    4.96686213e+03   1.22864769e+03   1.99486030e+03   2.42222760e+03\n",
      "    1.29819243e+03   2.43066387e+03   1.57204356e+03   1.56340220e+03\n",
      "    1.44251725e+03   2.84789120e+03   4.73414705e+03   1.97093606e+03\n",
      "    2.09143875e+03   1.13356016e+03   2.97782447e+03   4.15709064e+03\n",
      "    2.35915092e+03   1.74859723e+03   1.34307392e+03   2.17912676e+03\n",
      "    4.74271570e+03   1.15301880e+03   1.90709615e+03   1.14332573e+03\n",
      "    3.18529210e+03   5.70183568e+03   1.75868321e+03   2.15521303e+03\n",
      "    1.19876284e+03   3.29932359e+03   5.81537350e+03   1.90170625e+03\n",
      "    2.12799987e+03   1.58648809e+03   2.83034759e+03   5.26729252e+03\n",
      "    1.80204533e+03   1.99216715e+03   2.47915766e+03   1.87016192e+03\n",
      "    5.82365919e+03   1.42427745e+03   1.92594845e+03   1.51912212e+03\n",
      "    3.31814818e+03   4.35901863e+03   2.25530171e+03   1.86503526e+03\n",
      "    2.02895489e+03   3.41270030e+03   4.67959045e+03   2.45427217e+03\n",
      "    1.97586886e+03   1.53530762e+03   1.98402839e+03   5.80664266e+03\n",
      "    2.23209824e+03   1.97254829e+03   1.29649949e+03   2.30443660e+03\n",
      "    3.95043108e+03   2.02268897e+03   2.66015677e+03   1.92247990e+03\n",
      "    3.13577475e+03   6.43761548e+03   2.15019767e+03   2.69824374e+03\n",
      "    1.88778015e+03   3.50176931e+03   5.62704487e+03   2.27325868e+03\n",
      "    3.05233700e+03   1.96588757e+03   2.22019541e+03   5.53611428e+03]]\n",
      "[[1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " ..., \n",
      " [0 0 1 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]]\n",
      "Iteration:  0\n",
      "Iteration:  1\n",
      "Iteration:  2\n",
      "Iteration:  3\n",
      "Iteration:  4\n",
      "Iteration:  5\n",
      "Iteration:  6\n",
      "Iteration:  7\n",
      "Iteration:  8\n",
      "Iteration:  9\n",
      "Iteration:  10\n",
      "Iteration:  11\n",
      "Iteration:  12\n",
      "Iteration:  13\n",
      "Iteration:  14\n",
      "Iteration:  15\n",
      "Iteration:  16\n",
      "Iteration:  17\n",
      "Iteration:  18\n",
      "Iteration:  19\n",
      "Iteration:  20\n",
      "Iteration:  21\n",
      "Iteration:  22\n",
      "Iteration:  23\n",
      "Iteration:  24\n",
      "Iteration:  25\n",
      "Iteration:  26\n",
      "Iteration:  27\n",
      "Iteration:  28\n",
      "Iteration:  29\n",
      "Iteration:  30\n",
      "Iteration:  31\n",
      "Iteration:  32\n",
      "Iteration:  33\n",
      "Iteration:  34\n",
      "Iteration:  35\n",
      "Iteration:  36\n",
      "Iteration:  37\n",
      "Iteration:  38\n",
      "Iteration:  39\n",
      "Iteration:  40\n",
      "Iteration:  41\n",
      "Iteration:  42\n",
      "Iteration:  43\n",
      "Iteration:  44\n",
      "Iteration:  45\n",
      "Iteration:  46\n",
      "Iteration:  47\n",
      "Iteration:  48\n",
      "Iteration:  49\n",
      "Iteration:  50\n",
      "Iteration:  51\n",
      "Iteration:  52\n",
      "Iteration:  53\n",
      "Iteration:  54\n",
      "Iteration:  55\n",
      "Iteration:  56\n",
      "Iteration:  57\n",
      "Iteration:  58\n",
      "Iteration:  59\n",
      "Iteration:  60\n",
      "Iteration:  61\n",
      "Iteration:  62\n",
      "Iteration:  63\n",
      "Iteration:  64\n",
      "Iteration:  65\n",
      "Iteration:  66\n",
      "Iteration:  67\n",
      "Iteration:  68\n",
      "Iteration:  69\n",
      "Iteration:  70\n",
      "Iteration:  71\n",
      "Iteration:  72\n",
      "Iteration:  73\n",
      "Iteration:  74\n",
      "Iteration:  75\n",
      "Iteration:  76\n",
      "Iteration:  77\n",
      "Iteration:  78\n",
      "Iteration:  79\n",
      "Iteration:  80\n",
      "Iteration:  81\n",
      "Iteration:  82\n",
      "Iteration:  83\n",
      "Iteration:  84\n",
      "Iteration:  85\n",
      "Iteration:  86\n",
      "Iteration:  87\n",
      "Iteration:  88\n",
      "Iteration:  89\n",
      "Iteration:  90\n",
      "Iteration:  91\n",
      "Iteration:  92\n",
      "Iteration:  93\n",
      "Iteration:  94\n",
      "Iteration:  95\n",
      "Iteration:  96\n",
      "Iteration:  97\n",
      "Iteration:  98\n",
      "Iteration:  99\n",
      "Iteration:  100\n",
      "Iteration:  101\n",
      "Iteration:  102\n",
      "Iteration:  103\n",
      "Iteration:  104\n",
      "Iteration:  105\n",
      "Iteration:  106\n",
      "Iteration:  107\n",
      "Iteration:  108\n",
      "Iteration:  109\n",
      "Iteration:  110\n",
      "Iteration:  111\n",
      "Iteration:  112\n",
      "Iteration:  113\n",
      "Iteration:  114\n",
      "Iteration:  115\n",
      "Iteration:  116\n",
      "Iteration:  117\n",
      "Iteration:  118\n",
      "Iteration:  119\n",
      "Iteration:  120\n",
      "Iteration:  121\n",
      "Iteration:  122\n",
      "Iteration:  123\n",
      "Iteration:  124\n",
      "Iteration:  125\n",
      "Iteration:  126\n",
      "Iteration:  127\n",
      "Iteration:  128\n",
      "Iteration:  129\n",
      "Iteration:  130\n",
      "Iteration:  131\n",
      "Iteration:  132\n",
      "Iteration:  133\n",
      "Iteration:  134\n",
      "Iteration:  135\n",
      "Iteration:  136\n",
      "Iteration:  137\n",
      "Iteration:  138\n",
      "Iteration:  139\n",
      "Iteration:  140\n",
      "Iteration:  141\n",
      "Iteration:  142\n",
      "Iteration:  143\n",
      "Iteration:  144\n",
      "Iteration:  145\n",
      "Iteration:  146\n",
      "Iteration:  147\n",
      "Iteration:  148\n",
      "Iteration:  149\n",
      "Iteration:  150\n",
      "Iteration:  151\n",
      "Iteration:  152\n",
      "Iteration:  153\n",
      "Iteration:  154\n",
      "Iteration:  155\n",
      "Iteration:  156\n",
      "Iteration:  157\n",
      "Iteration:  158\n",
      "Iteration:  159\n",
      "Iteration:  160\n",
      "Iteration:  161\n",
      "Iteration:  162\n",
      "Iteration:  163\n",
      "Iteration:  164\n",
      "Iteration:  165\n",
      "Iteration:  166\n",
      "Iteration:  167\n",
      "Iteration:  168\n",
      "Iteration:  169\n",
      "Iteration:  170\n",
      "Iteration:  171\n",
      "Iteration:  172\n",
      "Iteration:  173\n",
      "Iteration:  174\n",
      "Iteration:  175\n",
      "Iteration:  176\n",
      "Iteration:  177\n",
      "Iteration:  178\n",
      "Iteration:  179\n",
      "Iteration:  180\n",
      "Iteration:  181\n",
      "Iteration:  182\n",
      "Iteration:  183\n",
      "Iteration:  184\n",
      "Iteration:  185\n",
      "Iteration:  186\n",
      "Iteration:  187\n",
      "Iteration:  188\n",
      "Iteration:  189\n",
      "Iteration:  190\n",
      "Iteration:  191\n",
      "Iteration:  192\n",
      "Iteration:  193\n",
      "Iteration:  194\n",
      "Iteration:  195\n",
      "Iteration:  196\n",
      "Iteration:  197\n",
      "Iteration:  198\n",
      "Iteration:  199\n",
      "Iteration:  200\n",
      "Iteration:  201\n",
      "Iteration:  202\n",
      "Iteration:  203\n",
      "Iteration:  204\n",
      "Iteration:  205\n",
      "Iteration:  206\n",
      "Iteration:  207\n",
      "Iteration:  208\n",
      "Iteration:  209\n",
      "Iteration:  210\n",
      "Iteration:  211\n",
      "Iteration:  212\n",
      "Iteration:  213\n",
      "Iteration:  214\n",
      "Iteration:  215\n",
      "Iteration:  216\n",
      "Iteration:  217\n",
      "Iteration:  218\n",
      "Iteration:  219\n",
      "Iteration:  220\n",
      "Iteration:  221\n",
      "Iteration:  222\n",
      "Iteration:  223\n",
      "Iteration:  224\n",
      "Iteration:  225\n",
      "Iteration:  226\n",
      "Iteration:  227\n",
      "Iteration:  228\n",
      "Iteration:  229\n",
      "Iteration:  230\n",
      "Iteration:  231\n",
      "Iteration:  232\n",
      "Iteration:  233\n",
      "Iteration:  234\n",
      "Iteration:  235\n",
      "Iteration:  236\n",
      "Iteration:  237\n",
      "Iteration:  238\n",
      "Iteration:  239\n",
      "Iteration:  240\n",
      "Iteration:  241\n",
      "Iteration:  242\n",
      "Iteration:  243\n",
      "Iteration:  244\n",
      "Iteration:  245\n",
      "Iteration:  246\n",
      "Iteration:  247\n",
      "Iteration:  248\n",
      "Iteration:  249\n",
      "Iteration:  250\n",
      "Iteration:  251\n",
      "Iteration:  252\n",
      "Iteration:  253\n",
      "Iteration:  254\n",
      "Iteration:  255\n",
      "Iteration:  256\n",
      "Iteration:  257\n",
      "Iteration:  258\n",
      "Iteration:  259\n",
      "Iteration:  260\n",
      "Iteration:  261\n",
      "Iteration:  262\n",
      "Iteration:  263\n",
      "Iteration:  264\n",
      "Iteration:  265\n",
      "Iteration:  266\n",
      "Iteration:  267\n",
      "Iteration:  268\n",
      "Iteration:  269\n",
      "Iteration:  270\n",
      "Iteration:  271\n",
      "Iteration:  272\n",
      "Iteration:  273\n",
      "Iteration:  274\n",
      "Iteration:  275\n",
      "Iteration:  276\n",
      "Iteration:  277\n",
      "Iteration:  278\n",
      "Iteration:  279\n",
      "Iteration:  280\n",
      "Iteration:  281\n",
      "Iteration:  282\n",
      "Iteration:  283\n",
      "Iteration:  284\n",
      "Iteration:  285\n",
      "Iteration:  286\n",
      "Iteration:  287\n",
      "Iteration:  288\n",
      "Iteration:  289\n",
      "Iteration:  290\n",
      "Iteration:  291\n",
      "Iteration:  292\n",
      "Iteration:  293\n",
      "Iteration:  294\n",
      "Iteration:  295\n",
      "Iteration:  296\n",
      "Iteration:  297\n",
      "Iteration:  298\n",
      "Iteration:  299\n",
      "Iteration:  300\n",
      "Iteration:  301\n",
      "Iteration:  302\n",
      "Iteration:  303\n",
      "Iteration:  304\n",
      "Iteration:  305\n",
      "Iteration:  306\n",
      "Iteration:  307\n",
      "Iteration:  308\n",
      "Iteration:  309\n",
      "Iteration:  310\n",
      "Iteration:  311\n",
      "Iteration:  312\n",
      "Iteration:  313\n",
      "Iteration:  314\n",
      "Iteration:  315\n",
      "Iteration:  316\n",
      "Iteration:  317\n",
      "Iteration:  318\n",
      "Iteration:  319\n",
      "Iteration:  320\n",
      "Iteration:  321\n",
      "Iteration:  322\n",
      "Iteration:  323\n",
      "Iteration:  324\n",
      "Iteration:  325\n",
      "Iteration:  326\n",
      "Iteration:  327\n",
      "Iteration:  328\n",
      "Iteration:  329\n",
      "Iteration:  330\n",
      "Iteration:  331\n",
      "Iteration:  332\n",
      "Iteration:  333\n",
      "Iteration:  334\n",
      "Iteration:  335\n",
      "Iteration:  336\n",
      "Iteration:  337\n",
      "Iteration:  338\n",
      "Iteration:  339\n",
      "Iteration:  340\n",
      "Iteration:  341\n",
      "Iteration:  342\n",
      "Iteration:  343\n",
      "Iteration:  344\n",
      "Iteration:  345\n",
      "Iteration:  346\n",
      "Iteration:  347\n",
      "Iteration:  348\n",
      "Iteration:  349\n",
      "Iteration:  350\n",
      "Iteration:  351\n",
      "Iteration:  352\n",
      "Iteration:  353\n",
      "Iteration:  354\n",
      "Iteration:  355\n",
      "Iteration:  356\n",
      "Iteration:  357\n",
      "Iteration:  358\n",
      "Iteration:  359\n",
      "Iteration:  360\n",
      "Iteration:  361\n",
      "Iteration:  362\n",
      "Iteration:  363\n",
      "Iteration:  364\n",
      "Iteration:  365\n",
      "Iteration:  366\n",
      "Iteration:  367\n",
      "Iteration:  368\n",
      "Iteration:  369\n",
      "Iteration:  370\n",
      "Iteration:  371\n",
      "Iteration:  372\n",
      "Iteration:  373\n",
      "Iteration:  374\n",
      "Iteration:  375\n",
      "Iteration:  376\n",
      "Iteration:  377\n",
      "Iteration:  378\n",
      "Iteration:  379\n",
      "Iteration:  380\n",
      "Iteration:  381\n",
      "Iteration:  382\n",
      "Iteration:  383\n",
      "Iteration:  384\n",
      "Iteration:  385\n",
      "Iteration:  386\n",
      "Iteration:  387\n",
      "Iteration:  388\n",
      "Iteration:  389\n",
      "Iteration:  390\n",
      "Iteration:  391\n",
      "Iteration:  392\n",
      "Iteration:  393\n",
      "Iteration:  394\n",
      "Iteration:  395\n",
      "Iteration:  396\n",
      "Iteration:  397\n",
      "Iteration:  398\n",
      "Iteration:  399\n",
      "Iteration:  400\n",
      "Iteration:  401\n",
      "Iteration:  402\n",
      "Iteration:  403\n",
      "Iteration:  404\n",
      "Iteration:  405\n",
      "Iteration:  406\n",
      "Iteration:  407\n",
      "Iteration:  408\n",
      "Iteration:  409\n",
      "Iteration:  410\n",
      "Iteration:  411\n",
      "Iteration:  412\n",
      "Iteration:  413\n",
      "Iteration:  414\n",
      "Iteration:  415\n",
      "Iteration:  416\n",
      "Iteration:  417\n",
      "Iteration:  418\n",
      "Iteration:  419\n",
      "Iteration:  420\n",
      "Iteration:  421\n",
      "Iteration:  422\n",
      "Iteration:  423\n",
      "Iteration:  424\n",
      "Iteration:  425\n",
      "Iteration:  426\n",
      "Iteration:  427\n",
      "Iteration:  428\n",
      "Iteration:  429\n",
      "Iteration:  430\n",
      "Iteration:  431\n",
      "Iteration:  432\n",
      "Iteration:  433\n",
      "Iteration:  434\n",
      "Iteration:  435\n",
      "Iteration:  436\n",
      "Iteration:  437\n",
      "Iteration:  438\n",
      "Iteration:  439\n",
      "Iteration:  440\n",
      "Iteration:  441\n",
      "Iteration:  442\n",
      "Iteration:  443\n",
      "Iteration:  444\n",
      "Iteration:  445\n",
      "Iteration:  446\n",
      "Iteration:  447\n",
      "Iteration:  448\n",
      "Iteration:  449\n",
      "Iteration:  450\n",
      "Iteration:  451\n",
      "Iteration:  452\n",
      "Iteration:  453\n",
      "Iteration:  454\n",
      "Iteration:  455\n",
      "Iteration:  456\n",
      "Iteration:  457\n",
      "Iteration:  458\n",
      "Iteration:  459\n",
      "Iteration:  460\n",
      "Iteration:  461\n",
      "Iteration:  462\n",
      "Iteration:  463\n",
      "Iteration:  464\n",
      "Iteration:  465\n",
      "Iteration:  466\n",
      "Iteration:  467\n",
      "Iteration:  468\n",
      "Iteration:  469\n",
      "Iteration:  470\n",
      "Iteration:  471\n",
      "Iteration:  472\n",
      "Iteration:  473\n",
      "Iteration:  474\n",
      "Iteration:  475\n",
      "Iteration:  476\n",
      "Iteration:  477\n",
      "Iteration:  478\n",
      "Iteration:  479\n",
      "Iteration:  480\n",
      "Iteration:  481\n",
      "Iteration:  482\n",
      "Iteration:  483\n",
      "Iteration:  484\n",
      "Iteration:  485\n",
      "Iteration:  486\n",
      "Iteration:  487\n",
      "Iteration:  488\n",
      "Iteration:  489\n",
      "Iteration:  490\n",
      "Iteration:  491\n",
      "Iteration:  492\n",
      "Iteration:  493\n",
      "Iteration:  494\n",
      "Iteration:  495\n",
      "Iteration:  496\n",
      "Iteration:  497\n",
      "Iteration:  498\n",
      "Iteration:  499\n",
      "Iteration:  500\n",
      "Iteration:  501\n",
      "Iteration:  502\n",
      "Iteration:  503\n",
      "Iteration:  504\n",
      "Iteration:  505\n",
      "Iteration:  506\n",
      "Iteration:  507\n",
      "Iteration:  508\n",
      "Iteration:  509\n",
      "Iteration:  510\n",
      "Iteration:  511\n",
      "Iteration:  512\n",
      "Iteration:  513\n",
      "Iteration:  514\n",
      "Iteration:  515\n",
      "Iteration:  516\n",
      "Iteration:  517\n",
      "Iteration:  518\n",
      "Iteration:  519\n",
      "Iteration:  520\n",
      "Iteration:  521\n",
      "Iteration:  522\n",
      "Iteration:  523\n",
      "Iteration:  524\n",
      "Iteration:  525\n",
      "Iteration:  526\n",
      "Iteration:  527\n",
      "Iteration:  528\n",
      "Iteration:  529\n",
      "Iteration:  530\n",
      "Iteration:  531\n",
      "Iteration:  532\n",
      "Iteration:  533\n",
      "Iteration:  534\n",
      "Iteration:  535\n",
      "Iteration:  536\n",
      "Iteration:  537\n",
      "Iteration:  538\n",
      "Iteration:  539\n",
      "Iteration:  540\n",
      "Iteration:  541\n",
      "Iteration:  542\n",
      "Iteration:  543\n",
      "Iteration:  544\n",
      "Iteration:  545\n",
      "Iteration:  546\n",
      "Iteration:  547\n",
      "Iteration:  548\n",
      "Iteration:  549\n",
      "Iteration:  550\n",
      "Iteration:  551\n",
      "Iteration:  552\n",
      "Iteration:  553\n",
      "Iteration:  554\n",
      "Iteration:  555\n",
      "Iteration:  556\n",
      "Iteration:  557\n",
      "Iteration:  558\n",
      "Iteration:  559\n",
      "Iteration:  560\n",
      "Iteration:  561\n",
      "Iteration:  562\n",
      "Iteration:  563\n",
      "Iteration:  564\n",
      "Iteration:  565\n",
      "Iteration:  566\n",
      "Iteration:  567\n",
      "Iteration:  568\n",
      "Iteration:  569\n",
      "Iteration:  570\n",
      "Iteration:  571\n",
      "Iteration:  572\n",
      "Iteration:  573\n",
      "Iteration:  574\n",
      "Iteration:  575\n",
      "Iteration:  576\n",
      "Iteration:  577\n",
      "Iteration:  578\n",
      "Iteration:  579\n",
      "Iteration:  580\n",
      "Iteration:  581\n",
      "Iteration:  582\n",
      "Iteration:  583\n",
      "Iteration:  584\n",
      "Iteration:  585\n",
      "Iteration:  586\n",
      "Iteration:  587\n",
      "Iteration:  588\n",
      "Iteration:  589\n",
      "Iteration:  590\n",
      "Iteration:  591\n",
      "Iteration:  592\n",
      "Iteration:  593\n",
      "Iteration:  594\n",
      "Iteration:  595\n",
      "Iteration:  596\n",
      "Iteration:  597\n",
      "Iteration:  598\n",
      "Iteration:  599\n",
      "Iteration:  600\n",
      "Iteration:  601\n",
      "Iteration:  602\n",
      "Iteration:  603\n",
      "Iteration:  604\n",
      "Iteration:  605\n",
      "Iteration:  606\n",
      "Iteration:  607\n",
      "Iteration:  608\n",
      "Iteration:  609\n",
      "Iteration:  610\n",
      "Iteration:  611\n",
      "Iteration:  612\n",
      "Iteration:  613\n",
      "Iteration:  614\n",
      "Iteration:  615\n",
      "Iteration:  616\n",
      "Iteration:  617\n",
      "Iteration:  618\n",
      "Iteration:  619\n",
      "Iteration:  620\n",
      "Iteration:  621\n",
      "Iteration:  622\n",
      "Iteration:  623\n",
      "Iteration:  624\n",
      "Iteration:  625\n",
      "Iteration:  626\n",
      "Iteration:  627\n",
      "Iteration:  628\n",
      "Iteration:  629\n",
      "Iteration:  630\n",
      "Iteration:  631\n",
      "Iteration:  632\n",
      "Iteration:  633\n",
      "Iteration:  634\n",
      "Iteration:  635\n",
      "Iteration:  636\n",
      "Iteration:  637\n",
      "Iteration:  638\n",
      "Iteration:  639\n",
      "Iteration:  640\n",
      "Iteration:  641\n",
      "Iteration:  642\n",
      "Iteration:  643\n",
      "Iteration:  644\n",
      "Iteration:  645\n",
      "Iteration:  646\n",
      "Iteration:  647\n",
      "Iteration:  648\n",
      "Iteration:  649\n",
      "Iteration:  650\n",
      "Iteration:  651\n",
      "Iteration:  652\n",
      "Iteration:  653\n",
      "Iteration:  654\n",
      "Iteration:  655\n",
      "Iteration:  656\n",
      "Iteration:  657\n",
      "Iteration:  658\n",
      "Iteration:  659\n",
      "Iteration:  660\n",
      "Iteration:  661\n",
      "Iteration:  662\n",
      "Iteration:  663\n",
      "Iteration:  664\n",
      "Iteration:  665\n",
      "Iteration:  666\n",
      "Iteration:  667\n",
      "Iteration:  668\n",
      "Iteration:  669\n",
      "Iteration:  670\n",
      "Iteration:  671\n",
      "Iteration:  672\n",
      "Iteration:  673\n",
      "Iteration:  674\n",
      "Iteration:  675\n",
      "Iteration:  676\n",
      "Iteration:  677\n",
      "Iteration:  678\n",
      "Iteration:  679\n",
      "Iteration:  680\n",
      "Iteration:  681\n",
      "Iteration:  682\n",
      "Iteration:  683\n",
      "Iteration:  684\n",
      "Iteration:  685\n",
      "Iteration:  686\n",
      "Iteration:  687\n",
      "Iteration:  688\n",
      "Iteration:  689\n",
      "Iteration:  690\n",
      "Iteration:  691\n",
      "Iteration:  692\n",
      "Iteration:  693\n",
      "Iteration:  694\n",
      "Iteration:  695\n",
      "Iteration:  696\n",
      "Iteration:  697\n",
      "Iteration:  698\n",
      "Iteration:  699\n",
      "Iteration:  700\n",
      "Iteration:  701\n",
      "Iteration:  702\n",
      "Iteration:  703\n",
      "Iteration:  704\n",
      "Iteration:  705\n",
      "Iteration:  706\n",
      "Iteration:  707\n",
      "Iteration:  708\n",
      "Iteration:  709\n",
      "Iteration:  710\n",
      "Iteration:  711\n",
      "Iteration:  712\n",
      "Iteration:  713\n",
      "Iteration:  714\n",
      "Iteration:  715\n",
      "Iteration:  716\n",
      "Iteration:  717\n",
      "Iteration:  718\n",
      "Iteration:  719\n",
      "Iteration:  720\n",
      "Iteration:  721\n",
      "Iteration:  722\n",
      "Iteration:  723\n",
      "Iteration:  724\n",
      "Iteration:  725\n",
      "Iteration:  726\n",
      "Iteration:  727\n",
      "Iteration:  728\n",
      "Iteration:  729\n",
      "Iteration:  730\n",
      "Iteration:  731\n",
      "Iteration:  732\n",
      "Iteration:  733\n",
      "Iteration:  734\n",
      "Iteration:  735\n",
      "Iteration:  736\n",
      "Iteration:  737\n",
      "Iteration:  738\n",
      "Iteration:  739\n",
      "Iteration:  740\n",
      "Iteration:  741\n",
      "Iteration:  742\n",
      "Iteration:  743\n",
      "Iteration:  744\n",
      "Iteration:  745\n",
      "Iteration:  746\n",
      "Iteration:  747\n",
      "Iteration:  748\n",
      "Iteration:  749\n",
      "Iteration:  750\n",
      "Iteration:  751\n",
      "Iteration:  752\n",
      "Iteration:  753\n",
      "Iteration:  754\n",
      "Iteration:  755\n",
      "Iteration:  756\n",
      "Iteration:  757\n",
      "Iteration:  758\n",
      "Iteration:  759\n",
      "Iteration:  760\n",
      "Iteration:  761\n",
      "Iteration:  762\n",
      "Iteration:  763\n",
      "Iteration:  764\n",
      "Iteration:  765\n",
      "Iteration:  766\n",
      "Iteration:  767\n",
      "Iteration:  768\n",
      "Iteration:  769\n",
      "Iteration:  770\n",
      "Iteration:  771\n",
      "Iteration:  772\n",
      "Iteration:  773\n",
      "Iteration:  774\n",
      "Iteration:  775\n",
      "Iteration:  776\n",
      "Iteration:  777\n",
      "Iteration:  778\n",
      "Iteration:  779\n",
      "Iteration:  780\n",
      "Iteration:  781\n",
      "Iteration:  782\n",
      "Iteration:  783\n",
      "Iteration:  784\n",
      "Iteration:  785\n",
      "Iteration:  786\n",
      "Iteration:  787\n",
      "Iteration:  788\n",
      "Iteration:  789\n",
      "Iteration:  790\n",
      "Iteration:  791\n",
      "Iteration:  792\n",
      "Iteration:  793\n",
      "Iteration:  794\n",
      "Iteration:  795\n",
      "Iteration:  796\n",
      "Iteration:  797\n",
      "Iteration:  798\n",
      "Iteration:  799\n",
      "Iteration:  800\n",
      "Iteration:  801\n",
      "Iteration:  802\n",
      "Iteration:  803\n",
      "Iteration:  804\n",
      "Iteration:  805\n",
      "Iteration:  806\n",
      "Iteration:  807\n",
      "Iteration:  808\n",
      "Iteration:  809\n",
      "Iteration:  810\n",
      "Iteration:  811\n",
      "Iteration:  812\n",
      "Iteration:  813\n",
      "Iteration:  814\n",
      "Iteration:  815\n",
      "Iteration:  816\n",
      "Iteration:  817\n",
      "Iteration:  818\n",
      "Iteration:  819\n",
      "Iteration:  820\n",
      "Iteration:  821\n",
      "Iteration:  822\n",
      "Iteration:  823\n",
      "Iteration:  824\n",
      "Iteration:  825\n",
      "Iteration:  826\n",
      "Iteration:  827\n",
      "Iteration:  828\n",
      "Iteration:  829\n",
      "Iteration:  830\n",
      "Iteration:  831\n",
      "Iteration:  832\n",
      "Iteration:  833\n",
      "Iteration:  834\n",
      "Iteration:  835\n",
      "Iteration:  836\n",
      "Iteration:  837\n",
      "Iteration:  838\n",
      "Iteration:  839\n",
      "Iteration:  840\n",
      "Iteration:  841\n",
      "Iteration:  842\n",
      "Iteration:  843\n",
      "Iteration:  844\n",
      "Iteration:  845\n",
      "Iteration:  846\n",
      "Iteration:  847\n",
      "Iteration:  848\n",
      "Iteration:  849\n",
      "Iteration:  850\n",
      "Iteration:  851\n",
      "Iteration:  852\n",
      "Iteration:  853\n",
      "Iteration:  854\n",
      "Iteration:  855\n",
      "Iteration:  856\n",
      "Iteration:  857\n",
      "Iteration:  858\n",
      "Iteration:  859\n",
      "Iteration:  860\n",
      "Iteration:  861\n",
      "Iteration:  862\n",
      "Iteration:  863\n",
      "Iteration:  864\n",
      "Iteration:  865\n",
      "Iteration:  866\n",
      "Iteration:  867\n",
      "Iteration:  868\n",
      "Iteration:  869\n",
      "Iteration:  870\n",
      "Iteration:  871\n",
      "Iteration:  872\n",
      "Iteration:  873\n",
      "Iteration:  874\n",
      "Iteration:  875\n",
      "Iteration:  876\n",
      "Iteration:  877\n",
      "Iteration:  878\n",
      "Iteration:  879\n",
      "Iteration:  880\n",
      "Iteration:  881\n",
      "Iteration:  882\n",
      "Iteration:  883\n",
      "Iteration:  884\n",
      "Iteration:  885\n",
      "Iteration:  886\n",
      "Iteration:  887\n",
      "Iteration:  888\n",
      "Iteration:  889\n",
      "Iteration:  890\n",
      "Iteration:  891\n",
      "Iteration:  892\n",
      "Iteration:  893\n",
      "Iteration:  894\n",
      "Iteration:  895\n",
      "Iteration:  896\n",
      "Iteration:  897\n",
      "Iteration:  898\n",
      "Iteration:  899\n",
      "Iteration:  900\n",
      "Iteration:  901\n",
      "Iteration:  902\n",
      "Iteration:  903\n",
      "Iteration:  904\n",
      "Iteration:  905\n",
      "Iteration:  906\n",
      "Iteration:  907\n",
      "Iteration:  908\n",
      "Iteration:  909\n",
      "Iteration:  910\n",
      "Iteration:  911\n",
      "Iteration:  912\n",
      "Iteration:  913\n",
      "Iteration:  914\n",
      "Iteration:  915\n",
      "Iteration:  916\n",
      "Iteration:  917\n",
      "Iteration:  918\n",
      "Iteration:  919\n",
      "Iteration:  920\n",
      "Iteration:  921\n",
      "Iteration:  922\n",
      "Iteration:  923\n",
      "Iteration:  924\n",
      "Iteration:  925\n",
      "Iteration:  926\n",
      "Iteration:  927\n",
      "Iteration:  928\n",
      "Iteration:  929\n",
      "Iteration:  930\n",
      "Iteration:  931\n",
      "Iteration:  932\n",
      "Iteration:  933\n",
      "Iteration:  934\n",
      "Iteration:  935\n",
      "Iteration:  936\n",
      "Iteration:  937\n",
      "Iteration:  938\n",
      "Iteration:  939\n",
      "Iteration:  940\n",
      "Iteration:  941\n",
      "Iteration:  942\n",
      "Iteration:  943\n",
      "Iteration:  944\n",
      "Iteration:  945\n",
      "Iteration:  946\n",
      "Iteration:  947\n",
      "Iteration:  948\n",
      "Iteration:  949\n",
      "Iteration:  950\n",
      "Iteration:  951\n",
      "Iteration:  952\n",
      "Iteration:  953\n",
      "Iteration:  954\n",
      "Iteration:  955\n",
      "Iteration:  956\n",
      "Iteration:  957\n",
      "Iteration:  958\n",
      "Iteration:  959\n",
      "Iteration:  960\n",
      "Iteration:  961\n",
      "Iteration:  962\n",
      "Iteration:  963\n",
      "Iteration:  964\n",
      "Iteration:  965\n",
      "Iteration:  966\n",
      "Iteration:  967\n",
      "Iteration:  968\n",
      "Iteration:  969\n",
      "Iteration:  970\n",
      "Iteration:  971\n",
      "Iteration:  972\n",
      "Iteration:  973\n",
      "Iteration:  974\n",
      "Iteration:  975\n",
      "Iteration:  976\n",
      "Iteration:  977\n",
      "Iteration:  978\n",
      "Iteration:  979\n",
      "Iteration:  980\n",
      "Iteration:  981\n",
      "Iteration:  982\n",
      "Iteration:  983\n",
      "Iteration:  984\n",
      "Iteration:  985\n",
      "Iteration:  986\n",
      "Iteration:  987\n",
      "Iteration:  988\n",
      "Iteration:  989\n",
      "Iteration:  990\n",
      "Iteration:  991\n",
      "Iteration:  992\n",
      "Iteration:  993\n",
      "Iteration:  994\n",
      "Iteration:  995\n",
      "Iteration:  996\n",
      "Iteration:  997\n",
      "Iteration:  998\n",
      "Iteration:  999\n",
      "[[ 0.19140912]\n",
      " [ 0.19374282]\n",
      " [ 0.19615019]\n",
      " [ 0.19863453]\n",
      " [ 0.20119972]]\n"
     ]
    }
   ],
   "source": [
    "zcr_data = [np.loadtxt(\"reps/clarinet_zcr.txt\"), np.loadtxt(\"reps/flute_zcr.txt\"), np.loadtxt(\"reps/guitar_zcr.txt\"), np.loadtxt(\"reps/saxophone_zcr.txt\"), np.loadtxt(\"reps/violin_zcr.txt\")]\n",
    "sc_data = [np.loadtxt(\"reps/clarinet_centroid.txt\"), np.loadtxt(\"reps/flute_centroid.txt\"), np.loadtxt(\"reps/guitar_centroid.txt\"), np.loadtxt(\"reps/saxophone_centroid.txt\"), np.loadtxt(\"reps/violin_centroid.txt\")]\n",
    "# f_data = \n",
    "# g_data = \n",
    "# s_data = \n",
    "# v_data = \n",
    "inp = []\n",
    "inputs = []\n",
    "outputs = []\n",
    "for i in range(100):\n",
    "    for j in range(5):\n",
    "        inputs = []\n",
    "        output = [0]*5\n",
    "        inputs.append(zcr_data[j][i])\n",
    "        inputs.append(sc_data[j][i])\n",
    "        output[j] = 1\n",
    "        outputs.append(output)\n",
    "        inp.append(np.array(inputs))\n",
    "        # for k in range(5):\n",
    "        #    if k != j:\n",
    "         #       outputs[k].append(0)\n",
    "# print(inp)\n",
    "inp = np.array(inp)\n",
    "print(inp.T)\n",
    "output = np.array(outputs)\n",
    "print(output)\n",
    "test_inp = np.array([inp[0]])\n",
    "nn.initialize_network(2, 5, 3, 5)\n",
    "nn.train_network(inp.T, output.T, nn.update_weights, 1000, 0.1)\n",
    "print(nn.predict_network(test_inp.T, nn.logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Testing the neural network on the data from ps6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Thursday, May 26\n",
    "\n",
    "Our neural network code is not predicting correctly, and I am spending my time going back to try and de-bug it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
